{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32030,"status":"ok","timestamp":1670562220534,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"RtEzs3dSaSYZ","outputId":"43b08d8b-2012-4f64-e3cf-d630b7dac77e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1602,"status":"ok","timestamp":1670562222133,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"_v0i2laGayPd","outputId":"b4f1a6e2-6edd-457b-fbea-3928df88f6a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/cs230/womens_edu\n"]}],"source":["%cd /content/drive/MyDrive/Colab Notebooks/cs230/womens_edu"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9530,"status":"ok","timestamp":1670562234234,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"DoPQFgjG8ZSX","outputId":"3e961e11-22c4-4159-c233-209dbd90c64d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fast_ml\n","  Downloading fast_ml-3.68-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 894 kB/s \n","\u001b[?25hInstalling collected packages: fast-ml\n","Successfully installed fast-ml-3.68\n"]}],"source":["!pip install fast_ml\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ru_XmlQDa17B","executionInfo":{"status":"ok","timestamp":1670562236393,"user_tz":480,"elapsed":2163,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["import argparse\n","import logging\n","import os\n","\n","import h5py\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models\n","from torchvision import transforms, utils\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","from skimage import io, transform\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n","from sklearn.model_selection import train_test_split\n","from fast_ml.model_development import train_valid_test_split\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ko_lpQBEa5dP","executionInfo":{"status":"ok","timestamp":1670562286627,"user_tz":480,"elapsed":316,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["dataset_root_dir = '/content/drive/MyDrive/Colab Notebooks/cs230/womens_edu/data/'"]},{"cell_type":"markdown","source":["# Git"],"metadata":{"id":"s4JcaPH7rDQ2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XMfPMgaH1nxi"},"outputs":[],"source":["!git config --global user.email \"disaaldan@gmail.com\"\n","!git config --global user.name \"disaalda\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ne1RynH13_hL"},"outputs":[],"source":["username='disaalda'\n","repository='cs230'\n","git_token='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'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1670359690031,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"msuxCKhQ4w3O","outputId":"74c0970f-6d1c-4d2b-947f-b76d01f733a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["origin\thttps://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@github.com/disaalda/cs230.git (fetch)\n","origin\thttps://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@github.com/disaalda/cs230.git (push)\n"]}],"source":["!git remote -v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6WqgPI06yTd"},"outputs":[],"source":["!git remote rm origin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kkiYY9S4Fwb"},"outputs":[],"source":["!git remote add origin https://{git_token}@github.com/{username}/{repository}.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1059,"status":"ok","timestamp":1670359694849,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"lithLEmz1qaT","outputId":"97960744-8e38-44e1-c904-e35a2a5cb092"},"outputs":[{"output_type":"stream","name":"stdout","text":["Everything up-to-date\n"]}],"source":["!git push origin main"]},{"cell_type":"markdown","metadata":{"id":"r9RfKqCIljld"},"source":["# Data Processing "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1670562292059,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"a9ALmpszliUN","outputId":"9d71c300-47b1-4b90-c538-e480d42893d3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0  Unnamed: 1  Unnamed: 0.1             DHSID_EA  year  cc  \\\n","0           0       74101         74101  AM-2010-6#-00000175  2010  AM   \n","1           1       74102         74102  AM-2010-6#-00000176  2010  AM   \n","2           2       74103         74103  AM-2010-6#-00000215  2010  AM   \n","3           3       74104         74104  AM-2010-6#-00000218  2010  AM   \n","4           4       74105         74105  AM-2010-6#-00000232  2010  AM   \n","\n","         lat        lon  women_edu  \\\n","0  40.865949  44.052637  10.307692   \n","1  40.878055  44.042707  10.761905   \n","2  40.776914  43.841243  12.476190   \n","3  40.808131  43.840526  12.600000   \n","4  40.765091  43.783366  11.480000   \n","\n","                                                path  img_captured_at  \\\n","0  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1341142789364   \n","1  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1341142789364   \n","2  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","3  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","4  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","\n","     img_lon    img_lat           img_id  \\\n","0  44.118895  40.914849  455621522386245   \n","1  44.118895  40.914849  455621522386245   \n","2  43.853536  40.802320  448238129596253   \n","3  43.852432  40.798289  803317710311828   \n","4  43.853536  40.802320  448238129596253   \n","\n","                                      img_path  \n","0  AM/AM-2010-6#-00000175/455621522386245.jpeg  \n","1  AM/AM-2010-6#-00000176/455621522386245.jpeg  \n","2  AM/AM-2010-6#-00000215/448238129596253.jpeg  \n","3  AM/AM-2010-6#-00000218/803317710311828.jpeg  \n","4  AM/AM-2010-6#-00000232/448238129596253.jpeg  "],"text/html":["\n","  <div id=\"df-e10d1039-b49e-45e8-b02b-e1c7001674fc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 1</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>DHSID_EA</th>\n","      <th>year</th>\n","      <th>cc</th>\n","      <th>lat</th>\n","      <th>lon</th>\n","      <th>women_edu</th>\n","      <th>path</th>\n","      <th>img_captured_at</th>\n","      <th>img_lon</th>\n","      <th>img_lat</th>\n","      <th>img_id</th>\n","      <th>img_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>74101</td>\n","      <td>74101</td>\n","      <td>AM-2010-6#-00000175</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.865949</td>\n","      <td>44.052637</td>\n","      <td>10.307692</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1341142789364</td>\n","      <td>44.118895</td>\n","      <td>40.914849</td>\n","      <td>455621522386245</td>\n","      <td>AM/AM-2010-6#-00000175/455621522386245.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>74102</td>\n","      <td>74102</td>\n","      <td>AM-2010-6#-00000176</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.878055</td>\n","      <td>44.042707</td>\n","      <td>10.761905</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1341142789364</td>\n","      <td>44.118895</td>\n","      <td>40.914849</td>\n","      <td>455621522386245</td>\n","      <td>AM/AM-2010-6#-00000176/455621522386245.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>74103</td>\n","      <td>74103</td>\n","      <td>AM-2010-6#-00000215</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.776914</td>\n","      <td>43.841243</td>\n","      <td>12.476190</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.853536</td>\n","      <td>40.802320</td>\n","      <td>448238129596253</td>\n","      <td>AM/AM-2010-6#-00000215/448238129596253.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>74104</td>\n","      <td>74104</td>\n","      <td>AM-2010-6#-00000218</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.808131</td>\n","      <td>43.840526</td>\n","      <td>12.600000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.852432</td>\n","      <td>40.798289</td>\n","      <td>803317710311828</td>\n","      <td>AM/AM-2010-6#-00000218/803317710311828.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>74105</td>\n","      <td>74105</td>\n","      <td>AM-2010-6#-00000232</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.765091</td>\n","      <td>43.783366</td>\n","      <td>11.480000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.853536</td>\n","      <td>40.802320</td>\n","      <td>448238129596253</td>\n","      <td>AM/AM-2010-6#-00000232/448238129596253.jpeg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e10d1039-b49e-45e8-b02b-e1c7001674fc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e10d1039-b49e-45e8-b02b-e1c7001674fc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e10d1039-b49e-45e8-b02b-e1c7001674fc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["df = pd.read_csv('data/filtered_sampled_ss.csv')\n","df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1670562292368,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"-_IfIqTp-BW1","outputId":"8fda7828-1250-426f-a3fc-cec869a093f0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0  Unnamed: 1  Unnamed: 0.1             DHSID_EA  year  cc  \\\n","0           0       74101         74101  AM-2010-6#-00000175  2010  AM   \n","1           1       74102         74102  AM-2010-6#-00000176  2010  AM   \n","2           2       74103         74103  AM-2010-6#-00000215  2010  AM   \n","3           3       74104         74104  AM-2010-6#-00000218  2010  AM   \n","4           4       74105         74105  AM-2010-6#-00000232  2010  AM   \n","\n","         lat        lon  women_edu  \\\n","0  40.865949  44.052637  10.307692   \n","1  40.878055  44.042707  10.761905   \n","2  40.776914  43.841243  12.476190   \n","3  40.808131  43.840526  12.600000   \n","4  40.765091  43.783366  11.480000   \n","\n","                                                path  img_captured_at  \\\n","0  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1341142789364   \n","1  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1341142789364   \n","2  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","3  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","4  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","\n","     img_lon    img_lat           img_id  \\\n","0  44.118895  40.914849  455621522386245   \n","1  44.118895  40.914849  455621522386245   \n","2  43.853536  40.802320  448238129596253   \n","3  43.852432  40.798289  803317710311828   \n","4  43.853536  40.802320  448238129596253   \n","\n","                                            img_path  \n","0  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","1  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","2  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","3  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","4  /content/drive/MyDrive/Colab Notebooks/cs230/w...  "],"text/html":["\n","  <div id=\"df-d6a116b7-8d79-42dd-9f84-10372b7ca0fd\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 1</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>DHSID_EA</th>\n","      <th>year</th>\n","      <th>cc</th>\n","      <th>lat</th>\n","      <th>lon</th>\n","      <th>women_edu</th>\n","      <th>path</th>\n","      <th>img_captured_at</th>\n","      <th>img_lon</th>\n","      <th>img_lat</th>\n","      <th>img_id</th>\n","      <th>img_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>74101</td>\n","      <td>74101</td>\n","      <td>AM-2010-6#-00000175</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.865949</td>\n","      <td>44.052637</td>\n","      <td>10.307692</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1341142789364</td>\n","      <td>44.118895</td>\n","      <td>40.914849</td>\n","      <td>455621522386245</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>74102</td>\n","      <td>74102</td>\n","      <td>AM-2010-6#-00000176</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.878055</td>\n","      <td>44.042707</td>\n","      <td>10.761905</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1341142789364</td>\n","      <td>44.118895</td>\n","      <td>40.914849</td>\n","      <td>455621522386245</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>74103</td>\n","      <td>74103</td>\n","      <td>AM-2010-6#-00000215</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.776914</td>\n","      <td>43.841243</td>\n","      <td>12.476190</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.853536</td>\n","      <td>40.802320</td>\n","      <td>448238129596253</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>74104</td>\n","      <td>74104</td>\n","      <td>AM-2010-6#-00000218</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.808131</td>\n","      <td>43.840526</td>\n","      <td>12.600000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.852432</td>\n","      <td>40.798289</td>\n","      <td>803317710311828</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>74105</td>\n","      <td>74105</td>\n","      <td>AM-2010-6#-00000232</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.765091</td>\n","      <td>43.783366</td>\n","      <td>11.480000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.853536</td>\n","      <td>40.802320</td>\n","      <td>448238129596253</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6a116b7-8d79-42dd-9f84-10372b7ca0fd')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d6a116b7-8d79-42dd-9f84-10372b7ca0fd button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d6a116b7-8d79-42dd-9f84-10372b7ca0fd');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}],"source":["df['img_path'] = dataset_root_dir + df['img_path']\n","df.head()"]},{"cell_type":"code","source":["# collapse the dataset s.t there is 1 satellite and 1 street image\n","dhsid_df = df.drop_duplicates(subset = ['DHSID_EA'], keep = 'first', inplace = False) \n","dhsid_df.tail(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":618},"id":"_s7y4Vj0rMvn","executionInfo":{"status":"ok","timestamp":1670562292698,"user_tz":480,"elapsed":3,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}},"outputId":"2bc34920-1c09-4e84-c9cc-c560fb8cb76c"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0  Unnamed: 1  Unnamed: 0.1             DHSID_EA  year  cc  \\\n","8913        1883     1016609       1016609  ZW-2015-7#-00000362  2015  ZW   \n","8918        1884     1016628       1016628  ZW-2015-7#-00000369  2015  ZW   \n","8923        1885     1016672       1016672  ZW-2015-7#-00000374  2015  ZW   \n","8928        1886     1016725       1016725  ZW-2015-7#-00000378  2015  ZW   \n","8933        1887     1016789       1016789  ZW-2015-7#-00000381  2015  ZW   \n","8938        1888     1016810       1016810  ZW-2015-7#-00000390  2015  ZW   \n","8942        1889     1016831       1016831  ZW-2015-7#-00000392  2015  ZW   \n","8947        1890     1016843       1016843  ZW-2015-7#-00000394  2015  ZW   \n","8951        1891     1016940       1016940  ZW-2015-7#-00000396  2015  ZW   \n","8956        1892     1016944       1016944  ZW-2015-7#-00000399  2015  ZW   \n","\n","            lat        lon  women_edu  \\\n","8913 -20.175557  28.534461  12.030303   \n","8918 -20.150383  28.535698  10.391304   \n","8923 -20.190443  28.518083  10.406250   \n","8928 -20.191504  28.541807  10.741935   \n","8933 -20.171571  28.506735  11.303030   \n","8938 -17.878083  31.033349  11.521739   \n","8942 -20.144304  28.568426   9.800000   \n","8947 -17.929422  30.997536   9.594595   \n","8951 -17.915288  31.156115   9.750000   \n","8956 -17.914251  30.956975  11.243243   \n","\n","                                                   path  img_captured_at  \\\n","8913  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1412877005000   \n","8918  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1420320244000   \n","8923  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1412877109000   \n","8928  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1420320243000   \n","8933  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1412877001000   \n","8938  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1427383510000   \n","8942  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1412877001000   \n","8947  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1505310237363   \n","8951  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1427374692000   \n","8956  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1426847246000   \n","\n","        img_lon    img_lat            img_id  \\\n","8913  28.594323 -20.159139  5613455692027756   \n","8918  28.592581 -20.158838   188787576419386   \n","8923  28.594326 -20.159137   290163482654021   \n","8928  28.587133 -20.154499   154794479980117   \n","8933  28.594329 -20.159140   164492678913868   \n","8938  31.133339 -17.885400   857902778399832   \n","8942  28.594329 -20.159140   164492678913868   \n","8947  31.082531 -17.836589  4134046553325197   \n","8951  31.141384 -17.916645   475346967083817   \n","8956  30.922883 -17.815147  1807287552809157   \n","\n","                                               img_path  \n","8913  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","8918  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","8923  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","8928  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","8933  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","8938  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","8942  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","8947  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","8951  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","8956  /content/drive/MyDrive/Colab Notebooks/cs230/w...  "],"text/html":["\n","  <div id=\"df-972a33c8-4f62-420f-9154-2a5281be9ded\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 1</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>DHSID_EA</th>\n","      <th>year</th>\n","      <th>cc</th>\n","      <th>lat</th>\n","      <th>lon</th>\n","      <th>women_edu</th>\n","      <th>path</th>\n","      <th>img_captured_at</th>\n","      <th>img_lon</th>\n","      <th>img_lat</th>\n","      <th>img_id</th>\n","      <th>img_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>8913</th>\n","      <td>1883</td>\n","      <td>1016609</td>\n","      <td>1016609</td>\n","      <td>ZW-2015-7#-00000362</td>\n","      <td>2015</td>\n","      <td>ZW</td>\n","      <td>-20.175557</td>\n","      <td>28.534461</td>\n","      <td>12.030303</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1412877005000</td>\n","      <td>28.594323</td>\n","      <td>-20.159139</td>\n","      <td>5613455692027756</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>8918</th>\n","      <td>1884</td>\n","      <td>1016628</td>\n","      <td>1016628</td>\n","      <td>ZW-2015-7#-00000369</td>\n","      <td>2015</td>\n","      <td>ZW</td>\n","      <td>-20.150383</td>\n","      <td>28.535698</td>\n","      <td>10.391304</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1420320244000</td>\n","      <td>28.592581</td>\n","      <td>-20.158838</td>\n","      <td>188787576419386</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>8923</th>\n","      <td>1885</td>\n","      <td>1016672</td>\n","      <td>1016672</td>\n","      <td>ZW-2015-7#-00000374</td>\n","      <td>2015</td>\n","      <td>ZW</td>\n","      <td>-20.190443</td>\n","      <td>28.518083</td>\n","      <td>10.406250</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1412877109000</td>\n","      <td>28.594326</td>\n","      <td>-20.159137</td>\n","      <td>290163482654021</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>8928</th>\n","      <td>1886</td>\n","      <td>1016725</td>\n","      <td>1016725</td>\n","      <td>ZW-2015-7#-00000378</td>\n","      <td>2015</td>\n","      <td>ZW</td>\n","      <td>-20.191504</td>\n","      <td>28.541807</td>\n","      <td>10.741935</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1420320243000</td>\n","      <td>28.587133</td>\n","      <td>-20.154499</td>\n","      <td>154794479980117</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>8933</th>\n","      <td>1887</td>\n","      <td>1016789</td>\n","      <td>1016789</td>\n","      <td>ZW-2015-7#-00000381</td>\n","      <td>2015</td>\n","      <td>ZW</td>\n","      <td>-20.171571</td>\n","      <td>28.506735</td>\n","      <td>11.303030</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1412877001000</td>\n","      <td>28.594329</td>\n","      <td>-20.159140</td>\n","      <td>164492678913868</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>8938</th>\n","      <td>1888</td>\n","      <td>1016810</td>\n","      <td>1016810</td>\n","      <td>ZW-2015-7#-00000390</td>\n","      <td>2015</td>\n","      <td>ZW</td>\n","      <td>-17.878083</td>\n","      <td>31.033349</td>\n","      <td>11.521739</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1427383510000</td>\n","      <td>31.133339</td>\n","      <td>-17.885400</td>\n","      <td>857902778399832</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>8942</th>\n","      <td>1889</td>\n","      <td>1016831</td>\n","      <td>1016831</td>\n","      <td>ZW-2015-7#-00000392</td>\n","      <td>2015</td>\n","      <td>ZW</td>\n","      <td>-20.144304</td>\n","      <td>28.568426</td>\n","      <td>9.800000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1412877001000</td>\n","      <td>28.594329</td>\n","      <td>-20.159140</td>\n","      <td>164492678913868</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>8947</th>\n","      <td>1890</td>\n","      <td>1016843</td>\n","      <td>1016843</td>\n","      <td>ZW-2015-7#-00000394</td>\n","      <td>2015</td>\n","      <td>ZW</td>\n","      <td>-17.929422</td>\n","      <td>30.997536</td>\n","      <td>9.594595</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1505310237363</td>\n","      <td>31.082531</td>\n","      <td>-17.836589</td>\n","      <td>4134046553325197</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>8951</th>\n","      <td>1891</td>\n","      <td>1016940</td>\n","      <td>1016940</td>\n","      <td>ZW-2015-7#-00000396</td>\n","      <td>2015</td>\n","      <td>ZW</td>\n","      <td>-17.915288</td>\n","      <td>31.156115</td>\n","      <td>9.750000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1427374692000</td>\n","      <td>31.141384</td>\n","      <td>-17.916645</td>\n","      <td>475346967083817</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>8956</th>\n","      <td>1892</td>\n","      <td>1016944</td>\n","      <td>1016944</td>\n","      <td>ZW-2015-7#-00000399</td>\n","      <td>2015</td>\n","      <td>ZW</td>\n","      <td>-17.914251</td>\n","      <td>30.956975</td>\n","      <td>11.243243</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1426847246000</td>\n","      <td>30.922883</td>\n","      <td>-17.815147</td>\n","      <td>1807287552809157</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-972a33c8-4f62-420f-9154-2a5281be9ded')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-972a33c8-4f62-420f-9154-2a5281be9ded button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-972a33c8-4f62-420f-9154-2a5281be9ded');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1670562293022,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"LYuvsLULl6o5"},"outputs":[],"source":["# collapse the dataset s.t. each row is a satellite path with a list of img paths \n","# dhsid_df = df.groupby(['DHSID_EA', 'year', 'women_edu', 'path', 'cc'])['img_path'].apply(list).reset_index()\n","# dhsid_df.tail(10) "]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1670562293779,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"YbUwWZI-xZI_","outputId":"ef280a2f-9abd-4b25-932f-22e0f2461d07"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/cs230/womens_edu/data/AM/AM-2016-7#-00000020/4370483359662416.jpeg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["dhsid_df['img_path'][100]"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1670562298818,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"pmH7bw3kv-5B","outputId":"2f340a61-9fca-4903-fc41-5546b0497584"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Unnamed: 0  Unnamed: 1  Unnamed: 0.1  DHSID_EA  year  lat  lon  women_edu  \\\n","cc                                                                              \n","AM         312         312           312       312   312  312  312        312   \n","BJ         265         265           265       265   265  265  265        265   \n","CD          37          37            37        37    37   37   37         37   \n","CM         443         443           443       443   443  443  443        443   \n","GH         184         184           184       184   184  184  184        184   \n","KY          83          83            83        83    83   83   83         83   \n","MD         127         127           127       127   127  127  127        127   \n","NM         180         180           180       180   180  180  180        180   \n","NP         172         172           172       172   172  172  172        172   \n","ZW          90          90            90        90    90   90   90         90   \n","\n","    path  img_captured_at  img_lon  img_lat  img_id  img_path  \n","cc                                                             \n","AM   312              312      312      312     312       312  \n","BJ   265              265      265      265     265       265  \n","CD    37               37       37       37      37        37  \n","CM   443              443      443      443     443       443  \n","GH   184              184      184      184     184       184  \n","KY    83               83       83       83      83        83  \n","MD   127              127      127      127     127       127  \n","NM   180              180      180      180     180       180  \n","NP   172              172      172      172     172       172  \n","ZW    90               90       90       90      90        90  "],"text/html":["\n","  <div id=\"df-28f70d8a-30be-4905-b029-5fbc6dc0d6cb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 1</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>DHSID_EA</th>\n","      <th>year</th>\n","      <th>lat</th>\n","      <th>lon</th>\n","      <th>women_edu</th>\n","      <th>path</th>\n","      <th>img_captured_at</th>\n","      <th>img_lon</th>\n","      <th>img_lat</th>\n","      <th>img_id</th>\n","      <th>img_path</th>\n","    </tr>\n","    <tr>\n","      <th>cc</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>AM</th>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","    </tr>\n","    <tr>\n","      <th>BJ</th>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","    </tr>\n","    <tr>\n","      <th>CD</th>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","    </tr>\n","    <tr>\n","      <th>CM</th>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","    </tr>\n","    <tr>\n","      <th>GH</th>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","    </tr>\n","    <tr>\n","      <th>KY</th>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","    </tr>\n","    <tr>\n","      <th>MD</th>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","    </tr>\n","    <tr>\n","      <th>NM</th>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","    </tr>\n","    <tr>\n","      <th>NP</th>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","    </tr>\n","    <tr>\n","      <th>ZW</th>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28f70d8a-30be-4905-b029-5fbc6dc0d6cb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-28f70d8a-30be-4905-b029-5fbc6dc0d6cb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-28f70d8a-30be-4905-b029-5fbc6dc0d6cb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}],"source":["dhsid_df.groupby(['cc']).count()"]},{"cell_type":"markdown","metadata":{"id":"F_mr_Fj6dXZK"},"source":["# Dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"UhnuHCRF3on7","executionInfo":{"status":"ok","timestamp":1670562305667,"user_tz":480,"elapsed":354,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["def get_street_tensor(img_path_list):\n","  image_tensor_list = [] \n","\n","  # added to handle string img_path input not formatted as list\n","  if not isinstance(img_path_list, list):\n","    img_path_list = [img_path_list]\n","\n","  for img_path in img_path_list:\n","    image = io.imread(img_path)\n","    image = (image - image.min()) / (image.max() - image.min())\n","    image_tensor = torch.from_numpy(image)     \n","    image_tensor = image_tensor.permute(2,0,1).float()\n","    #print(image_tensor.shape)\n","    #print(img_path)\n","    image_tensor_list.append(image_tensor)\n","\n","  return image_tensor_list"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"xhfJGHrS33Kt","executionInfo":{"status":"ok","timestamp":1670562306512,"user_tz":480,"elapsed":2,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["def get_satellite_tensor(img_path):\n","    image = io.imread(img_path)\n","    image = image[:3]\n","    image = image[::-1]\n","    image = (image - image.min()) / (image.max() - image.min())\n","    image_tensor = torch.from_numpy(image)     \n","\n","    return image_tensor "]},{"cell_type":"code","execution_count":16,"metadata":{"id":"JuCF77524YuE","executionInfo":{"status":"ok","timestamp":1670562306832,"user_tz":480,"elapsed":3,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["# Combined Dataset class \n","class DHSIDDataset(Dataset):\n","  def __init__(self, df):\n","    self.satellite_path = df['path'].to_numpy()\n","    self.street_path = df['img_path'].to_numpy() # a list of paths \n","    self.targets = df['women_edu'].to_numpy()\n","\n","  def __len__(self):\n","    return self.satellite_path.shape[0] \n","\n","  def __getitem__(self, idx):\n","    sat_tensor = get_satellite_tensor(self.satellite_path[idx])\n","    street_tensor_list = get_street_tensor(self.street_path[idx]) # a list of tensors \n","    target = torch.Tensor(np.array([self.targets[idx]]))\n","\n","    return sat_tensor, street_tensor_list, target"]},{"cell_type":"markdown","metadata":{"id":"zDrgLqlQbUUR"},"source":["# Model"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"VE1UfKCMkkZH","executionInfo":{"status":"ok","timestamp":1670562821513,"user_tz":480,"elapsed":310,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["def create_model(model):\n","    if model == 'resnet18':\n","        model = models.resnet18(pretrained=True)\n","    elif model == 'resnet34':\n","        model = models.resnet34(pretrained=True)\n","    elif model == 'resnet50':\n","        model = models.resnet50(pretrained=True)\n","    model = nn.Sequential(*list(model.children())[:-1]) # get model only up to second to last layer \n","    return model"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"iAumopwn3cd4","executionInfo":{"status":"ok","timestamp":1670562821871,"user_tz":480,"elapsed":3,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["class SatelliteStreetModel(nn.Module):\n","  def __init__(self, sat_model, street_model):\n","    super(SatelliteStreetModel, self).__init__()\n","\n","    self.satellite_model = create_model(sat_model)\n","    self.street_model = create_model(street_model)\n","    self.activation2 = nn.LeakyReLU()\n","    self.fc1 = nn.Linear(512*2, 1000)\n","    self.fc2 = nn.Linear(1000, 750)\n","    self.fc3 = nn.Linear(750, 512)\n","    self.fc4 = nn.Linear(512, 256)\n","    self.fc5 = nn.Linear(256, 128)\n","    self.fc6 = nn.Linear(128,64)\n","    self.fclast = nn.Linear(64,1)\n","\n","\n","  def forward(self, sat_x, street_x):\n","    satellite_fv = self.satellite_model(sat_x)\n","    satellite_fv = torch.flatten(satellite_fv , start_dim=2)\n","    # print(satellite_fv.shape)\n","    street_fv = self.street_model(street_x) \n","    street_fv = torch.mean(street_fv, 0)\n","    street_fv = torch.flatten(satellite_fv, start_dim=2)\n","    # print(satellite_fv.shape, street_fv.shape)\n","    # print(street_fv.shape)\n","    concat_fv = torch.cat( (satellite_fv, street_fv), dim=1)\n","    # print(concat_fv.shape)\n","    concat_fv = torch.flatten(concat_fv, start_dim = 1)\n","    # TO DO: add more Dense/FC layers\n","    concat_fv = self.fc1(concat_fv)\n","    concat_fv = self.activation2(concat_fv)\n","    concat_fv = self.fc2(concat_fv)\n","    concat_fv = self.activation2(concat_fv)\n","    concat_fv = self.fc3(concat_fv)\n","    concat_fv = self.activation2(concat_fv)\n","    concat_fv = self.fc4(concat_fv)\n","    concat_fv = self.fc5(concat_fv)\n","    concat_fv = self.fc6(concat_fv)\n","\n","    # print(concat_fv.shape)\n","    # print('first activ + FC ayer')\n","    # concat_fc = self.activation(concat_fv)\n","    # concat_fv = torch.squeeze(concat_fv)\n","    out = self.fclast(concat_fv)\n","    # out = torch.squeeze(out)\n","    # print('second layer')\n","    # print(out.shape)\n","    # print('done')\n","    return out"]},{"cell_type":"markdown","metadata":{"id":"XQQjRAjacXTJ"},"source":["# Train"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"9G3NajicdO3S","executionInfo":{"status":"ok","timestamp":1670562416743,"user_tz":480,"elapsed":300,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["def split_train_val_test(df):\n","  X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(df, target = 'women_edu', \n","                                                                            train_size=0.7, valid_size=0.15, test_size=0.15)\n","  X_train['women_edu'] = y_train\n","  train_df = X_train\n","  X_valid['women_edu'] = y_valid\n","  val_df = X_valid\n","  X_test['women_edu'] = y_test\n","  test_df = X_test\n","  return train_df, val_df, test_df \n","\n","\n","\n","  "]},{"cell_type":"code","execution_count":20,"metadata":{"id":"pDEz3sK9WtiS","executionInfo":{"status":"ok","timestamp":1670562417042,"user_tz":480,"elapsed":1,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["def split_train_val_test2(df, testlist, vallist):\n","  test_df = df.loc[df['cc'].isin(testlist)]\n","  val_df = df.loc[df['cc'].isin(vallist)]\n","  train_df = df.loc[~df['cc'].isin(testlist + vallist)]\n","\n","  return train_df, val_df, test_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksrTxeUA-KtB"},"outputs":[],"source":["tb = SummaryWriter()"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":391,"status":"ok","timestamp":1670562419712,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"6sPO5QBvrwBk","outputId":"fd3bac22-20e5-4e13-cd67-377f77788e77"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":21}],"source":["# use GPU \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"CiUBPXZaAPWv","executionInfo":{"status":"ok","timestamp":1670562423080,"user_tz":480,"elapsed":310,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, batch_size, verbose=True):\n","    batch_size = torch.from_numpy(np.array(batch_size)).to(device)\n","\n","\n","    for epoch in range(1, num_epochs + 1):\n","        preds = []\n","        trues = []\n","        epoch_loss = 0.0\n","        batch_loss = torch.tensor([0.0], requires_grad=True)\n","        batch_loss =  batch_loss.to(device)\n","        valid_loss = 0.0\n","        batch_num = 1\n","\n","        optimizer.zero_grad()\n","\n","        for sat_inputs, street_inputs, targets in train_loader:\n","            # satellite \n","            sat_inputs = sat_inputs.to(device)\n","            # true values\n","            targets = targets.to(device)\n","            # street \n","            street_inputs = torch.stack(street_inputs, dim=1).squeeze(0)\n","            street_inputs = street_inputs.to(device)\n","            # model\n","            output = model(sat_inputs, street_inputs)\n","            # print(output, targets[0])\n","            loss = criterion(output, targets[0]) \n","            # investigate targets[0]\n","            # r2 prep\n","            pred = output.cpu().detach().numpy()\n","            pred = np.asscalar(pred)\n","            preds.append(pred)\n","            true = targets[0].cpu().detach().numpy()\n","            true = np.asscalar(true)\n","            trues.append(true)\n","            # batch_loss, epoch_loss\n","            batch_loss = batch_loss + loss\n","            epoch_loss += loss.item()\n"," \n","            # when batch num (num iterations) reaches batch_size\n","            if batch_num % batch_size.item() == 0:\n","              # calculate r2 at batch \n","              numpreds = np.array(preds)\n","              numtrues = np.array(trues)\n","              r2 = r2_score(numtrues, numpreds) \n","              preds = []\n","              trues = []\n","              # batch_loss\n","              batch_loss = batch_loss / batch_size.item()\n","              # clear grads + backprop\n","              batch_loss.backward() # batch loss\n","              optimizer.step()\n","              optimizer.zero_grad()\n","              # if verbose then print these things\n","              if verbose:                              \n","                print(f'Epoch [{epoch}/{num_epochs}], Step [{batch_num}/{len(train_loader)}], '\n","                      f'Loss: {batch_loss.item():.4f}',\n","                      f'R2: {r2}')\n","                \n","              batch_loss = torch.tensor([0.0], requires_grad=True)\n","              batch_loss =  batch_loss.to(device)\n","  \n","\n","            batch_num += 1\n"," \n","        # Validation loop\n","        with torch.no_grad():\n","          preds = []\n","          trues = []\n","          for sat_inputs, street_inputs, targets in val_loader:\n","            sat_inputs = sat_inputs.to(device)\n","            targets = targets.to(device)\n","\n","            street_inputs = torch.stack(street_inputs, dim=1).squeeze(0)\n","            street_inputs = street_inputs.to(device)\n","\n","            output = model(sat_inputs, street_inputs)\n","            # loop for r2\n","            pred = output.cpu().detach().numpy()\n","            pred = np.asscalar(pred)\n","            preds.append(pred)\n","            true = targets[0].cpu().detach().numpy()\n","            true = np.asscalar(true)\n","            trues.append(true)\n","            # loss and validation_loss \n","            loss = criterion(output,targets[0])\n","            valid_loss += loss.item()\n","        \n","        #r2 processing\n","        numpreds = np.array(preds)\n","        numtrues = np.array(trues)\n","        r2val = r2_score(numtrues, numpreds)\n","        # epoch loss + val loss\n","        epoch_loss /= len(train_loader)\n","        valid_loss /= len(val_loader)\n","        print(f'Epoch: {epoch}/{num_epochs}.. Epoch Loss: {epoch_loss}.. Validation Loss: {valid_loss}',\n","              f'R2: {r2val}')\n","        # after printing reset total/epoch_loss\n","        # epoch_loss = 0\n","\n","\n","\n","# train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, device, BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"qppsfyXPMtBo"},"source":["# Run"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1670562445708,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"wwponmNbbgcv","outputId":"0ad96c5f-99b3-4554-ea39-086415603a7d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":23}],"source":["# use GPU \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"21GGDl7VnORt","executionInfo":{"status":"ok","timestamp":1670562447275,"user_tz":480,"elapsed":3,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["# Hyperparameters\n","BATCH_SIZE = 64\n","LEARNING_RATE = 0.0001\n","NUM_EPOCHS = 50\n","WEIGHT_DECAY = 0.001"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1670562452802,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"3trJVAaSfuJh","outputId":"dfcaaf9f-a3c9-4baf-d7d9-334d8cbb04d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Preparing the dataloader\n"]}],"source":["print('Preparing the dataloader')\n","df = dhsid_df.sample(frac=1, random_state = 1234) # shuffle dataset \n","# original random split\n","train_df, val_df, test_df = split_train_val_test(df)\n","\n","# country-based split\n","# testlist = ['AM']\n","# vallist = ['BJ']\n","\n","# train_df, val_df, test_df = split_train_val_test2(df, testlist, vallist)\n","\n","# load images\n","train_imgs = DHSIDDataset(train_df)\n","val_imgs = DHSIDDataset(val_df)\n","test_imgs = DHSIDDataset(test_df)\n","# DataLoader\n","\n","train_loader = DataLoader(train_imgs, batch_size=1, num_workers=2) # always set batch_size = 1 here \n","val_loader = DataLoader(val_imgs, batch_size=1, num_workers=2)\n","test_loader = DataLoader(test_imgs, batch_size=1, num_workers=2)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1670562454524,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"8wunM-fUK2pM","outputId":"0facc65c-3d39-412a-8461-ccae43177c7c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    1325.000000\n","mean        8.419713\n","std         3.194714\n","min         0.051282\n","25%         6.166667\n","50%         8.807692\n","75%        11.000000\n","max        15.318182\n","Name: women_edu, dtype: float64"]},"metadata":{},"execution_count":26}],"source":["train_df['women_edu'].describe() "]},{"cell_type":"code","execution_count":31,"metadata":{"id":"X8TSor1VMSob","executionInfo":{"status":"ok","timestamp":1670562832580,"user_tz":480,"elapsed":790,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["model = SatelliteStreetModel('resnet18', 'resnet50') # resnet18 is the best model for satellite model \n","model.to(device)\n","criterion = torch.nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":336,"status":"ok","timestamp":1670562836679,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"f1MOXZAbfj-Y","outputId":"2e772a24-3360-4762-ce6b-a6621e5e7b5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["1325 284\n"]}],"source":["print(len(train_imgs), len(test_imgs))"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YhmJO2-QAz2I","executionInfo":{"status":"ok","timestamp":1670567180656,"user_tz":480,"elapsed":4343068,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}},"outputId":"0643ca4a-3139-4ee8-cbd1-d7d11b9290b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/50], Step [64/1325], Loss: 60.6710 R2: -6.077081576802937\n","Epoch [1/50], Step [128/1325], Loss: 80.5163 R2: -8.718463668924326\n","Epoch [1/50], Step [192/1325], Loss: 75.0597 R2: -6.013768341317616\n","Epoch [1/50], Step [256/1325], Loss: 61.1638 R2: -4.3678604105673715\n","Epoch [1/50], Step [320/1325], Loss: 78.1909 R2: -9.06241359470441\n","Epoch [1/50], Step [384/1325], Loss: 80.9656 R2: -5.5531521233169245\n","Epoch [1/50], Step [448/1325], Loss: 73.7554 R2: -5.049673260824521\n","Epoch [1/50], Step [512/1325], Loss: 73.2012 R2: -6.745474128041803\n","Epoch [1/50], Step [576/1325], Loss: 72.1021 R2: -5.923017267179819\n","Epoch [1/50], Step [640/1325], Loss: 65.4810 R2: -4.468235937583124\n","Epoch [1/50], Step [704/1325], Loss: 58.8813 R2: -5.072498627821519\n","Epoch [1/50], Step [768/1325], Loss: 54.9883 R2: -4.009934908382535\n","Epoch [1/50], Step [832/1325], Loss: 54.3744 R2: -4.692226734102757\n","Epoch [1/50], Step [896/1325], Loss: 40.5897 R2: -2.8857375138712373\n","Epoch [1/50], Step [960/1325], Loss: 43.7388 R2: -3.1638192494929473\n","Epoch [1/50], Step [1024/1325], Loss: 31.7956 R2: -2.4790623160244283\n","Epoch [1/50], Step [1088/1325], Loss: 22.9686 R2: -1.3958293292069635\n","Epoch [1/50], Step [1152/1325], Loss: 17.3878 R2: -0.6142291542555576\n","Epoch [1/50], Step [1216/1325], Loss: 12.5410 R2: -0.27754436225507173\n","Epoch [1/50], Step [1280/1325], Loss: 8.0568 R2: -0.004339191226080175\n","Epoch: 1/50.. Epoch Loss: 51.927590548337925.. Validation Loss: 17.349185469043526 R2: -0.4834427596416013\n","Epoch [2/50], Step [64/1325], Loss: 19.3326 R2: -1.2550796588539992\n","Epoch [2/50], Step [128/1325], Loss: 15.9535 R2: -0.9256213246148222\n","Epoch [2/50], Step [192/1325], Loss: 22.5284 R2: -1.1051095928637826\n","Epoch [2/50], Step [256/1325], Loss: 27.6682 R2: -1.42821543943053\n","Epoch [2/50], Step [320/1325], Loss: 11.3235 R2: -0.457219690498488\n","Epoch [2/50], Step [384/1325], Loss: 13.5335 R2: -0.09536748392450867\n","Epoch [2/50], Step [448/1325], Loss: 12.5360 R2: -0.02824232905249424\n","Epoch [2/50], Step [512/1325], Loss: 9.5378 R2: -0.009204819171650946\n","Epoch [2/50], Step [576/1325], Loss: 11.3305 R2: -0.08791872921894872\n","Epoch [2/50], Step [640/1325], Loss: 13.1120 R2: -0.09496473696241492\n","Epoch [2/50], Step [704/1325], Loss: 11.3888 R2: -0.17454233656213014\n","Epoch [2/50], Step [768/1325], Loss: 12.9656 R2: -0.1812820743210255\n","Epoch [2/50], Step [832/1325], Loss: 13.3794 R2: -0.40063648804722884\n","Epoch [2/50], Step [896/1325], Loss: 11.8883 R2: -0.13809351546047965\n","Epoch [2/50], Step [960/1325], Loss: 14.3299 R2: -0.36416551611719417\n","Epoch [2/50], Step [1024/1325], Loss: 11.3367 R2: -0.24046131366620904\n","Epoch [2/50], Step [1088/1325], Loss: 10.5654 R2: -0.10206043018635169\n","Epoch [2/50], Step [1152/1325], Loss: 11.2133 R2: -0.041013756881813634\n","Epoch [2/50], Step [1216/1325], Loss: 10.3762 R2: -0.057020298335110464\n","Epoch [2/50], Step [1280/1325], Loss: 7.9648 R2: 0.0071282782600478045\n","Epoch: 2/50.. Epoch Loss: 13.427082004131803.. Validation Loss: 12.156205558585343 R2: -0.039416814065692085\n","Epoch [3/50], Step [64/1325], Loss: 11.0266 R2: -0.2862177150100571\n","Epoch [3/50], Step [128/1325], Loss: 8.4132 R2: -0.015492045673564325\n","Epoch [3/50], Step [192/1325], Loss: 11.6485 R2: -0.08846865755165512\n","Epoch [3/50], Step [256/1325], Loss: 15.1458 R2: -0.3292223490115853\n","Epoch [3/50], Step [320/1325], Loss: 7.8165 R2: -0.005909192588427148\n","Epoch [3/50], Step [384/1325], Loss: 12.3193 R2: 0.002907021415600841\n","Epoch [3/50], Step [448/1325], Loss: 12.0910 R2: 0.008254716624123137\n","Epoch [3/50], Step [512/1325], Loss: 9.3388 R2: 0.011856910042241098\n","Epoch [3/50], Step [576/1325], Loss: 10.4339 R2: -0.0018326785959181802\n","Epoch [3/50], Step [640/1325], Loss: 11.8473 R2: 0.010648644510236172\n","Epoch [3/50], Step [704/1325], Loss: 9.6160 R2: 0.008294057673807531\n","Epoch [3/50], Step [768/1325], Loss: 10.8871 R2: 0.008084325352805122\n","Epoch [3/50], Step [832/1325], Loss: 9.7787 R2: -0.023686528652539174\n","Epoch [3/50], Step [896/1325], Loss: 10.2591 R2: 0.01787216627281485\n","Epoch [3/50], Step [960/1325], Loss: 10.7649 R2: -0.024794242696315028\n","Epoch [3/50], Step [1024/1325], Loss: 9.0073 R2: 0.014426611102759668\n","Epoch [3/50], Step [1088/1325], Loss: 9.2146 R2: 0.03883894409315469\n","Epoch [3/50], Step [1152/1325], Loss: 10.2680 R2: 0.046748239411634196\n","Epoch [3/50], Step [1216/1325], Loss: 9.3391 R2: 0.04863475950785112\n","Epoch [3/50], Step [1280/1325], Loss: 7.5699 R2: 0.05635075076907459\n","Epoch: 3/50.. Epoch Loss: 10.258218361433803.. Validation Loss: 11.908142997106065 R2: -0.018206212774211084\n","Epoch [4/50], Step [64/1325], Loss: 10.6310 R2: -0.24007370356945845\n","Epoch [4/50], Step [128/1325], Loss: 7.7840 R2: 0.060459022712606614\n","Epoch [4/50], Step [192/1325], Loss: 10.1823 R2: 0.04853621288343224\n","Epoch [4/50], Step [256/1325], Loss: 12.0026 R2: -0.05337468930958722\n","Epoch [4/50], Step [320/1325], Loss: 7.2226 R2: 0.07051842008030829\n","Epoch [4/50], Step [384/1325], Loss: 11.6854 R2: 0.05421548444473223\n","Epoch [4/50], Step [448/1325], Loss: 11.0882 R2: 0.09051139182773504\n","Epoch [4/50], Step [512/1325], Loss: 8.8379 R2: 0.06485959554255061\n","Epoch [4/50], Step [576/1325], Loss: 9.6086 R2: 0.07741511049903682\n","Epoch [4/50], Step [640/1325], Loss: 10.6837 R2: 0.10781721909711861\n","Epoch [4/50], Step [704/1325], Loss: 8.6262 R2: 0.11036482502195522\n","Epoch [4/50], Step [768/1325], Loss: 9.5803 R2: 0.12715197017620783\n","Epoch [4/50], Step [832/1325], Loss: 8.0169 R2: 0.16074220515089077\n","Epoch [4/50], Step [896/1325], Loss: 9.2551 R2: 0.11398475157577215\n","Epoch [4/50], Step [960/1325], Loss: 8.7384 R2: 0.1681287256726891\n","Epoch [4/50], Step [1024/1325], Loss: 7.4670 R2: 0.1829659634516434\n","Epoch [4/50], Step [1088/1325], Loss: 7.6711 R2: 0.1998370617369598\n","Epoch [4/50], Step [1152/1325], Loss: 8.0842 R2: 0.24948965695098868\n","Epoch [4/50], Step [1216/1325], Loss: 7.2855 R2: 0.25783425910978564\n","Epoch [4/50], Step [1280/1325], Loss: 6.1333 R2: 0.23543344946783085\n","Epoch: 4/50.. Epoch Loss: 8.947902486220555.. Validation Loss: 9.986552812105261 R2: 0.14609942431259282\n","Epoch [5/50], Step [64/1325], Loss: 8.5015 R2: 0.00832471614827246\n","Epoch [5/50], Step [128/1325], Loss: 6.0912 R2: 0.26477918249293153\n","Epoch [5/50], Step [192/1325], Loss: 7.5635 R2: 0.29324633865159155\n","Epoch [5/50], Step [256/1325], Loss: 8.3953 R2: 0.2632110018793802\n","Epoch [5/50], Step [320/1325], Loss: 5.4657 R2: 0.29661116138708676\n","Epoch [5/50], Step [384/1325], Loss: 7.7471 R2: 0.3729670915543335\n","Epoch [5/50], Step [448/1325], Loss: 6.9088 R2: 0.4333189810088379\n","Epoch [5/50], Step [512/1325], Loss: 5.3231 R2: 0.43675989741476173\n","Epoch [5/50], Step [576/1325], Loss: 6.2307 R2: 0.4017469051910779\n","Epoch [5/50], Step [640/1325], Loss: 6.8693 R2: 0.42635191794442273\n","Epoch [5/50], Step [704/1325], Loss: 5.1688 R2: 0.46693329397037153\n","Epoch [5/50], Step [768/1325], Loss: 5.6354 R2: 0.4865598649590215\n","Epoch [5/50], Step [832/1325], Loss: 4.9759 R2: 0.4790949435605626\n","Epoch [5/50], Step [896/1325], Loss: 5.6997 R2: 0.45435669217029173\n","Epoch [5/50], Step [960/1325], Loss: 4.4098 R2: 0.5801954969480121\n","Epoch [5/50], Step [1024/1325], Loss: 4.0686 R2: 0.5548160597310856\n","Epoch [5/50], Step [1088/1325], Loss: 3.8550 R2: 0.5978861206857538\n","Epoch [5/50], Step [1152/1325], Loss: 3.5124 R2: 0.6739152747988213\n","Epoch [5/50], Step [1216/1325], Loss: 3.5187 R2: 0.6415486069536627\n","Epoch [5/50], Step [1280/1325], Loss: 3.5128 R2: 0.5621011206789042\n","Epoch: 5/50.. Epoch Loss: 5.643024806568102.. Validation Loss: 7.293302764531928 R2: 0.3763858731562859\n","Epoch [6/50], Step [64/1325], Loss: 3.8792 R2: 0.5474996255379492\n","Epoch [6/50], Step [128/1325], Loss: 2.9123 R2: 0.648475465535022\n","Epoch [6/50], Step [192/1325], Loss: 3.6514 R2: 0.6588035570872282\n","Epoch [6/50], Step [256/1325], Loss: 4.0456 R2: 0.6449461240820844\n","Epoch [6/50], Step [320/1325], Loss: 1.8365 R2: 0.7636643873680414\n","Epoch [6/50], Step [384/1325], Loss: 3.3586 R2: 0.7281609837957985\n","Epoch [6/50], Step [448/1325], Loss: 3.3399 R2: 0.7260462555424505\n","Epoch [6/50], Step [512/1325], Loss: 2.5037 R2: 0.7350815395943023\n","Epoch [6/50], Step [576/1325], Loss: 2.8813 R2: 0.7233453509028062\n","Epoch [6/50], Step [640/1325], Loss: 2.9718 R2: 0.7518301116886328\n","Epoch [6/50], Step [704/1325], Loss: 2.9597 R2: 0.6947646465008801\n","Epoch [6/50], Step [768/1325], Loss: 2.8343 R2: 0.7417664893634645\n","Epoch [6/50], Step [832/1325], Loss: 2.0246 R2: 0.7880508784403738\n","Epoch [6/50], Step [896/1325], Loss: 2.3706 R2: 0.7730527387799526\n","Epoch [6/50], Step [960/1325], Loss: 2.9041 R2: 0.7235361433754358\n","Epoch [6/50], Step [1024/1325], Loss: 3.1850 R2: 0.651502718957246\n","Epoch [6/50], Step [1088/1325], Loss: 2.0111 R2: 0.7902292635001076\n","Epoch [6/50], Step [1152/1325], Loss: 1.7664 R2: 0.8360147618969599\n","Epoch [6/50], Step [1216/1325], Loss: 2.6359 R2: 0.731483685341732\n","Epoch [6/50], Step [1280/1325], Loss: 2.6331 R2: 0.6717588604189353\n","Epoch: 6/50.. Epoch Loss: 2.9173586548783508.. Validation Loss: 8.701793620031667 R2: 0.25595279983115216\n","Epoch [7/50], Step [64/1325], Loss: 2.3504 R2: 0.7258285109168797\n","Epoch [7/50], Step [128/1325], Loss: 1.3287 R2: 0.8396195479278485\n","Epoch [7/50], Step [192/1325], Loss: 2.1967 R2: 0.7947352974595521\n","Epoch [7/50], Step [256/1325], Loss: 2.7992 R2: 0.7543367612181513\n","Epoch [7/50], Step [320/1325], Loss: 1.3873 R2: 0.821472624822717\n","Epoch [7/50], Step [384/1325], Loss: 2.1224 R2: 0.8282179437762692\n","Epoch [7/50], Step [448/1325], Loss: 2.1834 R2: 0.8209060468584644\n","Epoch [7/50], Step [512/1325], Loss: 1.7305 R2: 0.8168926160262936\n","Epoch [7/50], Step [576/1325], Loss: 1.8104 R2: 0.8261732123793355\n","Epoch [7/50], Step [640/1325], Loss: 1.8321 R2: 0.8469997387255038\n","Epoch [7/50], Step [704/1325], Loss: 1.7748 R2: 0.8169667974324275\n","Epoch [7/50], Step [768/1325], Loss: 1.7225 R2: 0.843060674668819\n","Epoch [7/50], Step [832/1325], Loss: 1.2379 R2: 0.870405633391115\n","Epoch [7/50], Step [896/1325], Loss: 1.7170 R2: 0.8356294561470747\n","Epoch [7/50], Step [960/1325], Loss: 1.8536 R2: 0.8235385152213319\n","Epoch [7/50], Step [1024/1325], Loss: 1.4395 R2: 0.8424955993781775\n","Epoch [7/50], Step [1088/1325], Loss: 1.3212 R2: 0.8621878118619916\n","Epoch [7/50], Step [1152/1325], Loss: 2.2325 R2: 0.7927383911949439\n","Epoch [7/50], Step [1216/1325], Loss: 2.1574 R2: 0.7802310435863391\n","Epoch [7/50], Step [1280/1325], Loss: 1.9734 R2: 0.7540068875774619\n","Epoch: 7/50.. Epoch Loss: 1.9526154706040983.. Validation Loss: 7.99708661171001 R2: 0.3162088074977085\n","Epoch [8/50], Step [64/1325], Loss: 1.4442 R2: 0.8315356043489892\n","Epoch [8/50], Step [128/1325], Loss: 2.7735 R2: 0.6652359246601232\n","Epoch [8/50], Step [192/1325], Loss: 2.9023 R2: 0.7288027649501955\n","Epoch [8/50], Step [256/1325], Loss: 2.9114 R2: 0.7444918064356887\n","Epoch [8/50], Step [320/1325], Loss: 1.0380 R2: 0.8664207271231853\n","Epoch [8/50], Step [384/1325], Loss: 2.1443 R2: 0.8264493742032089\n","Epoch [8/50], Step [448/1325], Loss: 3.0547 R2: 0.7494441367184473\n","Epoch [8/50], Step [512/1325], Loss: 3.6515 R2: 0.613631492483906\n","Epoch [8/50], Step [576/1325], Loss: 2.8623 R2: 0.725166863971975\n","Epoch [8/50], Step [640/1325], Loss: 1.8275 R2: 0.8473903884471828\n","Epoch [8/50], Step [704/1325], Loss: 1.1829 R2: 0.8780031169946719\n","Epoch [8/50], Step [768/1325], Loss: 2.6063 R2: 0.7625388290401784\n","Epoch [8/50], Step [832/1325], Loss: 2.5510 R2: 0.7329451167418419\n","Epoch [8/50], Step [896/1325], Loss: 2.8126 R2: 0.7307399037651445\n","Epoch [8/50], Step [960/1325], Loss: 1.8910 R2: 0.819980138241959\n","Epoch [8/50], Step [1024/1325], Loss: 1.3117 R2: 0.8564759380108122\n","Epoch [8/50], Step [1088/1325], Loss: 1.2219 R2: 0.8725403605407492\n","Epoch [8/50], Step [1152/1325], Loss: 1.8180 R2: 0.831225896753766\n","Epoch [8/50], Step [1216/1325], Loss: 3.0937 R2: 0.6848423915746076\n","Epoch [8/50], Step [1280/1325], Loss: 3.1204 R2: 0.6110224789142409\n","Epoch: 8/50.. Epoch Loss: 2.389154161686093.. Validation Loss: 7.505385557625427 R2: 0.3582517210501438\n","Epoch [9/50], Step [64/1325], Loss: 2.7713 R2: 0.6767411267252262\n","Epoch [9/50], Step [128/1325], Loss: 2.7730 R2: 0.6652981704755685\n","Epoch [9/50], Step [192/1325], Loss: 1.5055 R2: 0.8593196678494004\n","Epoch [9/50], Step [256/1325], Loss: 2.5847 R2: 0.773164521879576\n","Epoch [9/50], Step [320/1325], Loss: 3.9322 R2: 0.4939623468817108\n","Epoch [9/50], Step [384/1325], Loss: 5.1205 R2: 0.585561333371361\n","Epoch [9/50], Step [448/1325], Loss: 2.8580 R2: 0.7655759072804229\n","Epoch [9/50], Step [512/1325], Loss: 1.5580 R2: 0.8351483088537215\n","Epoch [9/50], Step [576/1325], Loss: 1.7812 R2: 0.8289793828782569\n","Epoch [9/50], Step [640/1325], Loss: 2.8727 R2: 0.7601087915940626\n","Epoch [9/50], Step [704/1325], Loss: 3.9168 R2: 0.5960536702649089\n","Epoch [9/50], Step [768/1325], Loss: 5.2383 R2: 0.5227466539001353\n","Epoch [9/50], Step [832/1325], Loss: 4.4691 R2: 0.5321458224198948\n","Epoch [9/50], Step [896/1325], Loss: 2.3950 R2: 0.7707214301975602\n","Epoch [9/50], Step [960/1325], Loss: 1.3659 R2: 0.8699729557108171\n","Epoch [9/50], Step [1024/1325], Loss: 1.7529 R2: 0.8081951660826192\n","Epoch [9/50], Step [1088/1325], Loss: 3.8433 R2: 0.5991046664076721\n","Epoch [9/50], Step [1152/1325], Loss: 4.3880 R2: 0.5926336011666631\n","Epoch [9/50], Step [1216/1325], Loss: 2.6503 R2: 0.7300174076226185\n","Epoch [9/50], Step [1280/1325], Loss: 1.6639 R2: 0.7925878588503401\n","Epoch: 9/50.. Epoch Loss: 3.0143844156398454.. Validation Loss: 7.3385915329139175 R2: 0.372513448785854\n","Epoch [10/50], Step [64/1325], Loss: 1.5507 R2: 0.8191116611706388\n","Epoch [10/50], Step [128/1325], Loss: 1.9817 R2: 0.7608007379552642\n","Epoch [10/50], Step [192/1325], Loss: 2.3935 R2: 0.7763457471921802\n","Epoch [10/50], Step [256/1325], Loss: 2.9828 R2: 0.7382205941543625\n","Epoch [10/50], Step [320/1325], Loss: 3.1974 R2: 0.5885297092686952\n","Epoch [10/50], Step [384/1325], Loss: 2.8708 R2: 0.7676438196401872\n","Epoch [10/50], Step [448/1325], Loss: 1.7460 R2: 0.8567876132244676\n","Epoch [10/50], Step [512/1325], Loss: 1.2309 R2: 0.8697571026431969\n","Epoch [10/50], Step [576/1325], Loss: 1.6718 R2: 0.8394759348293455\n","Epoch [10/50], Step [640/1325], Loss: 2.7435 R2: 0.7708916436051773\n","Epoch [10/50], Step [704/1325], Loss: 1.4104 R2: 0.8545477573192448\n","Epoch [10/50], Step [768/1325], Loss: 1.6396 R2: 0.8506179585399908\n","Epoch [10/50], Step [832/1325], Loss: 1.1897 R2: 0.8754514991928155\n","Epoch [10/50], Step [896/1325], Loss: 1.1288 R2: 0.8919394153297474\n","Epoch [10/50], Step [960/1325], Loss: 1.1392 R2: 0.8915521183750416\n","Epoch [10/50], Step [1024/1325], Loss: 1.3350 R2: 0.8539278324061702\n","Epoch [10/50], Step [1088/1325], Loss: 1.5895 R2: 0.8342050581165928\n","Epoch [10/50], Step [1152/1325], Loss: 1.2564 R2: 0.883359663231247\n","Epoch [10/50], Step [1216/1325], Loss: 1.1074 R2: 0.8871872742468311\n","Epoch [10/50], Step [1280/1325], Loss: 0.7421 R2: 0.9074948541043492\n","Epoch: 10/50.. Epoch Loss: 1.8315785824126654.. Validation Loss: 7.659546582300157 R2: 0.3450701790954457\n","Epoch [11/50], Step [64/1325], Loss: 1.2172 R2: 0.8580155152286216\n","Epoch [11/50], Step [128/1325], Loss: 1.2533 R2: 0.8487278277397545\n","Epoch [11/50], Step [192/1325], Loss: 1.2341 R2: 0.8846798629290549\n","Epoch [11/50], Step [256/1325], Loss: 1.4493 R2: 0.872805304677488\n","Epoch [11/50], Step [320/1325], Loss: 0.8343 R2: 0.8926312973736509\n","Epoch [11/50], Step [384/1325], Loss: 1.4529 R2: 0.8824071723520075\n","Epoch [11/50], Step [448/1325], Loss: 1.2784 R2: 0.8951447410982245\n","Epoch [11/50], Step [512/1325], Loss: 0.9445 R2: 0.9000582413214211\n","Epoch [11/50], Step [576/1325], Loss: 1.1690 R2: 0.8877602307119565\n","Epoch [11/50], Step [640/1325], Loss: 1.3929 R2: 0.8836780670830341\n","Epoch [11/50], Step [704/1325], Loss: 0.7027 R2: 0.9275301097908771\n","Epoch [11/50], Step [768/1325], Loss: 1.0265 R2: 0.9064796537117352\n","Epoch [11/50], Step [832/1325], Loss: 0.8686 R2: 0.9090703487304553\n","Epoch [11/50], Step [896/1325], Loss: 0.9506 R2: 0.9089970839555117\n","Epoch [11/50], Step [960/1325], Loss: 1.0038 R2: 0.9044435948928551\n","Epoch [11/50], Step [1024/1325], Loss: 0.8149 R2: 0.9108350762546487\n","Epoch [11/50], Step [1088/1325], Loss: 0.8266 R2: 0.9137807590685487\n","Epoch [11/50], Step [1152/1325], Loss: 0.8307 R2: 0.9228790696698483\n","Epoch [11/50], Step [1216/1325], Loss: 0.9041 R2: 0.9078972097602169\n","Epoch [11/50], Step [1280/1325], Loss: 0.4379 R2: 0.9454118914500401\n","Epoch: 11/50.. Epoch Loss: 1.127477361863195.. Validation Loss: 6.641789335000077 R2: 0.4320935471543417\n","Epoch [12/50], Step [64/1325], Loss: 0.9014 R2: 0.8948500737327754\n","Epoch [12/50], Step [128/1325], Loss: 0.7930 R2: 0.9042805575742369\n","Epoch [12/50], Step [192/1325], Loss: 0.8059 R2: 0.9246910660297489\n","Epoch [12/50], Step [256/1325], Loss: 1.1179 R2: 0.901889661906056\n","Epoch [12/50], Step [320/1325], Loss: 0.5335 R2: 0.9313412572044225\n","Epoch [12/50], Step [384/1325], Loss: 1.1853 R2: 0.904063430732981\n","Epoch [12/50], Step [448/1325], Loss: 1.0692 R2: 0.9123031408106786\n","Epoch [12/50], Step [512/1325], Loss: 0.8298 R2: 0.9122013887801245\n","Epoch [12/50], Step [576/1325], Loss: 0.9053 R2: 0.9130797921020662\n","Epoch [12/50], Step [640/1325], Loss: 1.0348 R2: 0.9135853226325884\n","Epoch [12/50], Step [704/1325], Loss: 0.5400 R2: 0.9443059619756214\n","Epoch [12/50], Step [768/1325], Loss: 0.9399 R2: 0.9143709872257959\n","Epoch [12/50], Step [832/1325], Loss: 1.0042 R2: 0.8948747991602086\n","Epoch [12/50], Step [896/1325], Loss: 0.6793 R2: 0.9349708370664386\n","Epoch [12/50], Step [960/1325], Loss: 0.8074 R2: 0.9231357203694623\n","Epoch [12/50], Step [1024/1325], Loss: 0.5206 R2: 0.9430409276213055\n","Epoch [12/50], Step [1088/1325], Loss: 0.6134 R2: 0.9360198387863108\n","Epoch [12/50], Step [1152/1325], Loss: 0.8166 R2: 0.9241885722856151\n","Epoch [12/50], Step [1216/1325], Loss: 1.0549 R2: 0.8925333832475666\n","Epoch [12/50], Step [1280/1325], Loss: 0.6163 R2: 0.9231747152874136\n","Epoch: 12/50.. Epoch Loss: 0.9512532615286325.. Validation Loss: 7.621852594639036 R2: 0.34829320138684816\n","Epoch [13/50], Step [64/1325], Loss: 0.8682 R2: 0.8987283716887805\n","Epoch [13/50], Step [128/1325], Loss: 0.6022 R2: 0.9273159437196665\n","Epoch [13/50], Step [192/1325], Loss: 0.6654 R2: 0.9378200365541449\n","Epoch [13/50], Step [256/1325], Loss: 1.0504 R2: 0.9078156260266684\n","Epoch [13/50], Step [320/1325], Loss: 0.8310 R2: 0.8930570446191126\n","Epoch [13/50], Step [384/1325], Loss: 1.1138 R2: 0.9098499782389264\n","Epoch [13/50], Step [448/1325], Loss: 0.9457 R2: 0.9224279307069179\n","Epoch [13/50], Step [512/1325], Loss: 0.7345 R2: 0.9222777349608208\n","Epoch [13/50], Step [576/1325], Loss: 0.7857 R2: 0.9245598068207027\n","Epoch [13/50], Step [640/1325], Loss: 0.8131 R2: 0.932101585652635\n","Epoch [13/50], Step [704/1325], Loss: 0.5225 R2: 0.9461091784511182\n","Epoch [13/50], Step [768/1325], Loss: 0.8887 R2: 0.9190309257605311\n","Epoch [13/50], Step [832/1325], Loss: 0.6039 R2: 0.9367820483108487\n","Epoch [13/50], Step [896/1325], Loss: 0.5686 R2: 0.9455683891657418\n","Epoch [13/50], Step [960/1325], Loss: 0.6057 R2: 0.9423379839457112\n","Epoch [13/50], Step [1024/1325], Loss: 0.5531 R2: 0.9394800227917758\n","Epoch [13/50], Step [1088/1325], Loss: 0.7153 R2: 0.9253880860141411\n","Epoch [13/50], Step [1152/1325], Loss: 0.7816 R2: 0.9274375954459155\n","Epoch [13/50], Step [1216/1325], Loss: 0.8317 R2: 0.9152796128084543\n","Epoch [13/50], Step [1280/1325], Loss: 0.3976 R2: 0.950432960700611\n","Epoch: 13/50.. Epoch Loss: 0.8644079027319111.. Validation Loss: 7.654554147156169 R2: 0.34549705827728394\n","Epoch [14/50], Step [64/1325], Loss: 0.6887 R2: 0.9196693640493584\n","Epoch [14/50], Step [128/1325], Loss: 0.9657 R2: 0.8834401339065763\n","Epoch [14/50], Step [192/1325], Loss: 1.1354 R2: 0.8939099502556338\n","Epoch [14/50], Step [256/1325], Loss: 1.2635 R2: 0.8891150647296058\n","Epoch [14/50], Step [320/1325], Loss: 0.3792 R2: 0.9512046444160164\n","Epoch [14/50], Step [384/1325], Loss: 0.8421 R2: 0.931839594491921\n","Epoch [14/50], Step [448/1325], Loss: 0.9087 R2: 0.9254657842213448\n","Epoch [14/50], Step [512/1325], Loss: 0.8371 R2: 0.9114206137716729\n","Epoch [14/50], Step [576/1325], Loss: 1.1665 R2: 0.887992871606093\n","Epoch [14/50], Step [640/1325], Loss: 1.1489 R2: 0.9040595836050145\n","Epoch [14/50], Step [704/1325], Loss: 0.5503 R2: 0.9432484009376914\n","Epoch [14/50], Step [768/1325], Loss: 0.6608 R2: 0.9397956025699123\n","Epoch [14/50], Step [832/1325], Loss: 0.4793 R2: 0.9498225535310378\n","Epoch [14/50], Step [896/1325], Loss: 0.7261 R2: 0.9304861855181394\n","Epoch [14/50], Step [960/1325], Loss: 0.8254 R2: 0.9214272679988333\n","Epoch [14/50], Step [1024/1325], Loss: 0.9390 R2: 0.8972551702084375\n","Epoch [14/50], Step [1088/1325], Loss: 0.6286 R2: 0.9344340456685397\n","Epoch [14/50], Step [1152/1325], Loss: 0.5590 R2: 0.9481014784563084\n","Epoch [14/50], Step [1216/1325], Loss: 0.7445 R2: 0.9241622555606047\n","Epoch [14/50], Step [1280/1325], Loss: 0.5161 R2: 0.93566776992255\n","Epoch: 14/50.. Epoch Loss: 0.9014218754591496.. Validation Loss: 6.679663230679081 R2: 0.4288551421202682\n","Epoch [15/50], Step [64/1325], Loss: 0.8663 R2: 0.8989522737031661\n","Epoch [15/50], Step [128/1325], Loss: 0.7991 R2: 0.9035497831974539\n","Epoch [15/50], Step [192/1325], Loss: 0.6082 R2: 0.943168594745059\n","Epoch [15/50], Step [256/1325], Loss: 0.8769 R2: 0.9230412090336575\n","Epoch [15/50], Step [320/1325], Loss: 0.4323 R2: 0.9443668234074384\n","Epoch [15/50], Step [384/1325], Loss: 1.0683 R2: 0.91353740470037\n","Epoch [15/50], Step [448/1325], Loss: 1.1457 R2: 0.9060224629959799\n","Epoch [15/50], Step [512/1325], Loss: 0.9838 R2: 0.895902213469463\n","Epoch [15/50], Step [576/1325], Loss: 0.9167 R2: 0.9119797398868338\n","Epoch [15/50], Step [640/1325], Loss: 0.9717 R2: 0.9188532096883986\n","Epoch [15/50], Step [704/1325], Loss: 0.7402 R2: 0.923658551344896\n","Epoch [15/50], Step [768/1325], Loss: 1.1686 R2: 0.8935330493654102\n","Epoch [15/50], Step [832/1325], Loss: 1.4676 R2: 0.8463675477948853\n","Epoch [15/50], Step [896/1325], Loss: 1.0748 R2: 0.8971098248303985\n","Epoch [15/50], Step [960/1325], Loss: 0.8586 R2: 0.9182666237805526\n","Epoch [15/50], Step [1024/1325], Loss: 0.4919 R2: 0.9461765770859398\n","Epoch [15/50], Step [1088/1325], Loss: 0.5940 R2: 0.9380423682849977\n","Epoch [15/50], Step [1152/1325], Loss: 1.2394 R2: 0.8849385389672996\n","Epoch [15/50], Step [1216/1325], Loss: 1.8314 R2: 0.8134377910382402\n","Epoch [15/50], Step [1280/1325], Loss: 1.1369 R2: 0.8582796108458816\n","Epoch: 15/50.. Epoch Loss: 1.0919752469964834.. Validation Loss: 8.386765076034939 R2: 0.28288932792686994\n","Epoch [16/50], Step [64/1325], Loss: 1.0073 R2: 0.8825057889717632\n","Epoch [16/50], Step [128/1325], Loss: 0.4764 R2: 0.9425023429820959\n","Epoch [16/50], Step [192/1325], Loss: 0.8619 R2: 0.919457598835349\n","Epoch [16/50], Step [256/1325], Loss: 1.3867 R2: 0.8783020012265625\n","Epoch [16/50], Step [320/1325], Loss: 1.4237 R2: 0.8167805395452372\n","Epoch [16/50], Step [384/1325], Loss: 1.3870 R2: 0.8877412611160014\n","Epoch [16/50], Step [448/1325], Loss: 1.0410 R2: 0.914612250580611\n","Epoch [16/50], Step [512/1325], Loss: 0.6321 R2: 0.9331124637061843\n","Epoch [16/50], Step [576/1325], Loss: 0.7957 R2: 0.9235992823730189\n","Epoch [16/50], Step [640/1325], Loss: 0.9295 R2: 0.922375975075275\n","Epoch [16/50], Step [704/1325], Loss: 0.8294 R2: 0.9144645817329199\n","Epoch [16/50], Step [768/1325], Loss: 1.0840 R2: 0.9012360765619434\n","Epoch [16/50], Step [832/1325], Loss: 0.6371 R2: 0.9333014552704477\n","Epoch [16/50], Step [896/1325], Loss: 0.4812 R2: 0.9539364051525199\n","Epoch [16/50], Step [960/1325], Loss: 0.7680 R2: 0.926885992282801\n","Epoch [16/50], Step [1024/1325], Loss: 0.9276 R2: 0.8985072987148488\n","Epoch [16/50], Step [1088/1325], Loss: 1.3268 R2: 0.8615997056536784\n","Epoch [16/50], Step [1152/1325], Loss: 1.1341 R2: 0.8947134294309711\n","Epoch [16/50], Step [1216/1325], Loss: 0.9737 R2: 0.9008093970133098\n","Epoch [16/50], Step [1280/1325], Loss: 0.3854 R2: 0.9519623011410017\n","Epoch: 16/50.. Epoch Loss: 1.0639356302205618.. Validation Loss: 8.38379130600052 R2: 0.2831435963708717\n","Epoch [17/50], Step [64/1325], Loss: 0.8469 R2: 0.9012164350223049\n","Epoch [17/50], Step [128/1325], Loss: 1.8499 R2: 0.7767081368944466\n","Epoch [17/50], Step [192/1325], Loss: 2.0002 R2: 0.813093071578685\n","Epoch [17/50], Step [256/1325], Loss: 1.4304 R2: 0.8744616482004945\n","Epoch [17/50], Step [320/1325], Loss: 0.4132 R2: 0.9468208084741027\n","Epoch [17/50], Step [384/1325], Loss: 0.8549 R2: 0.9308053827831098\n","Epoch [17/50], Step [448/1325], Loss: 1.1572 R2: 0.9050808449287157\n","Epoch [17/50], Step [512/1325], Loss: 1.3850 R2: 0.8534513224082242\n","Epoch [17/50], Step [576/1325], Loss: 1.9035 R2: 0.8172360637497409\n","Epoch [17/50], Step [640/1325], Loss: 1.5984 R2: 0.8665217799682751\n","Epoch [17/50], Step [704/1325], Loss: 0.7001 R2: 0.9278016530446552\n","Epoch [17/50], Step [768/1325], Loss: 0.5939 R2: 0.9458868383656669\n","Epoch [17/50], Step [832/1325], Loss: 0.6067 R2: 0.9364900551742794\n","Epoch [17/50], Step [896/1325], Loss: 1.3938 R2: 0.8665688032618031\n","Epoch [17/50], Step [960/1325], Loss: 1.2287 R2: 0.8830275960589711\n","Epoch [17/50], Step [1024/1325], Loss: 1.1253 R2: 0.8768698365098403\n","Epoch [17/50], Step [1088/1325], Loss: 0.5681 R2: 0.9407437862772232\n","Epoch [17/50], Step [1152/1325], Loss: 0.7615 R2: 0.9293032444870091\n","Epoch [17/50], Step [1216/1325], Loss: 1.1744 R2: 0.8803652493944139\n","Epoch [17/50], Step [1280/1325], Loss: 0.8677 R2: 0.8918329566464314\n","Epoch: 17/50.. Epoch Loss: 1.2189971423220138.. Validation Loss: 7.464770544113236 R2: 0.36172450868214623\n","Epoch [18/50], Step [64/1325], Loss: 0.8008 R2: 0.9065851198113837\n","Epoch [18/50], Step [128/1325], Loss: 0.6212 R2: 0.9250209971853293\n","Epoch [18/50], Step [192/1325], Loss: 0.5320 R2: 0.9502860108708269\n","Epoch [18/50], Step [256/1325], Loss: 0.9592 R2: 0.9158149270240193\n","Epoch [18/50], Step [320/1325], Loss: 0.7040 R2: 0.9094065634782025\n","Epoch [18/50], Step [384/1325], Loss: 1.3082 R2: 0.894114956365851\n","Epoch [18/50], Step [448/1325], Loss: 1.1254 R2: 0.9076908270788402\n","Epoch [18/50], Step [512/1325], Loss: 0.7044 R2: 0.9254684682775619\n","Epoch [18/50], Step [576/1325], Loss: 0.7372 R2: 0.9292135290389154\n","Epoch [18/50], Step [640/1325], Loss: 0.9876 R2: 0.9175286398616129\n","Epoch [18/50], Step [704/1325], Loss: 0.7928 R2: 0.9182366325194733\n","Epoch [18/50], Step [768/1325], Loss: 1.1895 R2: 0.8916277440446158\n","Epoch [18/50], Step [832/1325], Loss: 1.6513 R2: 0.8271366077259998\n","Epoch [18/50], Step [896/1325], Loss: 1.0591 R2: 0.8986082938098441\n","Epoch [18/50], Step [960/1325], Loss: 0.8156 R2: 0.9223599267678634\n","Epoch [18/50], Step [1024/1325], Loss: 0.4599 R2: 0.9496726704851645\n","Epoch [18/50], Step [1088/1325], Loss: 0.6671 R2: 0.9304147440716957\n","Epoch [18/50], Step [1152/1325], Loss: 1.1831 R2: 0.8901635289603744\n","Epoch [18/50], Step [1216/1325], Loss: 1.5729 R2: 0.8397695611138785\n","Epoch [18/50], Step [1280/1325], Loss: 0.7454 R2: 0.9070790907477829\n","Epoch: 18/50.. Epoch Loss: 1.064715784342412.. Validation Loss: 8.833737459757945 R2: 0.24467093856369226\n","Epoch [19/50], Step [64/1325], Loss: 0.6799 R2: 0.9206955987389217\n","Epoch [19/50], Step [128/1325], Loss: 0.3858 R2: 0.9534278790808931\n","Epoch [19/50], Step [192/1325], Loss: 0.6913 R2: 0.9354045144723457\n","Epoch [19/50], Step [256/1325], Loss: 1.1081 R2: 0.9027537016549319\n","Epoch [19/50], Step [320/1325], Loss: 0.9080 R2: 0.8831466067951401\n","Epoch [19/50], Step [384/1325], Loss: 1.0172 R2: 0.917668812417699\n","Epoch [19/50], Step [448/1325], Loss: 0.7135 R2: 0.9414788642755251\n","Epoch [19/50], Step [512/1325], Loss: 0.5375 R2: 0.9431230756867741\n","Epoch [19/50], Step [576/1325], Loss: 0.7897 R2: 0.9241733239613364\n","Epoch [19/50], Step [640/1325], Loss: 0.8906 R2: 0.9256271454839038\n","Epoch [19/50], Step [704/1325], Loss: 0.5809 R2: 0.9400950813377267\n","Epoch [19/50], Step [768/1325], Loss: 0.6380 R2: 0.94187642900544\n","Epoch [19/50], Step [832/1325], Loss: 0.5582 R2: 0.9415637076863949\n","Epoch [19/50], Step [896/1325], Loss: 0.4503 R2: 0.9568905259381606\n","Epoch [19/50], Step [960/1325], Loss: 0.7498 R2: 0.9286203339550217\n","Epoch [19/50], Step [1024/1325], Loss: 0.7181 R2: 0.9214272838114121\n","Epoch [19/50], Step [1088/1325], Loss: 0.7517 R2: 0.9215932466360187\n","Epoch [19/50], Step [1152/1325], Loss: 0.6644 R2: 0.9383179444044971\n","Epoch [19/50], Step [1216/1325], Loss: 0.6588 R2: 0.9328901443870647\n","Epoch [19/50], Step [1280/1325], Loss: 0.2536 R2: 0.9683846123731397\n","Epoch: 19/50.. Epoch Loss: 0.8462917782590595.. Validation Loss: 8.58997262481655 R2: 0.26551405830126085\n","Epoch [20/50], Step [64/1325], Loss: 0.6995 R2: 0.9184014424662018\n","Epoch [20/50], Step [128/1325], Loss: 1.0747 R2: 0.8702849287837078\n","Epoch [20/50], Step [192/1325], Loss: 1.3585 R2: 0.8730614692574341\n","Epoch [20/50], Step [256/1325], Loss: 1.0846 R2: 0.9048175833142148\n","Epoch [20/50], Step [320/1325], Loss: 0.3590 R2: 0.9537983145417736\n","Epoch [20/50], Step [384/1325], Loss: 0.6143 R2: 0.950281985645101\n","Epoch [20/50], Step [448/1325], Loss: 0.6877 R2: 0.9435910844547283\n","Epoch [20/50], Step [512/1325], Loss: 0.8157 R2: 0.9136921601365549\n","Epoch [20/50], Step [576/1325], Loss: 1.1772 R2: 0.8869674684747704\n","Epoch [20/50], Step [640/1325], Loss: 1.4109 R2: 0.8821776927923178\n","Epoch [20/50], Step [704/1325], Loss: 1.0017 R2: 0.8966970476311008\n","Epoch [20/50], Step [768/1325], Loss: 0.7296 R2: 0.9335235945959769\n","Epoch [20/50], Step [832/1325], Loss: 0.4806 R2: 0.9496870818703376\n","Epoch [20/50], Step [896/1325], Loss: 0.5323 R2: 0.9490388314502411\n","Epoch [20/50], Step [960/1325], Loss: 0.8716 R2: 0.9170228043340423\n","Epoch [20/50], Step [1024/1325], Loss: 0.8814 R2: 0.9035616673764817\n","Epoch [20/50], Step [1088/1325], Loss: 0.7797 R2: 0.9186691335905145\n","Epoch [20/50], Step [1152/1325], Loss: 0.4811 R2: 0.9553328974945837\n","Epoch [20/50], Step [1216/1325], Loss: 0.6668 R2: 0.932077756985785\n","Epoch [20/50], Step [1280/1325], Loss: 0.4854 R2: 0.9394868785054945\n","Epoch: 20/50.. Epoch Loss: 0.9087906244330622.. Validation Loss: 7.061038501107811 R2: 0.3962456358623113\n","Epoch [21/50], Step [64/1325], Loss: 0.7095 R2: 0.9172438498096059\n","Epoch [21/50], Step [128/1325], Loss: 0.8331 R2: 0.8994429434298762\n","Epoch [21/50], Step [192/1325], Loss: 0.5080 R2: 0.9525314398008489\n","Epoch [21/50], Step [256/1325], Loss: 0.7628 R2: 0.9330535931771681\n","Epoch [21/50], Step [320/1325], Loss: 0.4510 R2: 0.9419645530757257\n","Epoch [21/50], Step [384/1325], Loss: 1.0528 R2: 0.914791380564209\n","Epoch [21/50], Step [448/1325], Loss: 1.2413 R2: 0.898186443399571\n","Epoch [21/50], Step [512/1325], Loss: 0.9257 R2: 0.9020533642567875\n","Epoch [21/50], Step [576/1325], Loss: 0.6834 R2: 0.9343796698138105\n","Epoch [21/50], Step [640/1325], Loss: 0.8418 R2: 0.9297047105245326\n","Epoch [21/50], Step [704/1325], Loss: 0.6254 R2: 0.9355060997552767\n","Epoch [21/50], Step [768/1325], Loss: 0.7724 R2: 0.9296268880719117\n","Epoch [21/50], Step [832/1325], Loss: 1.0681 R2: 0.8881807261608952\n","Epoch [21/50], Step [896/1325], Loss: 0.7613 R2: 0.9271214216766167\n","Epoch [21/50], Step [960/1325], Loss: 0.9141 R2: 0.9129794611360622\n","Epoch [21/50], Step [1024/1325], Loss: 0.5063 R2: 0.9446009787403276\n","Epoch [21/50], Step [1088/1325], Loss: 0.4046 R2: 0.9577917390729634\n","Epoch [21/50], Step [1152/1325], Loss: 0.5894 R2: 0.9452826574311789\n","Epoch [21/50], Step [1216/1325], Loss: 1.0889 R2: 0.8890779536171212\n","Epoch [21/50], Step [1280/1325], Loss: 0.7696 R2: 0.9040637523071398\n","Epoch: 21/50.. Epoch Loss: 0.9334236391532827.. Validation Loss: 8.791662947369844 R2: 0.24826852871134986\n","Epoch [22/50], Step [64/1325], Loss: 0.8755 R2: 0.8978715387202701\n","Epoch [22/50], Step [128/1325], Loss: 0.6302 R2: 0.9239324322406645\n","Epoch [22/50], Step [192/1325], Loss: 0.5169 R2: 0.9517013190406824\n","Epoch [22/50], Step [256/1325], Loss: 0.7663 R2: 0.9327457337949107\n","Epoch [22/50], Step [320/1325], Loss: 0.5694 R2: 0.9267232456309156\n","Epoch [22/50], Step [384/1325], Loss: 0.8122 R2: 0.9342615898747347\n","Epoch [22/50], Step [448/1325], Loss: 0.8251 R2: 0.9323187297011931\n","Epoch [22/50], Step [512/1325], Loss: 0.6547 R2: 0.9307239803884422\n","Epoch [22/50], Step [576/1325], Loss: 0.6237 R2: 0.940114647649633\n","Epoch [22/50], Step [640/1325], Loss: 0.6593 R2: 0.9449462297969158\n","Epoch [22/50], Step [704/1325], Loss: 0.3662 R2: 0.962235395491447\n","Epoch [22/50], Step [768/1325], Loss: 0.6386 R2: 0.9418201717439284\n","Epoch [22/50], Step [832/1325], Loss: 0.5133 R2: 0.9462695382459088\n","Epoch [22/50], Step [896/1325], Loss: 0.2909 R2: 0.9721551253412308\n","Epoch [22/50], Step [960/1325], Loss: 0.4715 R2: 0.9551139946331428\n","Epoch [22/50], Step [1024/1325], Loss: 0.4182 R2: 0.9542391302004307\n","Epoch [22/50], Step [1088/1325], Loss: 0.5640 R2: 0.9411673016351128\n","Epoch [22/50], Step [1152/1325], Loss: 0.7067 R2: 0.9343941829170759\n","Epoch [22/50], Step [1216/1325], Loss: 0.8360 R2: 0.9148344436352494\n","Epoch [22/50], Step [1280/1325], Loss: 0.2605 R2: 0.9675287394697691\n","Epoch: 22/50.. Epoch Loss: 0.761082256188638.. Validation Loss: 8.713491605833324 R2: 0.2549525705794149\n","Epoch [23/50], Step [64/1325], Loss: 0.4777 R2: 0.9442738228024181\n","Epoch [23/50], Step [128/1325], Loss: 0.8307 R2: 0.89973100249796\n","Epoch [23/50], Step [192/1325], Loss: 1.5225 R2: 0.8577376090615406\n","Epoch [23/50], Step [256/1325], Loss: 1.5013 R2: 0.8682391979275335\n","Epoch [23/50], Step [320/1325], Loss: 0.6535 R2: 0.9159019636529772\n","Epoch [23/50], Step [384/1325], Loss: 0.6368 R2: 0.9484607832633482\n","Epoch [23/50], Step [448/1325], Loss: 0.5641 R2: 0.9537326204982585\n","Epoch [23/50], Step [512/1325], Loss: 0.5843 R2: 0.9381714586963747\n","Epoch [23/50], Step [576/1325], Loss: 0.9616 R2: 0.907670555050585\n","Epoch [23/50], Step [640/1325], Loss: 1.2376 R2: 0.8966490642418664\n","Epoch [23/50], Step [704/1325], Loss: 0.8879 R2: 0.9084303790717342\n","Epoch [23/50], Step [768/1325], Loss: 0.8108 R2: 0.9261253893255689\n","Epoch [23/50], Step [832/1325], Loss: 0.6788 R2: 0.9289385769573961\n","Epoch [23/50], Step [896/1325], Loss: 0.2476 R2: 0.9762954112603714\n","Epoch [23/50], Step [960/1325], Loss: 0.3983 R2: 0.9620784128290643\n","Epoch [23/50], Step [1024/1325], Loss: 0.4948 R2: 0.9458636084486785\n","Epoch [23/50], Step [1088/1325], Loss: 0.6647 R2: 0.9306684518740806\n","Epoch [23/50], Step [1152/1325], Loss: 0.5562 R2: 0.9483673203178532\n","Epoch [23/50], Step [1216/1325], Loss: 0.5960 R2: 0.9392840358790876\n","Epoch [23/50], Step [1280/1325], Loss: 0.1953 R2: 0.9756491857347561\n","Epoch: 23/50.. Epoch Loss: 0.8454813150199872.. Validation Loss: 7.369057359230257 R2: 0.3699084723820536\n","Epoch [24/50], Step [64/1325], Loss: 0.4813 R2: 0.9438634763674288\n","Epoch [24/50], Step [128/1325], Loss: 0.8914 R2: 0.8924060531031001\n","Epoch [24/50], Step [192/1325], Loss: 0.7923 R2: 0.9259630882118738\n","Epoch [24/50], Step [256/1325], Loss: 0.7226 R2: 0.9365796618652193\n","Epoch [24/50], Step [320/1325], Loss: 0.2484 R2: 0.9680366031378983\n","Epoch [24/50], Step [384/1325], Loss: 0.8414 R2: 0.9318963596248473\n","Epoch [24/50], Step [448/1325], Loss: 1.1628 R2: 0.904620101524855\n","Epoch [24/50], Step [512/1325], Loss: 1.1811 R2: 0.8750228263924064\n","Epoch [24/50], Step [576/1325], Loss: 0.9198 R2: 0.9116808876183254\n","Epoch [24/50], Step [640/1325], Loss: 0.8237 R2: 0.9312144679704314\n","Epoch [24/50], Step [704/1325], Loss: 0.6189 R2: 0.9361713007563756\n","Epoch [24/50], Step [768/1325], Loss: 0.8577 R2: 0.921856711106854\n","Epoch [24/50], Step [832/1325], Loss: 1.5113 R2: 0.8417932387758573\n","Epoch [24/50], Step [896/1325], Loss: 1.0823 R2: 0.8963915263341872\n","Epoch [24/50], Step [960/1325], Loss: 1.8149 R2: 0.8272265935559091\n","Epoch [24/50], Step [1024/1325], Loss: 1.1698 R2: 0.8719972944819652\n","Epoch [24/50], Step [1088/1325], Loss: 0.6304 R2: 0.9342475486202353\n","Epoch [24/50], Step [1152/1325], Loss: 0.4196 R2: 0.961043931506189\n","Epoch [24/50], Step [1216/1325], Loss: 0.7857 R2: 0.9199605818863138\n","Epoch [24/50], Step [1280/1325], Loss: 0.8287 R2: 0.8966949990213587\n","Epoch: 24/50.. Epoch Loss: 1.0911715858875446.. Validation Loss: 9.95736514576007 R2: 0.14859511161335193\n","Epoch [25/50], Step [64/1325], Loss: 1.1334 R2: 0.8677925108462683\n","Epoch [25/50], Step [128/1325], Loss: 1.0573 R2: 0.87237972022387\n","Epoch [25/50], Step [192/1325], Loss: 0.9745 R2: 0.9089390425335945\n","Epoch [25/50], Step [256/1325], Loss: 0.8930 R2: 0.9216264839889429\n","Epoch [25/50], Step [320/1325], Loss: 0.3575 R2: 0.9539882259173708\n","Epoch [25/50], Step [384/1325], Loss: 0.7865 R2: 0.936341258692413\n","Epoch [25/50], Step [448/1325], Loss: 0.8734 R2: 0.9283602942132462\n","Epoch [25/50], Step [512/1325], Loss: 0.7822 R2: 0.9172384166866517\n","Epoch [25/50], Step [576/1325], Loss: 0.8632 R2: 0.9171136913786044\n","Epoch [25/50], Step [640/1325], Loss: 0.9520 R2: 0.9204969312316912\n","Epoch [25/50], Step [704/1325], Loss: 0.5461 R2: 0.9436779933214813\n","Epoch [25/50], Step [768/1325], Loss: 0.6082 R2: 0.9445897562371636\n","Epoch [25/50], Step [832/1325], Loss: 0.5385 R2: 0.9436286624629794\n","Epoch [25/50], Step [896/1325], Loss: 0.4038 R2: 0.9613471970563294\n","Epoch [25/50], Step [960/1325], Loss: 0.6542 R2: 0.9377176301670842\n","Epoch [25/50], Step [1024/1325], Loss: 0.4015 R2: 0.9560694206112968\n","Epoch [25/50], Step [1088/1325], Loss: 0.3158 R2: 0.9670607327740056\n","Epoch [25/50], Step [1152/1325], Loss: 0.5294 R2: 0.9508485594208864\n","Epoch [25/50], Step [1216/1325], Loss: 1.0017 R2: 0.8979578124814226\n","Epoch [25/50], Step [1280/1325], Loss: 0.9360 R2: 0.8833159631773614\n","Epoch: 25/50.. Epoch Loss: 0.8479028257826782.. Validation Loss: 7.109923148549749 R2: 0.3920657517433468\n","Epoch [26/50], Step [64/1325], Loss: 1.1416 R2: 0.8668341213923463\n","Epoch [26/50], Step [128/1325], Loss: 0.7016 R2: 0.9153098897299965\n","Epoch [26/50], Step [192/1325], Loss: 0.8516 R2: 0.9204222626515661\n","Epoch [26/50], Step [256/1325], Loss: 2.1059 R2: 0.8151823087179684\n","Epoch [26/50], Step [320/1325], Loss: 1.8264 R2: 0.7649541314166135\n","Epoch [26/50], Step [384/1325], Loss: 1.3300 R2: 0.8923543214642549\n","Epoch [26/50], Step [448/1325], Loss: 0.8030 R2: 0.9341356114877316\n","Epoch [26/50], Step [512/1325], Loss: 0.4856 R2: 0.948618019947986\n","Epoch [26/50], Step [576/1325], Loss: 0.8424 R2: 0.9191123852021887\n","Epoch [26/50], Step [640/1325], Loss: 1.2174 R2: 0.8983392264121679\n","Epoch [26/50], Step [704/1325], Loss: 0.9609 R2: 0.9008979146166995\n","Epoch [26/50], Step [768/1325], Loss: 0.8852 R2: 0.919351721597608\n","Epoch [26/50], Step [832/1325], Loss: 1.1035 R2: 0.8844767963159499\n","Epoch [26/50], Step [896/1325], Loss: 0.5365 R2: 0.9486387888666835\n","Epoch [26/50], Step [960/1325], Loss: 0.6157 R2: 0.9413891832982793\n","Epoch [26/50], Step [1024/1325], Loss: 0.2423 R2: 0.9734895763562211\n","Epoch [26/50], Step [1088/1325], Loss: 0.3187 R2: 0.9667609571529163\n","Epoch [26/50], Step [1152/1325], Loss: 0.4654 R2: 0.9567897362382147\n","Epoch [26/50], Step [1216/1325], Loss: 0.6553 R2: 0.933244340292991\n","Epoch [26/50], Step [1280/1325], Loss: 0.3964 R2: 0.950587842373975\n","Epoch: 26/50.. Epoch Loss: 1.0614617702514026.. Validation Loss: 9.502908784183271 R2: 0.18745342769258833\n","Epoch [27/50], Step [64/1325], Loss: 0.6815 R2: 0.9205003399046024\n","Epoch [27/50], Step [128/1325], Loss: 0.4996 R2: 0.9396958979381408\n","Epoch [27/50], Step [192/1325], Loss: 0.6388 R2: 0.9403048751430619\n","Epoch [27/50], Step [256/1325], Loss: 1.0266 R2: 0.909906000055247\n","Epoch [27/50], Step [320/1325], Loss: 0.4043 R2: 0.9479664899220969\n","Epoch [27/50], Step [384/1325], Loss: 0.6781 R2: 0.9451159854549267\n","Epoch [27/50], Step [448/1325], Loss: 0.7254 R2: 0.9404972314949588\n","Epoch [27/50], Step [512/1325], Loss: 0.7570 R2: 0.9198977070035528\n","Epoch [27/50], Step [576/1325], Loss: 0.9382 R2: 0.9099189265179369\n","Epoch [27/50], Step [640/1325], Loss: 0.7266 R2: 0.9393203869936072\n","Epoch [27/50], Step [704/1325], Loss: 0.4061 R2: 0.9581149473590006\n","Epoch [27/50], Step [768/1325], Loss: 0.4879 R2: 0.9555507102951278\n","Epoch [27/50], Step [832/1325], Loss: 0.8379 R2: 0.9122877049339565\n","Epoch [27/50], Step [896/1325], Loss: 0.7407 R2: 0.9290939421285561\n","Epoch [27/50], Step [960/1325], Loss: 1.4129 R2: 0.8654957920102586\n","Epoch [27/50], Step [1024/1325], Loss: 1.0848 R2: 0.8813023626898567\n","Epoch [27/50], Step [1088/1325], Loss: 0.9309 R2: 0.902896052510074\n","Epoch [27/50], Step [1152/1325], Loss: 0.7991 R2: 0.92580993715058\n","Epoch [27/50], Step [1216/1325], Loss: 0.5696 R2: 0.9419726728397552\n","Epoch [27/50], Step [1280/1325], Loss: 0.2351 R2: 0.9706872865395197\n","Epoch: 27/50.. Epoch Loss: 1.0143724872416249.. Validation Loss: 12.468852974159617 R2: -0.06614974827657516\n","Epoch [28/50], Step [64/1325], Loss: 0.9586 R2: 0.8881796225848175\n","Epoch [28/50], Step [128/1325], Loss: 1.5917 R2: 0.8078751969989258\n","Epoch [28/50], Step [192/1325], Loss: 1.4623 R2: 0.8633628417825514\n","Epoch [28/50], Step [256/1325], Loss: 0.9324 R2: 0.918168264389689\n","Epoch [28/50], Step [320/1325], Loss: 0.2521 R2: 0.9675508793395187\n","Epoch [28/50], Step [384/1325], Loss: 0.5882 R2: 0.9523923516405212\n","Epoch [28/50], Step [448/1325], Loss: 0.6020 R2: 0.9506254814047834\n","Epoch [28/50], Step [512/1325], Loss: 0.4733 R2: 0.9499211150393456\n","Epoch [28/50], Step [576/1325], Loss: 0.5132 R2: 0.9507199857224164\n","Epoch [28/50], Step [640/1325], Loss: 0.5041 R2: 0.9579034317846296\n","Epoch [28/50], Step [704/1325], Loss: 0.2683 R2: 0.9723344571344946\n","Epoch [28/50], Step [768/1325], Loss: 0.4375 R2: 0.9601427563537352\n","Epoch [28/50], Step [832/1325], Loss: 0.5634 R2: 0.9410220644412193\n","Epoch [28/50], Step [896/1325], Loss: 0.3845 R2: 0.9631898079896548\n","Epoch [28/50], Step [960/1325], Loss: 0.5843 R2: 0.9443758930125108\n","Epoch [28/50], Step [1024/1325], Loss: 0.2832 R2: 0.969008311542694\n","Epoch [28/50], Step [1088/1325], Loss: 0.2984 R2: 0.9688694631348607\n","Epoch [28/50], Step [1152/1325], Loss: 0.3795 R2: 0.964765587096041\n","Epoch [28/50], Step [1216/1325], Loss: 0.4791 R2: 0.9511909225001616\n","Epoch [28/50], Step [1280/1325], Loss: 0.3751 R2: 0.9532387921346136\n","Epoch: 28/50.. Epoch Loss: 0.7485007000796761.. Validation Loss: 8.03106015410665 R2: 0.3133038994015971\n","Epoch [29/50], Step [64/1325], Loss: 0.7871 R2: 0.908183616746162\n","Epoch [29/50], Step [128/1325], Loss: 0.6933 R2: 0.9163116491921395\n","Epoch [29/50], Step [192/1325], Loss: 0.6501 R2: 0.9392526098144479\n","Epoch [29/50], Step [256/1325], Loss: 1.2816 R2: 0.8875234461958504\n","Epoch [29/50], Step [320/1325], Loss: 1.3317 R2: 0.8286209328875873\n","Epoch [29/50], Step [384/1325], Loss: 1.7632 R2: 0.8572872896519372\n","Epoch [29/50], Step [448/1325], Loss: 1.2387 R2: 0.8984006069692635\n","Epoch [29/50], Step [512/1325], Loss: 0.5487 R2: 0.9419458919497784\n","Epoch [29/50], Step [576/1325], Loss: 0.5308 R2: 0.949030052758621\n","Epoch [29/50], Step [640/1325], Loss: 0.7588 R2: 0.9366356673645654\n","Epoch [29/50], Step [704/1325], Loss: 0.6571 R2: 0.9322288832276595\n","Epoch [29/50], Step [768/1325], Loss: 0.8919 R2: 0.9187353653076157\n","Epoch [29/50], Step [832/1325], Loss: 1.2907 R2: 0.8648799193993484\n","Epoch [29/50], Step [896/1325], Loss: 0.7497 R2: 0.9282265471348701\n","Epoch [29/50], Step [960/1325], Loss: 1.1013 R2: 0.8951566121275598\n","Epoch [29/50], Step [1024/1325], Loss: 0.4423 R2: 0.9516019675940041\n","Epoch [29/50], Step [1088/1325], Loss: 0.3117 R2: 0.9674851786566624\n","Epoch [29/50], Step [1152/1325], Loss: 0.3841 R2: 0.9643367106799976\n","Epoch [29/50], Step [1216/1325], Loss: 0.4731 R2: 0.9518015057663448\n","Epoch [29/50], Step [1280/1325], Loss: 0.1624 R2: 0.9797532314683713\n","Epoch: 29/50.. Epoch Loss: 0.9729092145779574.. Validation Loss: 8.476007796437024 R2: 0.27525863581626386\n","Epoch [30/50], Step [64/1325], Loss: 0.4183 R2: 0.9512008862789324\n","Epoch [30/50], Step [128/1325], Loss: 0.4771 R2: 0.9424115965294207\n","Epoch [30/50], Step [192/1325], Loss: 0.5658 R2: 0.9471320795302013\n","Epoch [30/50], Step [256/1325], Loss: 0.8690 R2: 0.9237330394026414\n","Epoch [30/50], Step [320/1325], Loss: 0.4375 R2: 0.9436936463374888\n","Epoch [30/50], Step [384/1325], Loss: 1.3568 R2: 0.8901856402243333\n","Epoch [30/50], Step [448/1325], Loss: 1.5006 R2: 0.8769134873433916\n","Epoch [30/50], Step [512/1325], Loss: 1.3010 R2: 0.8623386629771231\n","Epoch [30/50], Step [576/1325], Loss: 0.8332 R2: 0.9199958826926194\n","Epoch [30/50], Step [640/1325], Loss: 0.4540 R2: 0.962090914224801\n","Epoch [30/50], Step [704/1325], Loss: 0.2515 R2: 0.974060813251301\n","Epoch [30/50], Step [768/1325], Loss: 0.6951 R2: 0.9366698554083619\n","Epoch [30/50], Step [832/1325], Loss: 1.4758 R2: 0.8455075646510815\n","Epoch [30/50], Step [896/1325], Loss: 1.0544 R2: 0.8990555994800771\n","Epoch [30/50], Step [960/1325], Loss: 1.7034 R2: 0.8378391451642728\n","Epoch [30/50], Step [1024/1325], Loss: 1.0062 R2: 0.8899068315346963\n","Epoch [30/50], Step [1088/1325], Loss: 0.8169 R2: 0.9147919651716665\n","Epoch [30/50], Step [1152/1325], Loss: 0.6894 R2: 0.9359984344450832\n","Epoch [30/50], Step [1216/1325], Loss: 0.5895 R2: 0.9399492561084071\n","Epoch [30/50], Step [1280/1325], Loss: 0.1569 R2: 0.9804467836588767\n","Epoch: 30/50.. Epoch Loss: 1.0855632644053563.. Validation Loss: 11.919311272227045 R2: -0.019161164755114868\n","Epoch [31/50], Step [64/1325], Loss: 0.7183 R2: 0.9162086084768644\n","Epoch [31/50], Step [128/1325], Loss: 0.8941 R2: 0.8920772252003986\n","Epoch [31/50], Step [192/1325], Loss: 0.9566 R2: 0.9106106844268976\n","Epoch [31/50], Step [256/1325], Loss: 1.0384 R2: 0.9088712768710547\n","Epoch [31/50], Step [320/1325], Loss: 0.3560 R2: 0.9541923236284459\n","Epoch [31/50], Step [384/1325], Loss: 0.9833 R2: 0.9204134016117447\n","Epoch [31/50], Step [448/1325], Loss: 0.8598 R2: 0.9294721749574946\n","Epoch [31/50], Step [512/1325], Loss: 0.9306 R2: 0.901528577226308\n","Epoch [31/50], Step [576/1325], Loss: 1.2960 R2: 0.8755665445103076\n","Epoch [31/50], Step [640/1325], Loss: 1.1380 R2: 0.9049657340129595\n","Epoch [31/50], Step [704/1325], Loss: 0.4347 R2: 0.9551694921968418\n","Epoch [31/50], Step [768/1325], Loss: 0.4760 R2: 0.9566319389483435\n","Epoch [31/50], Step [832/1325], Loss: 1.1094 R2: 0.8838637644273118\n","Epoch [31/50], Step [896/1325], Loss: 1.1436 R2: 0.8905254087871106\n","Epoch [31/50], Step [960/1325], Loss: 2.0544 R2: 0.8044253110385724\n","Epoch [31/50], Step [1024/1325], Loss: 1.1543 R2: 0.8736969636902652\n","Epoch [31/50], Step [1088/1325], Loss: 0.8264 R2: 0.9137963548839287\n","Epoch [31/50], Step [1152/1325], Loss: 0.7653 R2: 0.9289505833339617\n","Epoch [31/50], Step [1216/1325], Loss: 0.7604 R2: 0.9225344681231329\n","Epoch [31/50], Step [1280/1325], Loss: 0.3632 R2: 0.9547203797562623\n","Epoch: 31/50.. Epoch Loss: 1.1828767450047053.. Validation Loss: 13.790398439041562 R2: -0.17914854039191908\n","Epoch [32/50], Step [64/1325], Loss: 0.5634 R2: 0.9342776079272996\n","Epoch [32/50], Step [128/1325], Loss: 0.5923 R2: 0.9285049878002862\n","Epoch [32/50], Step [192/1325], Loss: 1.2263 R2: 0.8854096445530864\n","Epoch [32/50], Step [256/1325], Loss: 1.8313 R2: 0.839281196131725\n","Epoch [32/50], Step [320/1325], Loss: 0.9613 R2: 0.8762937984030548\n","Epoch [32/50], Step [384/1325], Loss: 1.2219 R2: 0.9011031325344817\n","Epoch [32/50], Step [448/1325], Loss: 0.9402 R2: 0.9228833205866127\n","Epoch [32/50], Step [512/1325], Loss: 0.6607 R2: 0.930088330572617\n","Epoch [32/50], Step [576/1325], Loss: 0.7692 R2: 0.9261407862634945\n","Epoch [32/50], Step [640/1325], Loss: 0.7431 R2: 0.9379474612523457\n","Epoch [32/50], Step [704/1325], Loss: 0.6303 R2: 0.9349962342431387\n","Epoch [32/50], Step [768/1325], Loss: 0.5608 R2: 0.9489075107178137\n","Epoch [32/50], Step [832/1325], Loss: 0.7670 R2: 0.9197109259261776\n","Epoch [32/50], Step [896/1325], Loss: 0.6447 R2: 0.9382798521437499\n","Epoch [32/50], Step [960/1325], Loss: 1.9661 R2: 0.8128365989722104\n","Epoch [32/50], Step [1024/1325], Loss: 1.5309 R2: 0.8324931019214806\n","Epoch [32/50], Step [1088/1325], Loss: 1.7161 R2: 0.820994202053667\n","Epoch [32/50], Step [1152/1325], Loss: 1.3969 R2: 0.870314089380537\n","Epoch [32/50], Step [1216/1325], Loss: 1.0997 R2: 0.8879713173023257\n","Epoch [32/50], Step [1280/1325], Loss: 0.5504 R2: 0.9313933798391419\n","Epoch: 32/50.. Epoch Loss: 1.1717520966839035.. Validation Loss: 8.038507759485212 R2: 0.3126670913842494\n","Epoch [33/50], Step [64/1325], Loss: 0.6861 R2: 0.919965292247592\n","Epoch [33/50], Step [128/1325], Loss: 0.6918 R2: 0.9165013177161546\n","Epoch [33/50], Step [192/1325], Loss: 1.1988 R2: 0.8879781954339722\n","Epoch [33/50], Step [256/1325], Loss: 1.7359 R2: 0.8476514604851213\n","Epoch [33/50], Step [320/1325], Loss: 1.5013 R2: 0.80679359617434\n","Epoch [33/50], Step [384/1325], Loss: 2.3140 R2: 0.8127093487546586\n","Epoch [33/50], Step [448/1325], Loss: 2.1248 R2: 0.8257204364633958\n","Epoch [33/50], Step [512/1325], Loss: 1.1889 R2: 0.8741999317798677\n","Epoch [33/50], Step [576/1325], Loss: 0.6870 R2: 0.9340357106836714\n","Epoch [33/50], Step [640/1325], Loss: 0.6905 R2: 0.9423400591031521\n","Epoch [33/50], Step [704/1325], Loss: 0.3101 R2: 0.9680155896850327\n","Epoch [33/50], Step [768/1325], Loss: 0.4265 R2: 0.9611422877336155\n","Epoch [33/50], Step [832/1325], Loss: 0.6163 R2: 0.9354775184800391\n","Epoch [33/50], Step [896/1325], Loss: 0.5774 R2: 0.9447201609490741\n","Epoch [33/50], Step [960/1325], Loss: 1.4036 R2: 0.8663810042819152\n","Epoch [33/50], Step [1024/1325], Loss: 1.1549 R2: 0.8736348494286248\n","Epoch [33/50], Step [1088/1325], Loss: 1.2787 R2: 0.8666178556135926\n","Epoch [33/50], Step [1152/1325], Loss: 1.4463 R2: 0.8657257931256693\n","Epoch [33/50], Step [1216/1325], Loss: 1.3751 R2: 0.8599230551797534\n","Epoch [33/50], Step [1280/1325], Loss: 1.0667 R2: 0.8670303382313324\n","Epoch: 33/50.. Epoch Loss: 1.254854534556965.. Validation Loss: 7.2522532604252 R2: 0.37989580981209836\n","Epoch [34/50], Step [64/1325], Loss: 0.8630 R2: 0.8993296560018998\n","Epoch [34/50], Step [128/1325], Loss: 1.0135 R2: 0.8776631599755338\n","Epoch [34/50], Step [192/1325], Loss: 1.2781 R2: 0.8805670670552272\n","Epoch [34/50], Step [256/1325], Loss: 1.3394 R2: 0.8824482186138837\n","Epoch [34/50], Step [320/1325], Loss: 0.8798 R2: 0.8867842687027191\n","Epoch [34/50], Step [384/1325], Loss: 2.7168 R2: 0.7801107736144048\n","Epoch [34/50], Step [448/1325], Loss: 3.5599 R2: 0.7080083112613209\n","Epoch [34/50], Step [512/1325], Loss: 3.6611 R2: 0.6126113437813918\n","Epoch [34/50], Step [576/1325], Loss: 3.1204 R2: 0.700391393364872\n","Epoch [34/50], Step [640/1325], Loss: 1.5834 R2: 0.8677760638776606\n","Epoch [34/50], Step [704/1325], Loss: 0.2921 R2: 0.9698801264116169\n","Epoch [34/50], Step [768/1325], Loss: 0.4720 R2: 0.9570006402635496\n","Epoch [34/50], Step [832/1325], Loss: 1.7374 R2: 0.8181206836175163\n","Epoch [34/50], Step [896/1325], Loss: 1.9642 R2: 0.8119673227308399\n","Epoch [34/50], Step [960/1325], Loss: 4.2354 R2: 0.5968010613683004\n","Epoch [34/50], Step [1024/1325], Loss: 3.0974 R2: 0.6610867438831725\n","Epoch [34/50], Step [1088/1325], Loss: 2.5590 R2: 0.7330694605442573\n","Epoch [34/50], Step [1152/1325], Loss: 2.6024 R2: 0.7584030034690642\n","Epoch [34/50], Step [1216/1325], Loss: 1.6179 R2: 0.8351861191608656\n","Epoch [34/50], Step [1280/1325], Loss: 0.6531 R2: 0.9185803170697442\n","Epoch: 34/50.. Epoch Loss: 2.0547131594030628.. Validation Loss: 7.157244525908437 R2: 0.3880195333892853\n","Epoch [35/50], Step [64/1325], Loss: 0.7919 R2: 0.907625329083374\n","Epoch [35/50], Step [128/1325], Loss: 1.0182 R2: 0.8771039270393899\n","Epoch [35/50], Step [192/1325], Loss: 1.3371 R2: 0.8750589770150532\n","Epoch [35/50], Step [256/1325], Loss: 1.7399 R2: 0.847299224851903\n","Epoch [35/50], Step [320/1325], Loss: 0.7996 R2: 0.8971021334700624\n","Epoch [35/50], Step [384/1325], Loss: 2.1230 R2: 0.8281703385032357\n","Epoch [35/50], Step [448/1325], Loss: 2.1629 R2: 0.8225887677543726\n","Epoch [35/50], Step [512/1325], Loss: 3.2094 R2: 0.6604117798172255\n","Epoch [35/50], Step [576/1325], Loss: 3.8095 R2: 0.6342232772145495\n","Epoch [35/50], Step [640/1325], Loss: 3.7432 R2: 0.6874065345914828\n","Epoch [35/50], Step [704/1325], Loss: 3.5743 R2: 0.6313757291611697\n","Epoch [35/50], Step [768/1325], Loss: 1.8117 R2: 0.8349384276956117\n","Epoch [35/50], Step [832/1325], Loss: 0.9201 R2: 0.9036742212930993\n","Epoch [35/50], Step [896/1325], Loss: 1.3750 R2: 0.8683722502266848\n","Epoch [35/50], Step [960/1325], Loss: 4.9038 R2: 0.5331717129731924\n","Epoch [35/50], Step [1024/1325], Loss: 5.1181 R2: 0.43998238298023085\n","Epoch [35/50], Step [1088/1325], Loss: 5.4908 R2: 0.42725920345301494\n","Epoch [35/50], Step [1152/1325], Loss: 5.1593 R2: 0.5210223255072292\n","Epoch [35/50], Step [1216/1325], Loss: 4.2262 R2: 0.5694779917439858\n","Epoch [35/50], Step [1280/1325], Loss: 1.9772 R2: 0.7535237110713061\n","Epoch: 35/50.. Epoch Loss: 2.8508005568797437.. Validation Loss: 8.064018121114358 R2: 0.31048582072453723\n","Epoch [36/50], Step [64/1325], Loss: 0.9664 R2: 0.8872718234618159\n","Epoch [36/50], Step [128/1325], Loss: 1.2809 R2: 0.8453897740250161\n","Epoch [36/50], Step [192/1325], Loss: 1.1169 R2: 0.8956372248336099\n","Epoch [36/50], Step [256/1325], Loss: 2.0984 R2: 0.8158395424621121\n","Epoch [36/50], Step [320/1325], Loss: 1.1268 R2: 0.8549942431888511\n","Epoch [36/50], Step [384/1325], Loss: 2.5082 R2: 0.7969904463993509\n","Epoch [36/50], Step [448/1325], Loss: 2.3811 R2: 0.8046971343856608\n","Epoch [36/50], Step [512/1325], Loss: 2.2269 R2: 0.7643677124364583\n","Epoch [36/50], Step [576/1325], Loss: 2.8639 R2: 0.7250196841622083\n","Epoch [36/50], Step [640/1325], Loss: 2.9399 R2: 0.7544966669976997\n","Epoch [36/50], Step [704/1325], Loss: 2.1980 R2: 0.7733162486302538\n","Epoch [36/50], Step [768/1325], Loss: 4.0418 R2: 0.6317555571977467\n","Epoch [36/50], Step [832/1325], Loss: 1.6898 R2: 0.823106254067691\n","Epoch [36/50], Step [896/1325], Loss: 0.5827 R2: 0.9442169172742303\n","Epoch [36/50], Step [960/1325], Loss: 2.7568 R2: 0.7375571347111425\n","Epoch [36/50], Step [1024/1325], Loss: 4.0198 R2: 0.5601526661974238\n","Epoch [36/50], Step [1088/1325], Loss: 6.0931 R2: 0.36443491812583884\n","Epoch [36/50], Step [1152/1325], Loss: 5.4424 R2: 0.49474420679877695\n","Epoch [36/50], Step [1216/1325], Loss: 4.6439 R2: 0.5269336380334688\n","Epoch [36/50], Step [1280/1325], Loss: 3.2830 R2: 0.5907498919943909\n","Epoch: 36/50.. Epoch Loss: 2.828446302351234.. Validation Loss: 8.093476513203711 R2: 0.3079669874803921\n","Epoch [37/50], Step [64/1325], Loss: 1.4828 R2: 0.8270408560824033\n","Epoch [37/50], Step [128/1325], Loss: 1.9655 R2: 0.7627549728054324\n","Epoch [37/50], Step [192/1325], Loss: 1.6100 R2: 0.849556118849931\n","Epoch [37/50], Step [256/1325], Loss: 1.7525 R2: 0.8461997307851449\n","Epoch [37/50], Step [320/1325], Loss: 1.3145 R2: 0.8308422181035211\n","Epoch [37/50], Step [384/1325], Loss: 2.0335 R2: 0.8354146455643106\n","Epoch [37/50], Step [448/1325], Loss: 2.0694 R2: 0.8302576912649287\n","Epoch [37/50], Step [512/1325], Loss: 2.6177 R2: 0.7230224106312846\n","Epoch [37/50], Step [576/1325], Loss: 3.2666 R2: 0.6863535577020008\n","Epoch [37/50], Step [640/1325], Loss: 3.1587 R2: 0.736223415453497\n","Epoch [37/50], Step [704/1325], Loss: 2.5246 R2: 0.7396373188579872\n","Epoch [37/50], Step [768/1325], Loss: 2.9735 R2: 0.7290843109109216\n","Epoch [37/50], Step [832/1325], Loss: 2.9432 R2: 0.6918843076810973\n","Epoch [37/50], Step [896/1325], Loss: 3.2319 R2: 0.6906069988764049\n","Epoch [37/50], Step [960/1325], Loss: 0.8553 R2: 0.9185800603390278\n","Epoch [37/50], Step [1024/1325], Loss: 2.9420 R2: 0.6780838801540153\n","Epoch [37/50], Step [1088/1325], Loss: 5.3875 R2: 0.43803769052418295\n","Epoch [37/50], Step [1152/1325], Loss: 6.3805 R2: 0.40765570524141315\n","Epoch [37/50], Step [1216/1325], Loss: 7.7846 R2: 0.20699167501057092\n","Epoch [37/50], Step [1280/1325], Loss: 5.1788 R2: 0.35441994522690423\n","Epoch: 37/50.. Epoch Loss: 3.212107003580559.. Validation Loss: 9.180636054021095 R2: 0.21500937093282935\n","Epoch [38/50], Step [64/1325], Loss: 2.9891 R2: 0.6513322473877333\n","Epoch [38/50], Step [128/1325], Loss: 2.4580 R2: 0.7033101733845728\n","Epoch [38/50], Step [192/1325], Loss: 2.3593 R2: 0.7795449280203977\n","Epoch [38/50], Step [256/1325], Loss: 3.1592 R2: 0.7227441162808219\n","Epoch [38/50], Step [320/1325], Loss: 2.0235 R2: 0.7395942777418666\n","Epoch [38/50], Step [384/1325], Loss: 2.5721 R2: 0.7918178642165284\n","Epoch [38/50], Step [448/1325], Loss: 2.8304 R2: 0.7678404718954737\n","Epoch [38/50], Step [512/1325], Loss: 1.8664 R2: 0.8025118742060631\n","Epoch [38/50], Step [576/1325], Loss: 2.5702 R2: 0.7532149936016257\n","Epoch [38/50], Step [640/1325], Loss: 3.5788 R2: 0.7011406802246359\n","Epoch [38/50], Step [704/1325], Loss: 2.9921 R2: 0.6914175288725042\n","Epoch [38/50], Step [768/1325], Loss: 3.6183 R2: 0.6703383459546473\n","Epoch [38/50], Step [832/1325], Loss: 3.8133 R2: 0.6008011210675344\n","Epoch [38/50], Step [896/1325], Loss: 3.6230 R2: 0.6531630516633766\n","Epoch [38/50], Step [960/1325], Loss: 2.2116 R2: 0.7894608545804079\n","Epoch [38/50], Step [1024/1325], Loss: 2.4726 R2: 0.729443942717659\n","Epoch [38/50], Step [1088/1325], Loss: 3.5638 R2: 0.6282602847790603\n","Epoch [38/50], Step [1152/1325], Loss: 4.3892 R2: 0.5925199080015875\n","Epoch [38/50], Step [1216/1325], Loss: 5.8318 R2: 0.4059214800747436\n","Epoch [38/50], Step [1280/1325], Loss: 2.7432 R2: 0.6580381255307587\n","Epoch: 38/50.. Epoch Loss: 3.2468182499293805.. Validation Loss: 9.33389837715311 R2: 0.20190466847092536\n","Epoch [39/50], Step [64/1325], Loss: 1.5812 R2: 0.8155576034014018\n","Epoch [39/50], Step [128/1325], Loss: 1.7285 R2: 0.7913658949228896\n","Epoch [39/50], Step [192/1325], Loss: 1.7725 R2: 0.834368959973754\n","Epoch [39/50], Step [256/1325], Loss: 1.9153 R2: 0.8319124579038295\n","Epoch [39/50], Step [320/1325], Loss: 1.3584 R2: 0.8251926173159365\n","Epoch [39/50], Step [384/1325], Loss: 4.2606 R2: 0.6551577571363596\n","Epoch [39/50], Step [448/1325], Loss: 6.8130 R2: 0.4411779054775242\n","Epoch [39/50], Step [512/1325], Loss: 4.2599 R2: 0.5492534902344712\n","Epoch [39/50], Step [576/1325], Loss: 2.7395 R2: 0.7369575139917509\n","Epoch [39/50], Step [640/1325], Loss: 4.1751 R2: 0.6513397902173674\n","Epoch [39/50], Step [704/1325], Loss: 2.4668 R2: 0.7455934749167878\n","Epoch [39/50], Step [768/1325], Loss: 3.2318 R2: 0.7055504268030379\n","Epoch [39/50], Step [832/1325], Loss: 3.7902 R2: 0.6032211929843712\n","Epoch [39/50], Step [896/1325], Loss: 4.9747 R2: 0.5237602074522685\n","Epoch [39/50], Step [960/1325], Loss: 5.4314 R2: 0.48294738830882555\n","Epoch [39/50], Step [1024/1325], Loss: 4.1117 R2: 0.5500991694994868\n","Epoch [39/50], Step [1088/1325], Loss: 1.3204 R2: 0.8622687843109371\n","Epoch [39/50], Step [1152/1325], Loss: 2.5334 R2: 0.7648017000515711\n","Epoch [39/50], Step [1216/1325], Loss: 6.4940 R2: 0.33846299894390897\n","Epoch [39/50], Step [1280/1325], Loss: 13.7669 R2: -0.7161473041316617\n","Epoch: 39/50.. Epoch Loss: 4.604188660901236.. Validation Loss: 24.371621928790596 R2: -1.0838964478827844\n","Epoch [40/50], Step [64/1325], Loss: 10.3177 R2: -0.20352295178912594\n","Epoch [40/50], Step [128/1325], Loss: 12.3246 R2: -0.48760614759629983\n","Epoch [40/50], Step [192/1325], Loss: 8.8017 R2: 0.17754284175261426\n","Epoch [40/50], Step [256/1325], Loss: 4.9593 R2: 0.5647610927505298\n","Epoch [40/50], Step [320/1325], Loss: 2.0064 R2: 0.741795705997909\n","Epoch [40/50], Step [384/1325], Loss: 2.1202 R2: 0.828399503227745\n","Epoch [40/50], Step [448/1325], Loss: 2.2658 R2: 0.8141542471209479\n","Epoch [40/50], Step [512/1325], Loss: 3.2798 R2: 0.6529609561290239\n","Epoch [40/50], Step [576/1325], Loss: 5.7865 R2: 0.44439413700133623\n","Epoch [40/50], Step [640/1325], Loss: 7.1956 R2: 0.3991071695550821\n","Epoch [40/50], Step [704/1325], Loss: 8.6823 R2: 0.10458147225486336\n","Epoch [40/50], Step [768/1325], Loss: 4.7616 R2: 0.5661756849926072\n","Epoch [40/50], Step [832/1325], Loss: 2.5642 R2: 0.7315621247997521\n","Epoch [40/50], Step [896/1325], Loss: 1.8779 R2: 0.8202220482056128\n","Epoch [40/50], Step [960/1325], Loss: 2.0585 R2: 0.8040392725562681\n","Epoch [40/50], Step [1024/1325], Loss: 1.4013 R2: 0.8466739749661542\n","Epoch [40/50], Step [1088/1325], Loss: 1.4071 R2: 0.8532311607832987\n","Epoch [40/50], Step [1152/1325], Loss: 1.8046 R2: 0.8324690974268205\n","Epoch [40/50], Step [1216/1325], Loss: 2.2770 R2: 0.7680481743849001\n","Epoch [40/50], Step [1280/1325], Loss: 2.4874 R2: 0.6899332894616537\n","Epoch: 40/50.. Epoch Loss: 4.630485757253062.. Validation Loss: 13.669102348431512 R2: -0.1687771150067443\n","Epoch [41/50], Step [64/1325], Loss: 2.7215 R2: 0.6825451047868603\n","Epoch [41/50], Step [128/1325], Loss: 3.8739 R2: 0.532413101075\n","Epoch [41/50], Step [192/1325], Loss: 4.0207 R2: 0.6242998140600493\n","Epoch [41/50], Step [256/1325], Loss: 5.1777 R2: 0.5455914152186767\n","Epoch [41/50], Step [320/1325], Loss: 3.1554 R2: 0.5939300392614307\n","Epoch [41/50], Step [384/1325], Loss: 2.8870 R2: 0.7663340444583986\n","Epoch [41/50], Step [448/1325], Loss: 1.8410 R2: 0.8489951714705183\n","Epoch [41/50], Step [512/1325], Loss: 1.8404 R2: 0.8052687599193157\n","Epoch [41/50], Step [576/1325], Loss: 1.9333 R2: 0.8143721833886506\n","Epoch [41/50], Step [640/1325], Loss: 3.2567 R2: 0.7280372171559604\n","Epoch [41/50], Step [704/1325], Loss: 3.7967 R2: 0.6084455896307378\n","Epoch [41/50], Step [768/1325], Loss: 4.6760 R2: 0.5739704631460799\n","Epoch [41/50], Step [832/1325], Loss: 3.3229 R2: 0.652136044385945\n","Epoch [41/50], Step [896/1325], Loss: 4.4767 R2: 0.5714341534564207\n","Epoch [41/50], Step [960/1325], Loss: 3.2539 R2: 0.6902385225441336\n","Epoch [41/50], Step [1024/1325], Loss: 1.8469 R2: 0.7979084258777265\n","Epoch [41/50], Step [1088/1325], Loss: 1.0162 R2: 0.8939992545615796\n","Epoch [41/50], Step [1152/1325], Loss: 1.4614 R2: 0.8643299174716316\n","Epoch [41/50], Step [1216/1325], Loss: 3.0383 R2: 0.6904946658174402\n","Epoch [41/50], Step [1280/1325], Loss: 4.1813 R2: 0.47876751874343126\n","Epoch: 41/50.. Epoch Loss: 3.427213904375548.. Validation Loss: 14.887916690764438 R2: -0.2729918850150823\n","Epoch [42/50], Step [64/1325], Loss: 4.0331 R2: 0.5295551404837066\n","Epoch [42/50], Step [128/1325], Loss: 7.1771 R2: 0.1337162693407702\n","Epoch [42/50], Step [192/1325], Loss: 8.0731 R2: 0.24562548629694914\n","Epoch [42/50], Step [256/1325], Loss: 6.4279 R2: 0.43587727524353015\n","Epoch [42/50], Step [320/1325], Loss: 6.4078 R2: 0.175379322811686\n","Epoch [42/50], Step [384/1325], Loss: 6.1207 R2: 0.5046094209212209\n","Epoch [42/50], Step [448/1325], Loss: 4.5392 R2: 0.6276786209698912\n","Epoch [42/50], Step [512/1325], Loss: 3.7746 R2: 0.6006023759617773\n","Epoch [42/50], Step [576/1325], Loss: 1.9986 R2: 0.8080985030575002\n","Epoch [42/50], Step [640/1325], Loss: 1.8908 R2: 0.8420994027697226\n","Epoch [42/50], Step [704/1325], Loss: 2.4526 R2: 0.7470640833159644\n","Epoch [42/50], Step [768/1325], Loss: 5.0053 R2: 0.5439758802606424\n","Epoch [42/50], Step [832/1325], Loss: 5.4029 R2: 0.4343970818767733\n","Epoch [42/50], Step [896/1325], Loss: 9.7196 R2: 0.0695172849603296\n","Epoch [42/50], Step [960/1325], Loss: 14.4861 R2: -0.3790427975725761\n","Epoch [42/50], Step [1024/1325], Loss: 12.6703 R2: -0.3863836022677649\n","Epoch [42/50], Step [1088/1325], Loss: 6.8256 R2: 0.2880284475307956\n","Epoch [42/50], Step [1152/1325], Loss: 1.7926 R2: 0.8335801400506333\n","Epoch [42/50], Step [1216/1325], Loss: 1.2071 R2: 0.8770290536323465\n","Epoch [42/50], Step [1280/1325], Loss: 3.9741 R2: 0.5046019577711716\n","Epoch: 42/50.. Epoch Loss: 6.183537912205094.. Validation Loss: 21.962605944033474 R2: -0.8779134650642679\n","Epoch [43/50], Step [64/1325], Loss: 5.4579 R2: 0.3633514849886642\n","Epoch [43/50], Step [128/1325], Loss: 11.9564 R2: -0.44315805476428416\n","Epoch [43/50], Step [192/1325], Loss: 15.0736 R2: -0.40851210177379627\n","Epoch [43/50], Step [256/1325], Loss: 13.3995 R2: -0.17596719944457884\n","Epoch [43/50], Step [320/1325], Loss: 17.0191 R2: -1.1901891659617299\n","Epoch [43/50], Step [384/1325], Loss: 18.3839 R2: -0.487948509894496\n","Epoch [43/50], Step [448/1325], Loss: 16.4421 R2: -0.3486388630129247\n","Epoch [43/50], Step [512/1325], Loss: 14.4170 R2: -0.5254725493827537\n","Epoch [43/50], Step [576/1325], Loss: 12.1887 R2: -0.17031750608878804\n","Epoch [43/50], Step [640/1325], Loss: 8.2765 R2: 0.3088375116167631\n","Epoch [43/50], Step [704/1325], Loss: 4.7505 R2: 0.5100787072060409\n","Epoch [43/50], Step [768/1325], Loss: 3.1503 R2: 0.7129771421744914\n","Epoch [43/50], Step [832/1325], Loss: 2.3997 R2: 0.7487815226994192\n","Epoch [43/50], Step [896/1325], Loss: 8.0371 R2: 0.23058687661647603\n","Epoch [43/50], Step [960/1325], Loss: 13.0714 R2: -0.2443583367150377\n","Epoch [43/50], Step [1024/1325], Loss: 21.1623 R2: -1.3155772177796408\n","Epoch [43/50], Step [1088/1325], Loss: 23.8338 R2: -1.4860783744289572\n","Epoch [43/50], Step [1152/1325], Loss: 22.4081 R2: -1.0803012075572447\n","Epoch [43/50], Step [1216/1325], Loss: 12.3487 R2: -0.25795757736917535\n","Epoch [43/50], Step [1280/1325], Loss: 2.4339 R2: 0.6965983450691122\n","Epoch: 43/50.. Epoch Loss: 12.352072324696357.. Validation Loss: 15.529276171991025 R2: -0.3278313536562705\n","Epoch [44/50], Step [64/1325], Loss: 1.7513 R2: 0.7957108629271368\n","Epoch [44/50], Step [128/1325], Loss: 7.6607 R2: 0.07534397174457719\n","Epoch [44/50], Step [192/1325], Loss: 15.9767 R2: -0.4928995657520041\n","Epoch [44/50], Step [256/1325], Loss: 15.9499 R2: -0.39979334358817775\n","Epoch [44/50], Step [320/1325], Loss: 25.7359 R2: -2.3119636855802153\n","Epoch [44/50], Step [384/1325], Loss: 31.6288 R2: -1.5599549750122215\n","Epoch [44/50], Step [448/1325], Loss: 29.6431 R2: -1.4314313106317296\n","Epoch [44/50], Step [512/1325], Loss: 29.5797 R2: -2.1298525681567466\n","Epoch [44/50], Step [576/1325], Loss: 28.3714 R2: -1.7241281999581317\n","Epoch [44/50], Step [640/1325], Loss: 23.9607 R2: -1.0009284543434744\n","Epoch [44/50], Step [704/1325], Loss: 18.8028 R2: -0.9391598533002032\n","Epoch [44/50], Step [768/1325], Loss: 16.4778 R2: -0.5012727102180798\n","Epoch [44/50], Step [832/1325], Loss: 14.1229 R2: -0.47846868158812583\n","Epoch [44/50], Step [896/1325], Loss: 8.2704 R2: 0.20825355562095282\n","Epoch [44/50], Step [960/1325], Loss: 5.4417 R2: 0.48196733864867747\n","Epoch [44/50], Step [1024/1325], Loss: 6.2209 R2: 0.3193118444294901\n","Epoch [44/50], Step [1088/1325], Loss: 14.8022 R2: -0.5439987779532112\n","Epoch [44/50], Step [1152/1325], Loss: 23.2752 R2: -1.1608076752264624\n","Epoch [44/50], Step [1216/1325], Loss: 28.3065 R2: -1.8835570855876616\n","Epoch [44/50], Step [1280/1325], Loss: 24.2372 R2: -2.0213383559485973\n","Epoch: 44/50.. Epoch Loss: 18.308617709974083.. Validation Loss: 17.593246144293467 R2: -0.5043111678276326\n","Epoch [45/50], Step [64/1325], Loss: 14.0870 R2: -0.6431979227698434\n","Epoch [45/50], Step [128/1325], Loss: 3.6161 R2: 0.5635257888461994\n","Epoch [45/50], Step [192/1325], Loss: 3.8095 R2: 0.64403425203443\n","Epoch [45/50], Step [256/1325], Loss: 7.0132 R2: 0.384510599650005\n","Epoch [45/50], Step [320/1325], Loss: 14.2856 R2: -0.8384150569826689\n","Epoch [45/50], Step [384/1325], Loss: 20.0979 R2: -0.6266737733256216\n","Epoch [45/50], Step [448/1325], Loss: 21.5568 R2: -0.7681652176176419\n","Epoch [45/50], Step [512/1325], Loss: 24.1302 R2: -1.5532340212510998\n","Epoch [45/50], Step [576/1325], Loss: 23.8917 R2: -1.294002565607331\n","Epoch [45/50], Step [640/1325], Loss: 22.5599 R2: -0.8839488019037376\n","Epoch [45/50], Step [704/1325], Loss: 20.1425 R2: -1.0773180080242994\n","Epoch [45/50], Step [768/1325], Loss: 17.5881 R2: -0.6024390708010083\n","Epoch [45/50], Step [832/1325], Loss: 18.7361 R2: -0.9614049690588682\n","Epoch [45/50], Step [896/1325], Loss: 14.1437 R2: -0.35401039753929253\n","Epoch [45/50], Step [960/1325], Loss: 11.7332 R2: -0.11697004082369311\n","Epoch [45/50], Step [1024/1325], Loss: 6.2317 R2: 0.3181239832216106\n","Epoch [45/50], Step [1088/1325], Loss: 3.8735 R2: 0.5959558760123713\n","Epoch [45/50], Step [1152/1325], Loss: 5.1568 R2: 0.5212583911290132\n","Epoch [45/50], Step [1216/1325], Loss: 9.7721 R2: 0.004525459037468149\n","Epoch [45/50], Step [1280/1325], Loss: 14.9648 R2: -0.8654745991693666\n","Epoch: 45/50.. Epoch Loss: 14.430474594354147.. Validation Loss: 34.88095966817378 R2: -1.9824978105396442\n","Epoch [46/50], Step [64/1325], Loss: 23.8130 R2: -1.777712651619685\n","Epoch [46/50], Step [128/1325], Loss: 21.2016 R2: -1.5590651694261313\n","Epoch [46/50], Step [192/1325], Loss: 11.1884 R2: -0.04547134067090397\n","Epoch [46/50], Step [256/1325], Loss: 6.6038 R2: 0.4204392737862137\n","Epoch [46/50], Step [320/1325], Loss: 2.0382 R2: 0.7377081071125988\n","Epoch [46/50], Step [384/1325], Loss: 4.3708 R2: 0.6462356174714938\n","Epoch [46/50], Step [448/1325], Loss: 6.5357 R2: 0.46391799040427495\n","Epoch [46/50], Step [512/1325], Loss: 10.2911 R2: -0.08890988428303048\n","Epoch [46/50], Step [576/1325], Loss: 12.3070 R2: -0.18167602802342664\n","Epoch [46/50], Step [640/1325], Loss: 13.9314 R2: -0.16338958230063727\n","Epoch [46/50], Step [704/1325], Loss: 13.4038 R2: -0.3823519960757533\n","Epoch [46/50], Step [768/1325], Loss: 12.8077 R2: -0.1668981914798271\n","Epoch [46/50], Step [832/1325], Loss: 13.5076 R2: -0.41405047520343063\n","Epoch [46/50], Step [896/1325], Loss: 11.8790 R2: -0.13720027463005025\n","Epoch [46/50], Step [960/1325], Loss: 12.0792 R2: -0.14991309891415372\n","Epoch [46/50], Step [1024/1325], Loss: 8.9880 R2: 0.016537545044780222\n","Epoch [46/50], Step [1088/1325], Loss: 7.0096 R2: 0.2688318762988573\n","Epoch [46/50], Step [1152/1325], Loss: 5.1583 R2: 0.5211188474857323\n","Epoch [46/50], Step [1216/1325], Loss: 3.2797 R2: 0.6658954345405563\n","Epoch [46/50], Step [1280/1325], Loss: 3.0343 R2: 0.621757371471073\n","Epoch: 46/50.. Epoch Loss: 10.532320315156062.. Validation Loss: 23.792973975019496 R2: -1.034419148360746\n","Epoch [47/50], Step [64/1325], Loss: 7.1402 R2: 0.16711540708169914\n","Epoch [47/50], Step [128/1325], Loss: 7.1253 R2: 0.13995955830688678\n","Epoch [47/50], Step [192/1325], Loss: 10.1934 R2: 0.047506767851315246\n","Epoch [47/50], Step [256/1325], Loss: 12.5947 R2: -0.10533581507752654\n","Epoch [47/50], Step [320/1325], Loss: 11.7985 R2: -0.5183540136763067\n","Epoch [47/50], Step [384/1325], Loss: 7.5886 R2: 0.38579957227039163\n","Epoch [47/50], Step [448/1325], Loss: 4.8321 R2: 0.6036536271508356\n","Epoch [47/50], Step [512/1325], Loss: 2.0280 R2: 0.7854129737240971\n","Epoch [47/50], Step [576/1325], Loss: 1.8901 R2: 0.8185164754603823\n","Epoch [47/50], Step [640/1325], Loss: 3.9305 R2: 0.671768546443952\n","Epoch [47/50], Step [704/1325], Loss: 4.7369 R2: 0.5114804815572582\n","Epoch [47/50], Step [768/1325], Loss: 6.5003 R2: 0.4077628069204703\n","Epoch [47/50], Step [832/1325], Loss: 8.1738 R2: 0.1443172922576218\n","Epoch [47/50], Step [896/1325], Loss: 8.6089 R2: 0.17585218861341945\n","Epoch [47/50], Step [960/1325], Loss: 10.2705 R2: 0.022270566074610865\n","Epoch [47/50], Step [1024/1325], Loss: 8.3327 R2: 0.0882353447106079\n","Epoch [47/50], Step [1088/1325], Loss: 7.6480 R2: 0.20224195483991625\n","Epoch [47/50], Step [1152/1325], Loss: 6.2228 R2: 0.4222913768931704\n","Epoch [47/50], Step [1216/1325], Loss: 5.7790 R2: 0.4113021779806054\n","Epoch [47/50], Step [1280/1325], Loss: 3.5921 R2: 0.5522135418540424\n","Epoch: 47/50.. Epoch Loss: 6.95709262856813.. Validation Loss: 9.444783955457675 R2: 0.192423387715559\n","Epoch [48/50], Step [64/1325], Loss: 2.4842 R2: 0.7102224964713999\n","Epoch [48/50], Step [128/1325], Loss: 2.1924 R2: 0.7353766616702464\n","Epoch [48/50], Step [192/1325], Loss: 2.1900 R2: 0.7953563905134353\n","Epoch [48/50], Step [256/1325], Loss: 3.4575 R2: 0.6965598317406052\n","Epoch [48/50], Step [320/1325], Loss: 3.2525 R2: 0.5814397899543655\n","Epoch [48/50], Step [384/1325], Loss: 5.2274 R2: 0.5769061356608318\n","Epoch [48/50], Step [448/1325], Loss: 7.3927 R2: 0.39362182030701665\n","Epoch [48/50], Step [512/1325], Loss: 5.5770 R2: 0.4098913730117031\n","Epoch [48/50], Step [576/1325], Loss: 5.5922 R2: 0.4630532474603921\n","Epoch [48/50], Step [640/1325], Loss: 3.6088 R2: 0.6986335888675081\n","Epoch [48/50], Step [704/1325], Loss: 2.0664 R2: 0.7868935823784072\n","Epoch [48/50], Step [768/1325], Loss: 1.7489 R2: 0.8406558837822988\n","Epoch [48/50], Step [832/1325], Loss: 2.0358 R2: 0.7868807029602298\n","Epoch [48/50], Step [896/1325], Loss: 2.9278 R2: 0.7197189065730867\n","Epoch [48/50], Step [960/1325], Loss: 4.5425 R2: 0.5675635305962432\n","Epoch [48/50], Step [1024/1325], Loss: 3.9968 R2: 0.5626737096484218\n","Epoch [48/50], Step [1088/1325], Loss: 5.1156 R2: 0.4664025438821574\n","Epoch [48/50], Step [1152/1325], Loss: 5.2873 R2: 0.5091399996247027\n","Epoch [48/50], Step [1216/1325], Loss: 6.2288 R2: 0.36547745448061253\n","Epoch [48/50], Step [1280/1325], Loss: 4.8381 R2: 0.39689574171611763\n","Epoch: 48/50.. Epoch Loss: 4.089978311396048.. Validation Loss: 8.906526692926926 R2: 0.2384471030980494\n","Epoch [49/50], Step [64/1325], Loss: 2.9958 R2: 0.6505543076592204\n","Epoch [49/50], Step [128/1325], Loss: 3.7890 R2: 0.5426552590462184\n","Epoch [49/50], Step [192/1325], Loss: 2.8685 R2: 0.7319626832814703\n","Epoch [49/50], Step [256/1325], Loss: 2.5337 R2: 0.7776399669120933\n","Epoch [49/50], Step [320/1325], Loss: 1.4055 R2: 0.8191270467813574\n","Epoch [49/50], Step [384/1325], Loss: 2.0962 R2: 0.8303420633650718\n","Epoch [49/50], Step [448/1325], Loss: 1.8957 R2: 0.8445107172389966\n","Epoch [49/50], Step [512/1325], Loss: 1.9466 R2: 0.7940245427773955\n","Epoch [49/50], Step [576/1325], Loss: 2.9933 R2: 0.7125916397437004\n","Epoch [49/50], Step [640/1325], Loss: 3.3411 R2: 0.7209905411856008\n","Epoch [49/50], Step [704/1325], Loss: 4.1091 R2: 0.5762223056084476\n","Epoch [49/50], Step [768/1325], Loss: 5.2946 R2: 0.5176172550749024\n","Epoch [49/50], Step [832/1325], Loss: 4.0585 R2: 0.5751288119382629\n","Epoch [49/50], Step [896/1325], Loss: 3.8419 R2: 0.6322106374400576\n","Epoch [49/50], Step [960/1325], Loss: 2.8595 R2: 0.727783115985775\n","Epoch [49/50], Step [1024/1325], Loss: 1.3926 R2: 0.8476257336738244\n","Epoch [49/50], Step [1088/1325], Loss: 1.4925 R2: 0.8443210028332743\n","Epoch [49/50], Step [1152/1325], Loss: 1.8917 R2: 0.8243842792015825\n","Epoch [49/50], Step [1216/1325], Loss: 3.2522 R2: 0.668696420834027\n","Epoch [49/50], Step [1280/1325], Loss: 3.4326 R2: 0.5721052822915569\n","Epoch: 49/50.. Epoch Loss: 3.055596241827345.. Validation Loss: 10.119236482674955 R2: 0.13475429906363856\n","Epoch [50/50], Step [64/1325], Loss: 2.5869 R2: 0.698246186499142\n","Epoch [50/50], Step [128/1325], Loss: 4.1417 R2: 0.5000873710780855\n","Epoch [50/50], Step [192/1325], Loss: 3.7037 R2: 0.6539149612903141\n","Epoch [50/50], Step [256/1325], Loss: 3.2470 R2: 0.7150349175289484\n","Epoch [50/50], Step [320/1325], Loss: 3.1267 R2: 0.5976203830947162\n","Epoch [50/50], Step [384/1325], Loss: 3.5296 R2: 0.7143195543537584\n","Epoch [50/50], Step [448/1325], Loss: 2.8039 R2: 0.7700145436304163\n","Epoch [50/50], Step [512/1325], Loss: 2.2899 R2: 0.7577022696853861\n","Epoch [50/50], Step [576/1325], Loss: 1.9832 R2: 0.8095767896318331\n","Epoch [50/50], Step [640/1325], Loss: 1.2721 R2: 0.893768445338155\n","Epoch [50/50], Step [704/1325], Loss: 0.9512 R2: 0.9019037039708522\n","Epoch [50/50], Step [768/1325], Loss: 2.4769 R2: 0.7743276846533171\n","Epoch [50/50], Step [832/1325], Loss: 2.0558 R2: 0.7847868546122024\n","Epoch [50/50], Step [896/1325], Loss: 3.9079 R2: 0.6258886826045129\n","Epoch [50/50], Step [960/1325], Loss: 4.2168 R2: 0.5985690205433774\n","Epoch [50/50], Step [1024/1325], Loss: 4.6746 R2: 0.48850739912071117\n","Epoch [50/50], Step [1088/1325], Loss: 3.3007 R2: 0.6557034075389825\n","Epoch [50/50], Step [1152/1325], Loss: 2.4219 R2: 0.7751559399687191\n","Epoch [50/50], Step [1216/1325], Loss: 1.5150 R2: 0.8456681422175858\n","Epoch [50/50], Step [1280/1325], Loss: 0.7150 R2: 0.9108654246852567\n","Epoch: 50/50.. Epoch Loss: 2.898918633738285.. Validation Loss: 8.865554474064531 R2: 0.24195043088119694\n"]}],"source":["train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, device, BATCH_SIZE)"]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'saved_models/multi_modal_1street_3.pth')"],"metadata":{"id":"Xrli9bOITh_s","executionInfo":{"status":"ok","timestamp":1670567181035,"user_tz":480,"elapsed":393,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"541EV6EvBTvU"},"source":["# Evaluate"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"UzmAIQYkBSqO","executionInfo":{"status":"ok","timestamp":1670567520000,"user_tz":480,"elapsed":354,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["def evaluate(model, device, test_loader, criterion):\n","  model.eval()\n","  test_loss = 0\n","  total = 0\n","  y_true = np.array([])\n","  y_pred = np.array([])\n","\n","  with torch.no_grad():\n","    for sat_inputs, street_inputs, targets in test_loader:\n","          sat_inputs = sat_inputs.to(device)\n","          targets = targets.to(device)\n","\n","          street_inputs = torch.stack(street_inputs, dim=1).squeeze(0)\n","          street_inputs = street_inputs.to(device)\n","\n","          output = model(sat_inputs, street_inputs)\n","          targets = targets.cpu().detach().numpy().squeeze()\n","          pred = output.cpu().detach().numpy().squeeze()\n","\n","          y_true = np.append(y_true, targets)\n","          y_pred = np.append(y_pred, pred)\n","\n","  # with torch.no_grad():\n","  #     for data, target in test_loader:\n","  #         data, target = data.to(device), target.to(device)\n","  #         total += target.size(0)\n","  #         output = model(data)\n","  #         target = target.cpu().detach().numpy().squeeze()\n","  #         pred = output.cpu().detach().numpy().squeeze()\n","  #         y_true = np.append(y_true, target)\n","  #         y_pred = np.append(y_pred, pred)\n","\n","  return y_true, y_pred"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"2KWjDsSmBfBg","executionInfo":{"status":"ok","timestamp":1670567800212,"user_tz":480,"elapsed":279643,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["y_test_preds, y_test = evaluate(model, device, test_loader, criterion)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1670567800214,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"5hItESg66lCw","outputId":"06329a1a-390d-4630-878a-11ea1bde285f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["284"]},"metadata":{},"execution_count":37}],"source":["len(y_test)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"vR7t-FcVfzFv","executionInfo":{"status":"ok","timestamp":1670567800215,"user_tz":480,"elapsed":26,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}}},"outputs":[],"source":["def calculate_metrics(pred, actual, verbose=True):\n","    result_metrics = {'mae' : mean_absolute_error(pred, actual),\n","                      'mape' : mean_absolute_percentage_error(pred, actual),\n","                      'mse' : mean_squared_error(pred, actual), \n","                      'rmse' : mean_squared_error(pred, actual) ** 0.5,\n","                      'r2': r2_score(pred, actual)\n","                      }\n","    \n","    if verbose:\n","      print(\"Mean Absolute Error:       \", result_metrics[\"mae\"])\n","      print(\"Mean Absolute Percentage Error:       \", result_metrics[\"mape\"])\n","      print(\"Mean Squared Error:   \", result_metrics[\"mse\"])\n","      print(\"Root Mean Squared Error:   \", result_metrics[\"rmse\"])\n","      print(\"R^2:   \", result_metrics[\"r2\"])\n","    return result_metrics"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":347,"status":"ok","timestamp":1670567963551,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"PggG1oHRh7hb","outputId":"2ee4ef6d-e98c-48e8-a64c-9cdf4de7b23a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute Error:        4.718254589173995\n","Mean Absolute Percentage Error:        73906542010253.4\n","Mean Squared Error:    38.86124278525396\n","Root Mean Squared Error:    6.233878630937079\n","R^2:    -2.7810525145010905\n"]}],"source":["metrics = calculate_metrics(y_test_preds, y_test)"]},{"cell_type":"code","source":[],"metadata":{"id":"wQ1KSWVwlX6N"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["s4JcaPH7rDQ2","zDrgLqlQbUUR"],"provenance":[{"file_id":"1NgRt-oQkfNw-3c7GigEUbEjCbRFq_xZC","timestamp":1670535378208},{"file_id":"11Cu1C2JKlRPuzKYFuSXGusthoWGqvRDE","timestamp":1670231146170},{"file_id":"1DvbM3z7J0U0c-3EXRDpjpzjy-o2NWRJP","timestamp":1670119902708},{"file_id":"1pox5ISxnu2Ka7mxWOge2FpGZ8X5LA504","timestamp":1669965274706}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}