{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24710,"status":"ok","timestamp":1670611810091,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"RtEzs3dSaSYZ","outputId":"6ede8595-96c0-4693-b378-921bcb7afe65"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":558,"status":"ok","timestamp":1670611810644,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"_v0i2laGayPd","outputId":"649e8666-23a7-43da-cf29-dc77fd6e8caf"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1UmSDcE4n7YnHUjCIvlrq4izeyNfRvNRj/cs230/womens_edu\n"]}],"source":["%cd /content/drive/MyDrive/Colab Notebooks/cs230/womens_edu"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10537,"status":"ok","timestamp":1670566684334,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"DoPQFgjG8ZSX","outputId":"c4470431-2c03-4bb7-aa35-ce7f65298696"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fast_ml\n","  Downloading fast_ml-3.68-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 207 kB/s \n","\u001b[?25hInstalling collected packages: fast-ml\n","Successfully installed fast-ml-3.68\n"]}],"source":["!pip install fast_ml\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ru_XmlQDa17B"},"outputs":[],"source":["import argparse\n","import logging\n","import os\n","\n","import h5py\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models\n","from torchvision import transforms, utils\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","from skimage import io, transform\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n","from sklearn.model_selection import train_test_split\n","from fast_ml.model_development import train_valid_test_split\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ko_lpQBEa5dP"},"outputs":[],"source":["dataset_root_dir = '/content/drive/MyDrive/Colab Notebooks/cs230/womens_edu/data/'"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"XMfPMgaH1nxi","executionInfo":{"status":"ok","timestamp":1670611813613,"user_tz":480,"elapsed":2980,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"}}},"outputs":[],"source":["!git config --global user.email \"hayah@stanford.edu\"\n","!git config --global user.name \"hayah08\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ne1RynH13_hL","executionInfo":{"status":"ok","timestamp":1670611813614,"user_tz":480,"elapsed":6,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"}}},"outputs":[],"source":["username='hayah08'\n","repository='cs230'\n","git_token='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'"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":960,"status":"ok","timestamp":1670611929857,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"msuxCKhQ4w3O","outputId":"e538cd86-dedc-4acc-bbf7-c33031fd5e95"},"outputs":[{"output_type":"stream","name":"stdout","text":["origin\thttps://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@github.com/hayah08/cs230.git (fetch)\n","origin\thttps://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@github.com/hayah08/cs230.git (push)\n"]}],"source":["!git remote -v"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"z6WqgPI06yTd","executionInfo":{"status":"ok","timestamp":1670611933574,"user_tz":480,"elapsed":3332,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"}}},"outputs":[],"source":["!git remote rm origin"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_kkiYY9S4Fwb","executionInfo":{"status":"ok","timestamp":1670611934243,"user_tz":480,"elapsed":678,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"}}},"outputs":[],"source":["!git remote add origin https://{git_token}@github.com/{username}/{repository}.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lithLEmz1qaT"},"outputs":[],"source":["!git push origin main"]},{"cell_type":"code","source":["!git add FinalHayamulti_modal_regressor.ipynb"],"metadata":{"id":"v1FoiT2vPJFZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m 'final multi-modal model for 5 street + 1 sat image'"],"metadata":{"id":"t-OfzHegPJ_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git push origin main"],"metadata":{"id":"IkghsW1YPLln"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r9RfKqCIljld"},"source":["# Data Processing "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670548425448,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"a9ALmpszliUN","outputId":"8054cd7c-3986-4529-bc30-5f970941a417"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-56f0e1ff-bb46-45dd-be87-4de4cd19798f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 1</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>DHSID_EA</th>\n","      <th>year</th>\n","      <th>cc</th>\n","      <th>lat</th>\n","      <th>lon</th>\n","      <th>women_edu</th>\n","      <th>path</th>\n","      <th>img_captured_at</th>\n","      <th>img_lon</th>\n","      <th>img_lat</th>\n","      <th>img_id</th>\n","      <th>img_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>74101</td>\n","      <td>74101</td>\n","      <td>AM-2010-6#-00000175</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.865949</td>\n","      <td>44.052637</td>\n","      <td>10.307692</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1341142789364</td>\n","      <td>44.118895</td>\n","      <td>40.914849</td>\n","      <td>455621522386245</td>\n","      <td>AM/AM-2010-6#-00000175/455621522386245.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>74102</td>\n","      <td>74102</td>\n","      <td>AM-2010-6#-00000176</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.878055</td>\n","      <td>44.042707</td>\n","      <td>10.761905</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1341142789364</td>\n","      <td>44.118895</td>\n","      <td>40.914849</td>\n","      <td>455621522386245</td>\n","      <td>AM/AM-2010-6#-00000176/455621522386245.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>74103</td>\n","      <td>74103</td>\n","      <td>AM-2010-6#-00000215</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.776914</td>\n","      <td>43.841243</td>\n","      <td>12.476190</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.853536</td>\n","      <td>40.802320</td>\n","      <td>448238129596253</td>\n","      <td>AM/AM-2010-6#-00000215/448238129596253.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>74104</td>\n","      <td>74104</td>\n","      <td>AM-2010-6#-00000218</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.808131</td>\n","      <td>43.840526</td>\n","      <td>12.600000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.852432</td>\n","      <td>40.798289</td>\n","      <td>803317710311828</td>\n","      <td>AM/AM-2010-6#-00000218/803317710311828.jpeg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>74105</td>\n","      <td>74105</td>\n","      <td>AM-2010-6#-00000232</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.765091</td>\n","      <td>43.783366</td>\n","      <td>11.480000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.853536</td>\n","      <td>40.802320</td>\n","      <td>448238129596253</td>\n","      <td>AM/AM-2010-6#-00000232/448238129596253.jpeg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56f0e1ff-bb46-45dd-be87-4de4cd19798f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-56f0e1ff-bb46-45dd-be87-4de4cd19798f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-56f0e1ff-bb46-45dd-be87-4de4cd19798f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Unnamed: 0  Unnamed: 1  Unnamed: 0.1             DHSID_EA  year  cc  \\\n","0           0       74101         74101  AM-2010-6#-00000175  2010  AM   \n","1           1       74102         74102  AM-2010-6#-00000176  2010  AM   \n","2           2       74103         74103  AM-2010-6#-00000215  2010  AM   \n","3           3       74104         74104  AM-2010-6#-00000218  2010  AM   \n","4           4       74105         74105  AM-2010-6#-00000232  2010  AM   \n","\n","         lat        lon  women_edu  \\\n","0  40.865949  44.052637  10.307692   \n","1  40.878055  44.042707  10.761905   \n","2  40.776914  43.841243  12.476190   \n","3  40.808131  43.840526  12.600000   \n","4  40.765091  43.783366  11.480000   \n","\n","                                                path  img_captured_at  \\\n","0  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1341142789364   \n","1  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1341142789364   \n","2  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","3  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","4  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","\n","     img_lon    img_lat           img_id  \\\n","0  44.118895  40.914849  455621522386245   \n","1  44.118895  40.914849  455621522386245   \n","2  43.853536  40.802320  448238129596253   \n","3  43.852432  40.798289  803317710311828   \n","4  43.853536  40.802320  448238129596253   \n","\n","                                      img_path  \n","0  AM/AM-2010-6#-00000175/455621522386245.jpeg  \n","1  AM/AM-2010-6#-00000176/455621522386245.jpeg  \n","2  AM/AM-2010-6#-00000215/448238129596253.jpeg  \n","3  AM/AM-2010-6#-00000218/803317710311828.jpeg  \n","4  AM/AM-2010-6#-00000232/448238129596253.jpeg  "]},"execution_count":153,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('data/filtered_sampled_ss.csv')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670548425449,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"-_IfIqTp-BW1","outputId":"0ff42f36-0f61-4bea-b420-94002a72a2df"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-509c1178-40d4-4efd-ab29-41f7565e0671\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 1</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>DHSID_EA</th>\n","      <th>year</th>\n","      <th>cc</th>\n","      <th>lat</th>\n","      <th>lon</th>\n","      <th>women_edu</th>\n","      <th>path</th>\n","      <th>img_captured_at</th>\n","      <th>img_lon</th>\n","      <th>img_lat</th>\n","      <th>img_id</th>\n","      <th>img_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>74101</td>\n","      <td>74101</td>\n","      <td>AM-2010-6#-00000175</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.865949</td>\n","      <td>44.052637</td>\n","      <td>10.307692</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1341142789364</td>\n","      <td>44.118895</td>\n","      <td>40.914849</td>\n","      <td>455621522386245</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>74102</td>\n","      <td>74102</td>\n","      <td>AM-2010-6#-00000176</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.878055</td>\n","      <td>44.042707</td>\n","      <td>10.761905</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1341142789364</td>\n","      <td>44.118895</td>\n","      <td>40.914849</td>\n","      <td>455621522386245</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>74103</td>\n","      <td>74103</td>\n","      <td>AM-2010-6#-00000215</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.776914</td>\n","      <td>43.841243</td>\n","      <td>12.476190</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.853536</td>\n","      <td>40.802320</td>\n","      <td>448238129596253</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>74104</td>\n","      <td>74104</td>\n","      <td>AM-2010-6#-00000218</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.808131</td>\n","      <td>43.840526</td>\n","      <td>12.600000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.852432</td>\n","      <td>40.798289</td>\n","      <td>803317710311828</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>74105</td>\n","      <td>74105</td>\n","      <td>AM-2010-6#-00000232</td>\n","      <td>2010</td>\n","      <td>AM</td>\n","      <td>40.765091</td>\n","      <td>43.783366</td>\n","      <td>11.480000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>1295470496500</td>\n","      <td>43.853536</td>\n","      <td>40.802320</td>\n","      <td>448238129596253</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-509c1178-40d4-4efd-ab29-41f7565e0671')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-509c1178-40d4-4efd-ab29-41f7565e0671 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-509c1178-40d4-4efd-ab29-41f7565e0671');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Unnamed: 0  Unnamed: 1  Unnamed: 0.1             DHSID_EA  year  cc  \\\n","0           0       74101         74101  AM-2010-6#-00000175  2010  AM   \n","1           1       74102         74102  AM-2010-6#-00000176  2010  AM   \n","2           2       74103         74103  AM-2010-6#-00000215  2010  AM   \n","3           3       74104         74104  AM-2010-6#-00000218  2010  AM   \n","4           4       74105         74105  AM-2010-6#-00000232  2010  AM   \n","\n","         lat        lon  women_edu  \\\n","0  40.865949  44.052637  10.307692   \n","1  40.878055  44.042707  10.761905   \n","2  40.776914  43.841243  12.476190   \n","3  40.808131  43.840526  12.600000   \n","4  40.765091  43.783366  11.480000   \n","\n","                                                path  img_captured_at  \\\n","0  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1341142789364   \n","1  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1341142789364   \n","2  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","3  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","4  /content/drive/MyDrive/Colab Notebooks/cs230/w...    1295470496500   \n","\n","     img_lon    img_lat           img_id  \\\n","0  44.118895  40.914849  455621522386245   \n","1  44.118895  40.914849  455621522386245   \n","2  43.853536  40.802320  448238129596253   \n","3  43.852432  40.798289  803317710311828   \n","4  43.853536  40.802320  448238129596253   \n","\n","                                            img_path  \n","0  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","1  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","2  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","3  /content/drive/MyDrive/Colab Notebooks/cs230/w...  \n","4  /content/drive/MyDrive/Colab Notebooks/cs230/w...  "]},"execution_count":154,"metadata":{},"output_type":"execute_result"}],"source":["df['img_path'] = dataset_root_dir + df['img_path']\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670548425449,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"4IBPB66uGbW0","outputId":"ecff3486-39b3-4ea2-84fd-08c8fec25c81"},"outputs":[{"data":{"text/plain":["8961"]},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":["# for now exclude these folders \n","# df = df[ df['cc'] != 'BJ'] # need to resize\n","len(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1670548425449,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"LYuvsLULl6o5","outputId":"a50e1719-334d-46f6-a7b4-1bccd1c29c47"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-663bdd33-a2ec-440f-a134-122168dd169f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DHSID_EA</th>\n","      <th>year</th>\n","      <th>women_edu</th>\n","      <th>path</th>\n","      <th>cc</th>\n","      <th>img_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1883</th>\n","      <td>ZW-2015-7#-00000362</td>\n","      <td>2015</td>\n","      <td>12.030303</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>ZW</td>\n","      <td>[/content/drive/MyDrive/Colab Notebooks/cs230/...</td>\n","    </tr>\n","    <tr>\n","      <th>1884</th>\n","      <td>ZW-2015-7#-00000369</td>\n","      <td>2015</td>\n","      <td>10.391304</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>ZW</td>\n","      <td>[/content/drive/MyDrive/Colab Notebooks/cs230/...</td>\n","    </tr>\n","    <tr>\n","      <th>1885</th>\n","      <td>ZW-2015-7#-00000374</td>\n","      <td>2015</td>\n","      <td>10.406250</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>ZW</td>\n","      <td>[/content/drive/MyDrive/Colab Notebooks/cs230/...</td>\n","    </tr>\n","    <tr>\n","      <th>1886</th>\n","      <td>ZW-2015-7#-00000378</td>\n","      <td>2015</td>\n","      <td>10.741935</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>ZW</td>\n","      <td>[/content/drive/MyDrive/Colab Notebooks/cs230/...</td>\n","    </tr>\n","    <tr>\n","      <th>1887</th>\n","      <td>ZW-2015-7#-00000381</td>\n","      <td>2015</td>\n","      <td>11.303030</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>ZW</td>\n","      <td>[/content/drive/MyDrive/Colab Notebooks/cs230/...</td>\n","    </tr>\n","    <tr>\n","      <th>1888</th>\n","      <td>ZW-2015-7#-00000390</td>\n","      <td>2015</td>\n","      <td>11.521739</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>ZW</td>\n","      <td>[/content/drive/MyDrive/Colab Notebooks/cs230/...</td>\n","    </tr>\n","    <tr>\n","      <th>1889</th>\n","      <td>ZW-2015-7#-00000392</td>\n","      <td>2015</td>\n","      <td>9.800000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>ZW</td>\n","      <td>[/content/drive/MyDrive/Colab Notebooks/cs230/...</td>\n","    </tr>\n","    <tr>\n","      <th>1890</th>\n","      <td>ZW-2015-7#-00000394</td>\n","      <td>2015</td>\n","      <td>9.594595</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>ZW</td>\n","      <td>[/content/drive/MyDrive/Colab Notebooks/cs230/...</td>\n","    </tr>\n","    <tr>\n","      <th>1891</th>\n","      <td>ZW-2015-7#-00000396</td>\n","      <td>2015</td>\n","      <td>9.750000</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>ZW</td>\n","      <td>[/content/drive/MyDrive/Colab Notebooks/cs230/...</td>\n","    </tr>\n","    <tr>\n","      <th>1892</th>\n","      <td>ZW-2015-7#-00000399</td>\n","      <td>2015</td>\n","      <td>11.243243</td>\n","      <td>/content/drive/MyDrive/Colab Notebooks/cs230/w...</td>\n","      <td>ZW</td>\n","      <td>[/content/drive/MyDrive/Colab Notebooks/cs230/...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-663bdd33-a2ec-440f-a134-122168dd169f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-663bdd33-a2ec-440f-a134-122168dd169f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-663bdd33-a2ec-440f-a134-122168dd169f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 DHSID_EA  year  women_edu  \\\n","1883  ZW-2015-7#-00000362  2015  12.030303   \n","1884  ZW-2015-7#-00000369  2015  10.391304   \n","1885  ZW-2015-7#-00000374  2015  10.406250   \n","1886  ZW-2015-7#-00000378  2015  10.741935   \n","1887  ZW-2015-7#-00000381  2015  11.303030   \n","1888  ZW-2015-7#-00000390  2015  11.521739   \n","1889  ZW-2015-7#-00000392  2015   9.800000   \n","1890  ZW-2015-7#-00000394  2015   9.594595   \n","1891  ZW-2015-7#-00000396  2015   9.750000   \n","1892  ZW-2015-7#-00000399  2015  11.243243   \n","\n","                                                   path  cc  \\\n","1883  /content/drive/MyDrive/Colab Notebooks/cs230/w...  ZW   \n","1884  /content/drive/MyDrive/Colab Notebooks/cs230/w...  ZW   \n","1885  /content/drive/MyDrive/Colab Notebooks/cs230/w...  ZW   \n","1886  /content/drive/MyDrive/Colab Notebooks/cs230/w...  ZW   \n","1887  /content/drive/MyDrive/Colab Notebooks/cs230/w...  ZW   \n","1888  /content/drive/MyDrive/Colab Notebooks/cs230/w...  ZW   \n","1889  /content/drive/MyDrive/Colab Notebooks/cs230/w...  ZW   \n","1890  /content/drive/MyDrive/Colab Notebooks/cs230/w...  ZW   \n","1891  /content/drive/MyDrive/Colab Notebooks/cs230/w...  ZW   \n","1892  /content/drive/MyDrive/Colab Notebooks/cs230/w...  ZW   \n","\n","                                               img_path  \n","1883  [/content/drive/MyDrive/Colab Notebooks/cs230/...  \n","1884  [/content/drive/MyDrive/Colab Notebooks/cs230/...  \n","1885  [/content/drive/MyDrive/Colab Notebooks/cs230/...  \n","1886  [/content/drive/MyDrive/Colab Notebooks/cs230/...  \n","1887  [/content/drive/MyDrive/Colab Notebooks/cs230/...  \n","1888  [/content/drive/MyDrive/Colab Notebooks/cs230/...  \n","1889  [/content/drive/MyDrive/Colab Notebooks/cs230/...  \n","1890  [/content/drive/MyDrive/Colab Notebooks/cs230/...  \n","1891  [/content/drive/MyDrive/Colab Notebooks/cs230/...  \n","1892  [/content/drive/MyDrive/Colab Notebooks/cs230/...  "]},"execution_count":156,"metadata":{},"output_type":"execute_result"}],"source":["# collapse the dataset s.t. each row is a satellite path with a list of img paths \n","dhsid_df = df.groupby(['DHSID_EA', 'year', 'women_edu', 'path', 'cc'])['img_path'].apply(list).reset_index()\n","dhsid_df.tail(10) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1670548425449,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"YbUwWZI-xZI_","outputId":"7b8b4353-eb72-4f13-8459-453363c461d8"},"outputs":[{"data":{"text/plain":["['/content/drive/MyDrive/Colab Notebooks/cs230/womens_edu/data/AM/AM-2016-7#-00000097/1161818567600875.jpeg',\n"," '/content/drive/MyDrive/Colab Notebooks/cs230/womens_edu/data/AM/AM-2016-7#-00000097/3911978088838780.jpeg',\n"," '/content/drive/MyDrive/Colab Notebooks/cs230/womens_edu/data/AM/AM-2016-7#-00000097/325791582223174.jpeg',\n"," '/content/drive/MyDrive/Colab Notebooks/cs230/womens_edu/data/AM/AM-2016-7#-00000097/340195134193338.jpeg',\n"," '/content/drive/MyDrive/Colab Notebooks/cs230/womens_edu/data/AM/AM-2016-7#-00000097/316863043145729.jpeg']"]},"execution_count":157,"metadata":{},"output_type":"execute_result"}],"source":["dhsid_df['img_path'][100]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"elapsed":942,"status":"ok","timestamp":1670548426386,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"pmH7bw3kv-5B","outputId":"ce14feb7-0a62-4622-ba26-9a90e19abf6f"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-15ff6d8d-783f-4465-acb8-1c22d275c571\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DHSID_EA</th>\n","      <th>year</th>\n","      <th>women_edu</th>\n","      <th>path</th>\n","      <th>img_path</th>\n","    </tr>\n","    <tr>\n","      <th>cc</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>AM</th>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","      <td>312</td>\n","    </tr>\n","    <tr>\n","      <th>BJ</th>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","      <td>265</td>\n","    </tr>\n","    <tr>\n","      <th>CD</th>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>37</td>\n","    </tr>\n","    <tr>\n","      <th>CM</th>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","      <td>443</td>\n","    </tr>\n","    <tr>\n","      <th>GH</th>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>184</td>\n","    </tr>\n","    <tr>\n","      <th>KY</th>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","      <td>83</td>\n","    </tr>\n","    <tr>\n","      <th>MD</th>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","      <td>127</td>\n","    </tr>\n","    <tr>\n","      <th>NM</th>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","    </tr>\n","    <tr>\n","      <th>NP</th>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>172</td>\n","    </tr>\n","    <tr>\n","      <th>ZW</th>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","      <td>90</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15ff6d8d-783f-4465-acb8-1c22d275c571')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-15ff6d8d-783f-4465-acb8-1c22d275c571 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-15ff6d8d-783f-4465-acb8-1c22d275c571');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["    DHSID_EA  year  women_edu  path  img_path\n","cc                                           \n","AM       312   312        312   312       312\n","BJ       265   265        265   265       265\n","CD        37    37         37    37        37\n","CM       443   443        443   443       443\n","GH       184   184        184   184       184\n","KY        83    83         83    83        83\n","MD       127   127        127   127       127\n","NM       180   180        180   180       180\n","NP       172   172        172   172       172\n","ZW        90    90         90    90        90"]},"execution_count":158,"metadata":{},"output_type":"execute_result"}],"source":["dhsid_df.groupby(['cc']).count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHE4wCTAeI13"},"outputs":[],"source":["# dhsid_df.to_csv('data/multi_modal_input.csv')"]},{"cell_type":"markdown","metadata":{"id":"F_mr_Fj6dXZK"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhnuHCRF3on7"},"outputs":[],"source":["def get_street_tensor(img_path_list):\n","  image_tensor_list = [] \n","  for img_path in img_path_list:\n","    image = io.imread(img_path)\n","    image = (image - image.min()) / (image.max() - image.min())\n","    image_tensor = torch.from_numpy(image)     \n","    image_tensor = image_tensor.permute(2,0,1).float()\n","    #print(image_tensor.shape)\n","    #print(img_path)\n","    image_tensor_list.append(image_tensor)\n","\n","  return image_tensor_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhfJGHrS33Kt"},"outputs":[],"source":["def get_satellite_tensor(img_path):\n","    image = io.imread(img_path)\n","    image = image[:3]\n","    image = image[::-1]\n","    image = (image - image.min()) / (image.max() - image.min())\n","    image_tensor = torch.from_numpy(image)     \n","\n","    return image_tensor "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JuCF77524YuE"},"outputs":[],"source":["# Combined Dataset class \n","class DHSIDDataset(Dataset):\n","  def __init__(self, df):\n","    self.satellite_path = df['path'].to_numpy()\n","    self.street_path = df['img_path'].to_numpy() # a list of paths \n","    self.targets = df['women_edu'].to_numpy()\n","\n","  def __len__(self):\n","    return self.satellite_path.shape[0] \n","\n","  def __getitem__(self, idx):\n","    sat_tensor = get_satellite_tensor(self.satellite_path[idx])\n","    street_tensor_list = get_street_tensor(self.street_path[idx]) # a list of tensors \n","    target = torch.Tensor(np.array([self.targets[idx]]))\n","\n","    return sat_tensor, street_tensor_list, target"]},{"cell_type":"markdown","metadata":{"id":"zDrgLqlQbUUR"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VE1UfKCMkkZH"},"outputs":[],"source":["def create_model(model):\n","    if model == 'resnet18':\n","        model = models.resnet18(pretrained=True)\n","    elif model == 'resnet34':\n","        model = models.resnet34(pretrained=True)\n","    elif model == 'resnet50':\n","        model = models.resnet50(pretrained=True)\n","    model = nn.Sequential(*list(model.children())[:-1]) # get model only up to second to last layer \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAumopwn3cd4"},"outputs":[],"source":["class SatelliteStreetModel(nn.Module):\n","  def __init__(self, sat_model, street_model):\n","    super(SatelliteStreetModel, self).__init__()\n","\n","    self.satellite_model = create_model(sat_model)\n","    self.street_model = create_model(street_model)\n","    self.activation2 = nn.LeakyReLU()\n","    self.fc1 = nn.Linear(512*2, 1000)\n","    self.fc2 = nn.Linear(1000, 750)\n","    self.fc3 = nn.Linear(750, 512)\n","    self.fc4 = nn.Linear(512, 256)\n","    self.fc5 = nn.Linear(256, 128)\n","    self.fc6 = nn.Linear(128,64)\n","    self.fclast = nn.Linear(64,1)\n","\n","\n","  def forward(self, sat_x, street_x):\n","    satellite_fv = self.satellite_model(sat_x)\n","    satellite_fv = torch.flatten(satellite_fv , start_dim=2)\n","    # print(satellite_fv.shape)\n","    street_fv = self.street_model(street_x) \n","    street_fv = torch.mean(street_fv, 0)\n","    street_fv = torch.flatten(satellite_fv, start_dim=2)\n","    # print(satellite_fv.shape, street_fv.shape)\n","    # print(street_fv.shape)\n","    concat_fv = torch.cat( (satellite_fv, street_fv), dim=1)\n","    # print(concat_fv.shape)\n","    concat_fv = torch.flatten(concat_fv, start_dim = 1)\n","    # TO DO: add more Dense/FC layers\n","    concat_fv = self.fc1(concat_fv)\n","    concat_fv = self.activation2(concat_fv)\n","    concat_fv = self.fc2(concat_fv)\n","    concat_fv = self.activation2(concat_fv)\n","    concat_fv = self.fc3(concat_fv)\n","    concat_fv = self.activation2(concat_fv)\n","    concat_fv = self.fc4(concat_fv)\n","    concat_fv = self.fc5(concat_fv)\n","    concat_fv = self.fc6(concat_fv)\n","\n","    # print(concat_fv.shape)\n","    # print('first activ + FC ayer')\n","    # concat_fc = self.activation(concat_fv)\n","    # concat_fv = torch.squeeze(concat_fv)\n","    out = self.fclast(concat_fv)\n","    # out = torch.squeeze(out)\n","    # print('second layer')\n","    # print(out.shape)\n","    # print('done')\n","    return out"]},{"cell_type":"markdown","metadata":{"id":"XQQjRAjacXTJ"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9G3NajicdO3S"},"outputs":[],"source":["def split_train_val_test(df):\n","  X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(df, target = 'women_edu', \n","                                                                            train_size=0.6, valid_size=0.2, test_size=0.2)\n","  X_train['women_edu'] = y_train\n","  train_df = X_train\n","  X_valid['women_edu'] = y_valid\n","  val_df = X_valid\n","  X_test['women_edu'] = y_test\n","  test_df = X_test\n","  return train_df, val_df, test_df \n","\n","\n","\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDEz3sK9WtiS"},"outputs":[],"source":["def split_train_val_test2(df, testlist, vallist):\n","  test_df = df.loc[df['cc'].isin(testlist)]\n","  val_df = df.loc[df['cc'].isin(vallist)]\n","  train_df = df.loc[~df['cc'].isin(testlist + vallist)]\n","\n","  return train_df, val_df, test_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksrTxeUA-KtB"},"outputs":[],"source":["tb = SummaryWriter()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1670548426388,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"6sPO5QBvrwBk","outputId":"53ede293-c29a-4d55-97e6-4d4f8bf3c585"},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["# use GPU \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CiUBPXZaAPWv"},"outputs":[],"source":["def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, batch_size, verbose=True):\n","    batch_size = torch.from_numpy(np.array(batch_size)).to(device)\n","\n","    min_val_loss = np.inf\n","    for epoch in range(1, num_epochs + 1):\n","        preds = []\n","        trues = []\n","        epoch_loss = 0.0\n","        batch_loss = torch.tensor([0.0], requires_grad=True)\n","        batch_loss =  batch_loss.to(device)\n","        batch_num = 1\n","\n","        optimizer.zero_grad()\n","\n","        for sat_inputs, street_inputs, targets in train_loader:\n","            # satellite \n","            sat_inputs = sat_inputs.to(device)\n","            # true values\n","            targets = targets.to(device)\n","            # street \n","            street_inputs = torch.stack(street_inputs, dim=1).squeeze(0)\n","            street_inputs = street_inputs.to(device)\n","            # model\n","            output = model(sat_inputs, street_inputs)\n","            # threshold output to be between 0 and 18\n","            #if output > 18.0:\n","              #output = 18.0\n","            #if output < 0.0:\n","              #output = 0.0\n","            # print(output, targets[0])\n","            loss = criterion(output, targets[0]) \n","            # investigate targets[0]\n","            # r2 prep\n","            pred = output.cpu().detach().numpy()\n","            pred = np.asscalar(pred)\n","            preds.append(pred)\n","            true = targets[0].cpu().detach().numpy()\n","            true = np.asscalar(true)\n","            trues.append(true)\n","            # batch_loss, epoch_loss\n","            batch_loss = batch_loss + loss\n","            epoch_loss += loss.item()\n"," \n","            # when batch num (num iterations) reaches batch_size\n","            if batch_num % batch_size.item() == 0:\n","              # calculate r2 at batch \n","              numpreds = np.array(preds)\n","              numtrues = np.array(trues)\n","              r2 = r2_score(numtrues, numpreds) \n","              preds = []\n","              trues = []\n","              # batch_loss\n","              batch_loss = batch_loss / batch_size.item()\n","              # clear grads + backprop\n","              batch_loss.backward() # batch loss\n","              optimizer.step()\n","              optimizer.zero_grad()\n","              # if verbose then print these things\n","              if verbose:                              \n","                print(f'Epoch [{epoch}/{num_epochs}], Step [{batch_num}/{len(train_loader)}], '\n","                      f'Loss: {batch_loss.item():.4f}',\n","                      f'R2: {r2}')\n","                \n","              batch_loss = torch.tensor([0.0], requires_grad=True)\n","              batch_loss =  batch_loss.to(device)\n","  \n","\n","            batch_num += 1\n","\n","        # init valid_loss\n","        valid_loss = 0.0  \n","        # Validation loop\n","        with torch.no_grad():\n","          preds = []\n","          trues = []\n","          for sat_inputs, street_inputs, targets in val_loader:\n","            sat_inputs = sat_inputs.to(device)\n","            targets = targets.to(device)\n","\n","            street_inputs = torch.stack(street_inputs, dim=1).squeeze(0)\n","            street_inputs = street_inputs.to(device)\n","\n","            output = model(sat_inputs, street_inputs)\n","            # loop for r2\n","            pred = output.cpu().detach().numpy()\n","            pred = np.asscalar(pred)\n","            preds.append(pred)\n","            true = targets[0].cpu().detach().numpy()\n","            true = np.asscalar(true)\n","            trues.append(true)\n","            # loss and validation_loss \n","            loss = criterion(output,targets[0])\n","            valid_loss += loss.item()\n","        \n","        #r2 processing\n","        numpreds = np.array(preds)\n","        numtrues = np.array(trues)\n","        r2val = r2_score(numtrues, numpreds)\n","        # epoch loss + val loss\n","        epoch_loss /= len(train_loader)\n","        valid_loss /= len(val_loader)\n","        print(f'Epoch: {epoch}/{num_epochs}.. Epoch Loss: {epoch_loss}.. Validation Loss: {valid_loss}',\n","              f'R2: {r2val}')\n","        # save model with least validation loss\n","        if valid_loss < min_val_loss:\n","          print(f'Validation Loss Decreased({min_val_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n","          # reset min_val_loss to this one\n","          min_val_loss = valid_loss\n","          # save model\n","          torch.save(model.state_dict(), 'best_model.pth')\n","        # after printing reset total/epoch_loss\n","\n","\n","\n","\n","# train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, device, BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"qppsfyXPMtBo"},"source":["# Run"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1670548426389,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"q1X9OGbeyrUc","outputId":"dad3c70a-32f3-4a7b-93e6-669336613301"},"outputs":[{"name":"stdout","output_type":"stream","text":["hi\n"]}],"source":["if 1:\n","  print(\"hi\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670548426389,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"wwponmNbbgcv","outputId":"1450221c-1c39-4e5b-c362-fd757cb427b0"},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["# use GPU \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21GGDl7VnORt"},"outputs":[],"source":["# Hyperparameters\n","BATCH_SIZE = 64\n","LEARNING_RATE = 0.00005\n","NUM_EPOCHS = 50\n","WEIGHT_DECAY = 1e-3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670548426389,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"3trJVAaSfuJh","outputId":"cc2d5ba5-3e7e-4797-97bf-fbaf7124364d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Preparing the dataloader\n"]}],"source":["print('Preparing the dataloader')\n","df = dhsid_df.sample(frac=1, random_state = 1234) # shuffle dataset \n","# original random split\n","train_df, val_df, test_df = split_train_val_test(df)\n","\n","\n","# testlist = ['AM']\n","# vallist = ['BJ']\n","\n","# train_df, val_df, test_df = split_train_val_test2(df, testlist, vallist)\n","\n","# load images\n","train_imgs = DHSIDDataset(train_df)\n","val_imgs = DHSIDDataset(val_df)\n","test_imgs = DHSIDDataset(test_df)\n","# DataLoader\n","\n","train_loader = DataLoader(train_imgs, batch_size=1, num_workers=2) # always set batch_size = 1 here \n","val_loader = DataLoader(val_imgs, batch_size=1, num_workers=2)\n","test_loader = DataLoader(test_imgs, batch_size=1, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670548426390,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"8wunM-fUK2pM","outputId":"210a8de6-4eb5-4650-f2f2-dfd794c68e51"},"outputs":[{"data":{"text/plain":["count    1135.000000\n","mean        8.366977\n","std         3.321694\n","min         0.000000\n","25%         5.964815\n","50%         8.791667\n","75%        11.095455\n","max        15.318182\n","Name: women_edu, dtype: float64"]},"execution_count":174,"metadata":{},"output_type":"execute_result"}],"source":["train_df['women_edu'].describe() "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8TSor1VMSob"},"outputs":[],"source":["model = SatelliteStreetModel('resnet18', 'resnet50') # resnet18 is the best model for satellite model \n","model.to(device)\n","criterion = torch.nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1670551869862,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"f1MOXZAbfj-Y","outputId":"2f66304b-a5d7-4f14-d3c7-e7d1e896ac07"},"outputs":[{"name":"stdout","output_type":"stream","text":["1135 379\n"]}],"source":["print(len(train_imgs), len(test_imgs))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"YhmJO2-QAz2I","outputId":"fc88b67e-f00a-4d9a-80a0-9a326a823350"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/50], Step [64/1135], Loss: 91.5638 R2: -8.465805692771628\n","Epoch [1/50], Step [128/1135], Loss: 71.4847 R2: -6.3749038406609495\n","Epoch [1/50], Step [192/1135], Loss: 81.4452 R2: -6.720073761701052\n","Epoch [1/50], Step [256/1135], Loss: 78.2803 R2: -10.22520228001472\n","Epoch [1/50], Step [320/1135], Loss: 82.8190 R2: -6.190291268298567\n","Epoch [1/50], Step [384/1135], Loss: 82.7894 R2: -6.912370671601357\n","Epoch [1/50], Step [448/1135], Loss: 86.0441 R2: -6.918852219150046\n","Epoch [1/50], Step [512/1135], Loss: 73.2915 R2: -5.390905269285694\n","Epoch [1/50], Step [576/1135], Loss: 75.1337 R2: -4.602228052699864\n","Epoch [1/50], Step [640/1135], Loss: 73.5732 R2: -4.999714772277084\n","Epoch [1/50], Step [704/1135], Loss: 65.6052 R2: -4.860717218664548\n","Epoch [1/50], Step [768/1135], Loss: 73.6134 R2: -6.212497211396883\n","Epoch [1/50], Step [832/1135], Loss: 74.9862 R2: -5.508299889010945\n","Epoch [1/50], Step [896/1135], Loss: 75.6657 R2: -5.847890232967493\n","Epoch [1/50], Step [960/1135], Loss: 75.3915 R2: -6.943878132806376\n","Epoch [1/50], Step [1024/1135], Loss: 74.8538 R2: -7.338999036935476\n","Epoch [1/50], Step [1088/1135], Loss: 67.6230 R2: -5.249842024501864\n","Epoch: 1/50.. Epoch Loss: 76.39002374875626.. Validation Loss: 65.61434653916996 R2: -5.44932967121942\n","Validation Loss Decreased(inf--->65.614347) \t Saving The Model\n","Epoch [2/50], Step [64/1135], Loss: 78.0844 R2: -7.072309534417446\n","Epoch [2/50], Step [128/1135], Loss: 58.5458 R2: -5.040028077711043\n","Epoch [2/50], Step [192/1135], Loss: 65.8682 R2: -5.243556376062828\n","Epoch [2/50], Step [256/1135], Loss: 60.7484 R2: -7.711170220714893\n","Epoch [2/50], Step [320/1135], Loss: 63.1149 R2: -4.47959999887032\n","Epoch [2/50], Step [384/1135], Loss: 60.4863 R2: -4.780815913562804\n","Epoch [2/50], Step [448/1135], Loss: 60.4838 R2: -4.566472997430058\n","Epoch [2/50], Step [512/1135], Loss: 47.6198 R2: -3.152377093832354\n","Epoch [2/50], Step [576/1135], Loss: 46.4761 R2: -2.4654197671482287\n","Epoch [2/50], Step [640/1135], Loss: 41.7956 R2: -2.4083284473578144\n","Epoch [2/50], Step [704/1135], Loss: 32.8672 R2: -1.9361269119101472\n","Epoch [2/50], Step [768/1135], Loss: 33.9688 R2: -2.3281954571518026\n","Epoch [2/50], Step [832/1135], Loss: 31.4113 R2: -1.7262930972688468\n","Epoch [2/50], Step [896/1135], Loss: 27.5362 R2: -1.4920760723265776\n","Epoch [2/50], Step [960/1135], Loss: 22.4585 R2: -1.3664159957139241\n","Epoch [2/50], Step [1024/1135], Loss: 17.9517 R2: -0.9998890521296151\n","Epoch [2/50], Step [1088/1135], Loss: 13.9162 R2: -0.28615931566363106\n","Epoch: 2/50.. Epoch Loss: 43.800551706585914.. Validation Loss: 11.090508125658236 R2: -0.09010219167979505\n","Validation Loss Decreased(65.614347--->11.090508) \t Saving The Model\n","Epoch [3/50], Step [64/1135], Loss: 12.8067 R2: -0.32394803656234483\n","Epoch [3/50], Step [128/1135], Loss: 9.7984 R2: -0.01087579354878443\n","Epoch [3/50], Step [192/1135], Loss: 11.0106 R2: -0.0436815412738063\n","Epoch [3/50], Step [256/1135], Loss: 8.9334 R2: -0.28102371217941613\n","Epoch [3/50], Step [320/1135], Loss: 15.6077 R2: -0.35505204427336556\n","Epoch [3/50], Step [384/1135], Loss: 15.9486 R2: -0.524246114597605\n","Epoch [3/50], Step [448/1135], Loss: 16.1495 R2: -0.4862806395364494\n","Epoch [3/50], Step [512/1135], Loss: 20.3641 R2: -0.7757233168494528\n","Epoch [3/50], Step [576/1135], Loss: 20.4498 R2: -0.5248073429696549\n","Epoch [3/50], Step [640/1135], Loss: 17.0729 R2: -0.39225103650308357\n","Epoch [3/50], Step [704/1135], Loss: 15.5261 R2: -0.38699757059406004\n","Epoch [3/50], Step [768/1135], Loss: 10.9741 R2: -0.07521639306016659\n","Epoch [3/50], Step [832/1135], Loss: 11.5284 R2: -0.0005878184058603075\n","Epoch [3/50], Step [896/1135], Loss: 11.0326 R2: 0.0015273796859837097\n","Epoch [3/50], Step [960/1135], Loss: 10.0297 R2: -0.05680881137559646\n","Epoch [3/50], Step [1024/1135], Loss: 10.3127 R2: -0.14887835317550935\n","Epoch [3/50], Step [1088/1135], Loss: 11.5518 R2: -0.06763396945102129\n","Epoch: 3/50.. Epoch Loss: 13.684408263048187.. Validation Loss: 11.369959594430533 R2: -0.11756988660647893\n","Epoch [4/50], Step [64/1135], Loss: 13.2536 R2: -0.37014926529847547\n","Epoch [4/50], Step [128/1135], Loss: 10.2549 R2: -0.0579719006257331\n","Epoch [4/50], Step [192/1135], Loss: 12.4730 R2: -0.18230271351137817\n","Epoch [4/50], Step [256/1135], Loss: 9.0052 R2: -0.29132438180545916\n","Epoch [4/50], Step [320/1135], Loss: 13.3175 R2: -0.15621572097128977\n","Epoch [4/50], Step [384/1135], Loss: 12.1793 R2: -0.1639990272280374\n","Epoch [4/50], Step [448/1135], Loss: 12.6510 R2: -0.16430054032931807\n","Epoch [4/50], Step [512/1135], Loss: 11.4814 R2: -0.0011580275530655992\n","Epoch [4/50], Step [576/1135], Loss: 13.2064 R2: 0.015281712968984729\n","Epoch [4/50], Step [640/1135], Loss: 12.0211 R2: 0.019708355067349803\n","Epoch [4/50], Step [704/1135], Loss: 11.2499 R2: -0.004992066162359254\n","Epoch [4/50], Step [768/1135], Loss: 9.9745 R2: 0.022721129794359585\n","Epoch [4/50], Step [832/1135], Loss: 11.2630 R2: 0.02244477065709294\n","Epoch [4/50], Step [896/1135], Loss: 10.7644 R2: 0.025803419958769558\n","Epoch [4/50], Step [960/1135], Loss: 9.2159 R2: 0.02893366207237913\n","Epoch [4/50], Step [1024/1135], Loss: 8.6871 R2: 0.03222707360078014\n","Epoch [4/50], Step [1088/1135], Loss: 10.9041 R2: -0.0077735639464633355\n","Epoch: 4/50.. Epoch Loss: 11.596678281926616.. Validation Loss: 10.536015219957104 R2: -0.03560026930777482\n","Validation Loss Decreased(11.090508--->10.536015) \t Saving The Model\n","Epoch [5/50], Step [64/1135], Loss: 9.4210 R2: 0.026068540705840082\n","Epoch [5/50], Step [128/1135], Loss: 10.3532 R2: -0.0681112908347179\n","Epoch [5/50], Step [192/1135], Loss: 10.3049 R2: 0.023212551007994464\n","Epoch [5/50], Step [256/1135], Loss: 6.8183 R2: 0.022266023535466473\n","Epoch [5/50], Step [320/1135], Loss: 11.1276 R2: 0.033911580552664855\n","Epoch [5/50], Step [384/1135], Loss: 10.0368 R2: 0.04076038003326721\n","Epoch [5/50], Step [448/1135], Loss: 10.4301 R2: 0.04008998203735803\n","Epoch [5/50], Step [512/1135], Loss: 11.2550 R2: 0.018582555497257403\n","Epoch [5/50], Step [576/1135], Loss: 12.8855 R2: 0.039213239518843035\n","Epoch [5/50], Step [640/1135], Loss: 11.7334 R2: 0.043169874586504675\n","Epoch [5/50], Step [704/1135], Loss: 11.0086 R2: 0.016569787565224425\n","Epoch [5/50], Step [768/1135], Loss: 9.5773 R2: 0.06163449936274035\n","Epoch [5/50], Step [832/1135], Loss: 10.8784 R2: 0.05582466371533601\n","Epoch [5/50], Step [896/1135], Loss: 10.5433 R2: 0.045812876171563044\n","Epoch [5/50], Step [960/1135], Loss: 9.2913 R2: 0.020987863040838217\n","Epoch [5/50], Step [1024/1135], Loss: 8.8060 R2: 0.018978046838142748\n","Epoch [5/50], Step [1088/1135], Loss: 10.1162 R2: 0.06503884501586532\n","Epoch: 5/50.. Epoch Loss: 10.574694308266723.. Validation Loss: 10.000671035873648 R2: 0.017019481591118546\n","Validation Loss Decreased(10.536015--->10.000671) \t Saving The Model\n","Epoch [6/50], Step [64/1135], Loss: 9.8823 R2: -0.021623423830064326\n","Epoch [6/50], Step [128/1135], Loss: 8.9294 R2: 0.07877094761438608\n","Epoch [6/50], Step [192/1135], Loss: 9.6026 R2: 0.08978601113064844\n","Epoch [6/50], Step [256/1135], Loss: 6.3697 R2: 0.08660608846896511\n","Epoch [6/50], Step [320/1135], Loss: 10.5360 R2: 0.0852684321513848\n","Epoch [6/50], Step [384/1135], Loss: 9.5131 R2: 0.09080659389612367\n","Epoch [6/50], Step [448/1135], Loss: 9.7578 R2: 0.10196885904781727\n","Epoch [6/50], Step [512/1135], Loss: 10.7396 R2: 0.06351947136374936\n","Epoch [6/50], Step [576/1135], Loss: 12.1587 R2: 0.09340581495072342\n","Epoch [6/50], Step [640/1135], Loss: 11.0733 R2: 0.09699713971371349\n","Epoch [6/50], Step [704/1135], Loss: 10.6304 R2: 0.050348438638393\n","Epoch [6/50], Step [768/1135], Loss: 8.5565 R2: 0.16165370998621342\n","Epoch [6/50], Step [832/1135], Loss: 9.6677 R2: 0.16090484884283118\n","Epoch [6/50], Step [896/1135], Loss: 9.1873 R2: 0.16853597004053356\n","Epoch [6/50], Step [960/1135], Loss: 8.1987 R2: 0.1361197465874966\n","Epoch [6/50], Step [1024/1135], Loss: 7.5472 R2: 0.15921283777740702\n","Epoch [6/50], Step [1088/1135], Loss: 8.8273 R2: 0.18416901640238403\n","Epoch: 6/50.. Epoch Loss: 9.7837170031761.. Validation Loss: 9.39848172728279 R2: 0.0762095468460916\n","Validation Loss Decreased(10.000671--->9.398482) \t Saving The Model\n","Epoch [7/50], Step [64/1135], Loss: 8.3804 R2: 0.13363738438233475\n","Epoch [7/50], Step [128/1135], Loss: 7.7379 R2: 0.2016981619142243\n","Epoch [7/50], Step [192/1135], Loss: 7.9298 R2: 0.24834294115762046\n","Epoch [7/50], Step [256/1135], Loss: 5.2789 R2: 0.24301912534922043\n","Epoch [7/50], Step [320/1135], Loss: 8.9843 R2: 0.21998777099179323\n","Epoch [7/50], Step [384/1135], Loss: 8.0408 R2: 0.23151790552199414\n","Epoch [7/50], Step [448/1135], Loss: 7.7718 R2: 0.2847401945133913\n","Epoch [7/50], Step [512/1135], Loss: 8.9889 R2: 0.21618196512116516\n","Epoch [7/50], Step [576/1135], Loss: 9.3750 R2: 0.3009700138447886\n","Epoch [7/50], Step [640/1135], Loss: 8.4342 R2: 0.3122119858079212\n","Epoch [7/50], Step [704/1135], Loss: 7.9496 R2: 0.2898375865623406\n","Epoch [7/50], Step [768/1135], Loss: 6.0528 R2: 0.4069599040623201\n","Epoch [7/50], Step [832/1135], Loss: 7.1218 R2: 0.3818763031401132\n","Epoch [7/50], Step [896/1135], Loss: 6.4626 R2: 0.4151262189670062\n","Epoch [7/50], Step [960/1135], Loss: 5.8733 R2: 0.3811390273885057\n","Epoch [7/50], Step [1024/1135], Loss: 4.8298 R2: 0.4619381595777622\n","Epoch [7/50], Step [1088/1135], Loss: 5.6790 R2: 0.475140652375193\n","Epoch: 7/50.. Epoch Loss: 7.6683501495658755.. Validation Loss: 8.288420554133046 R2: 0.1853190761749599\n","Validation Loss Decreased(9.398482--->8.288421) \t Saving The Model\n","Epoch [8/50], Step [64/1135], Loss: 5.2072 R2: 0.4616884315866101\n","Epoch [8/50], Step [128/1135], Loss: 5.4265 R2: 0.44015965900768506\n","Epoch [8/50], Step [192/1135], Loss: 4.6685 R2: 0.5574779462363924\n","Epoch [8/50], Step [256/1135], Loss: 3.3210 R2: 0.5237712544673134\n","Epoch [8/50], Step [320/1135], Loss: 5.6006 R2: 0.5137613887237453\n","Epoch [8/50], Step [384/1135], Loss: 5.0156 R2: 0.5206485058783574\n","Epoch [8/50], Step [448/1135], Loss: 4.8851 R2: 0.5504144314011088\n","Epoch [8/50], Step [512/1135], Loss: 5.4225 R2: 0.5271676938475061\n","Epoch [8/50], Step [576/1135], Loss: 5.8869 R2: 0.561054463447437\n","Epoch [8/50], Step [640/1135], Loss: 4.9949 R2: 0.5926809867040698\n","Epoch [8/50], Step [704/1135], Loss: 5.0888 R2: 0.5454043180463715\n","Epoch [8/50], Step [768/1135], Loss: 3.3255 R2: 0.6741768200773459\n","Epoch [8/50], Step [832/1135], Loss: 4.2192 R2: 0.6338011696932764\n","Epoch [8/50], Step [896/1135], Loss: 3.5045 R2: 0.6828398404898838\n","Epoch [8/50], Step [960/1135], Loss: 3.1268 R2: 0.6705332108168481\n","Epoch [8/50], Step [1024/1135], Loss: 2.7853 R2: 0.6897109628183795\n","Epoch [8/50], Step [1088/1135], Loss: 3.2620 R2: 0.6985226465460619\n","Epoch: 8/50.. Epoch Loss: 4.865659318643503.. Validation Loss: 7.889072286422194 R2: 0.2245715981342402\n","Validation Loss Decreased(8.288421--->7.889072) \t Saving The Model\n","Epoch [9/50], Step [64/1135], Loss: 3.3098 R2: 0.6578350128755275\n","Epoch [9/50], Step [128/1135], Loss: 2.9909 R2: 0.6914401177649692\n","Epoch [9/50], Step [192/1135], Loss: 2.6624 R2: 0.7476323874077899\n","Epoch [9/50], Step [256/1135], Loss: 2.0545 R2: 0.7053848763988233\n","Epoch [9/50], Step [320/1135], Loss: 3.2381 R2: 0.7188710974391818\n","Epoch [9/50], Step [384/1135], Loss: 2.9100 R2: 0.7218858045889107\n","Epoch [9/50], Step [448/1135], Loss: 3.0922 R2: 0.7154137354180544\n","Epoch [9/50], Step [512/1135], Loss: 2.9437 R2: 0.7433168728902431\n","Epoch [9/50], Step [576/1135], Loss: 3.4866 R2: 0.7400272989418384\n","Epoch [9/50], Step [640/1135], Loss: 3.2417 R2: 0.7356457408298014\n","Epoch [9/50], Step [704/1135], Loss: 3.0398 R2: 0.7284417724420578\n","Epoch [9/50], Step [768/1135], Loss: 2.0121 R2: 0.8028563573432008\n","Epoch [9/50], Step [832/1135], Loss: 2.7802 R2: 0.7586942544165461\n","Epoch [9/50], Step [896/1135], Loss: 2.3754 R2: 0.7850237048151943\n","Epoch [9/50], Step [960/1135], Loss: 1.9964 R2: 0.7896442870576794\n","Epoch [9/50], Step [1024/1135], Loss: 1.8037 R2: 0.7990561074467007\n","Epoch [9/50], Step [1088/1135], Loss: 1.9972 R2: 0.8154182033993165\n","Epoch: 9/50.. Epoch Loss: 3.191870008565408.. Validation Loss: 8.265279342325005 R2: 0.18759366134239341\n","Epoch [10/50], Step [64/1135], Loss: 2.3067 R2: 0.7615322213836928\n","Epoch [10/50], Step [128/1135], Loss: 2.5658 R2: 0.7352963842088263\n","Epoch [10/50], Step [192/1135], Loss: 2.3628 R2: 0.7760335602125459\n","Epoch [10/50], Step [256/1135], Loss: 1.5722 R2: 0.7745441014362994\n","Epoch [10/50], Step [320/1135], Loss: 2.1699 R2: 0.8116126037656567\n","Epoch [10/50], Step [384/1135], Loss: 2.0150 R2: 0.807423157769729\n","Epoch [10/50], Step [448/1135], Loss: 3.0400 R2: 0.7202232652254706\n","Epoch [10/50], Step [512/1135], Loss: 3.1212 R2: 0.7278392446391548\n","Epoch [10/50], Step [576/1135], Loss: 3.4267 R2: 0.7444941392644961\n","Epoch [10/50], Step [640/1135], Loss: 3.0575 R2: 0.750667247136207\n","Epoch [10/50], Step [704/1135], Loss: 2.6658 R2: 0.7618566562099962\n","Epoch [10/50], Step [768/1135], Loss: 1.4580 R2: 0.8571460200386681\n","Epoch [10/50], Step [832/1135], Loss: 2.3700 R2: 0.7943037805165811\n","Epoch [10/50], Step [896/1135], Loss: 2.3595 R2: 0.7864564503225102\n","Epoch [10/50], Step [960/1135], Loss: 2.0080 R2: 0.7884195511305498\n","Epoch [10/50], Step [1024/1135], Loss: 1.7401 R2: 0.8061514380927581\n","Epoch [10/50], Step [1088/1135], Loss: 1.7341 R2: 0.83972938189723\n","Epoch: 10/50.. Epoch Loss: 2.843252900829036.. Validation Loss: 7.934552737304665 R2: 0.2201012528447901\n","Epoch [11/50], Step [64/1135], Loss: 1.8228 R2: 0.811565362195011\n","Epoch [11/50], Step [128/1135], Loss: 1.7218 R2: 0.8223636707409395\n","Epoch [11/50], Step [192/1135], Loss: 1.6238 R2: 0.8460804946721987\n","Epoch [11/50], Step [256/1135], Loss: 1.1459 R2: 0.835681621023771\n","Epoch [11/50], Step [320/1135], Loss: 1.9286 R2: 0.8325636360793331\n","Epoch [11/50], Step [384/1135], Loss: 1.9786 R2: 0.8109051117602889\n","Epoch [11/50], Step [448/1135], Loss: 2.5159 R2: 0.7684543494304562\n","Epoch [11/50], Step [512/1135], Loss: 2.0065 R2: 0.8250331400643768\n","Epoch [11/50], Step [576/1135], Loss: 2.4763 R2: 0.8153602301658769\n","Epoch [11/50], Step [640/1135], Loss: 2.7330 R2: 0.777131088154183\n","Epoch [11/50], Step [704/1135], Loss: 2.5118 R2: 0.7756140290316204\n","Epoch [11/50], Step [768/1135], Loss: 1.7182 R2: 0.8316563913679345\n","Epoch [11/50], Step [832/1135], Loss: 2.4547 R2: 0.7869527900937155\n","Epoch [11/50], Step [896/1135], Loss: 1.6865 R2: 0.8473718609378915\n","Epoch [11/50], Step [960/1135], Loss: 1.4144 R2: 0.8509664164721084\n","Epoch [11/50], Step [1024/1135], Loss: 1.3519 R2: 0.8493919374452449\n","Epoch [11/50], Step [1088/1135], Loss: 1.6680 R2: 0.8458420440623603\n","Epoch: 11/50.. Epoch Loss: 2.4707807643849287.. Validation Loss: 9.68326638314739 R2: 0.04821764749143875\n","Epoch [12/50], Step [64/1135], Loss: 2.2157 R2: 0.7709460521904253\n","Epoch [12/50], Step [128/1135], Loss: 1.8135 R2: 0.8129068452753421\n","Epoch [12/50], Step [192/1135], Loss: 1.8844 R2: 0.821376686390572\n","Epoch [12/50], Step [256/1135], Loss: 1.4565 R2: 0.7911400107780069\n","Epoch [12/50], Step [320/1135], Loss: 1.7096 R2: 0.8515703642579698\n","Epoch [12/50], Step [384/1135], Loss: 1.5346 R2: 0.8533332430740216\n","Epoch [12/50], Step [448/1135], Loss: 1.9477 R2: 0.820750168239225\n","Epoch [12/50], Step [512/1135], Loss: 1.7230 R2: 0.8497596967020942\n","Epoch [12/50], Step [576/1135], Loss: 2.2036 R2: 0.8356883745465776\n","Epoch [12/50], Step [640/1135], Loss: 2.5747 R2: 0.7900402868736042\n","Epoch [12/50], Step [704/1135], Loss: 2.5067 R2: 0.776071772616708\n","Epoch [12/50], Step [768/1135], Loss: 1.7551 R2: 0.828039889988349\n","Epoch [12/50], Step [832/1135], Loss: 2.3662 R2: 0.7946319346918568\n","Epoch [12/50], Step [896/1135], Loss: 1.7253 R2: 0.8438552863005226\n","Epoch [12/50], Step [960/1135], Loss: 1.4184 R2: 0.8505460621816353\n","Epoch [12/50], Step [1024/1135], Loss: 1.3288 R2: 0.8519642758234252\n","Epoch [12/50], Step [1088/1135], Loss: 1.4216 R2: 0.8686103377418809\n","Epoch: 12/50.. Epoch Loss: 2.38059791328031.. Validation Loss: 9.273809182569972 R2: 0.08846378551642564\n","Epoch [13/50], Step [64/1135], Loss: 1.6066 R2: 0.833912992298462\n","Epoch [13/50], Step [128/1135], Loss: 1.4605 R2: 0.8493265359320401\n","Epoch [13/50], Step [192/1135], Loss: 2.4150 R2: 0.7710814136779608\n","Epoch [13/50], Step [256/1135], Loss: 2.7338 R2: 0.6079745061226561\n","Epoch [13/50], Step [320/1135], Loss: 3.1151 R2: 0.7295487370126892\n","Epoch [13/50], Step [384/1135], Loss: 2.5272 R2: 0.7584665717700532\n","Epoch [13/50], Step [448/1135], Loss: 2.0444 R2: 0.8118517844531886\n","Epoch [13/50], Step [512/1135], Loss: 1.5217 R2: 0.8673131445337168\n","Epoch [13/50], Step [576/1135], Loss: 1.9232 R2: 0.8565985418494105\n","Epoch [13/50], Step [640/1135], Loss: 2.3284 R2: 0.8101281785278921\n","Epoch [13/50], Step [704/1135], Loss: 2.7162 R2: 0.7573564170964097\n","Epoch [13/50], Step [768/1135], Loss: 2.6953 R2: 0.7359180256935953\n","Epoch [13/50], Step [832/1135], Loss: 3.6338 R2: 0.6846113496378619\n","Epoch [13/50], Step [896/1135], Loss: 2.5509 R2: 0.7691381490850556\n","Epoch [13/50], Step [960/1135], Loss: 2.0421 R2: 0.7848221830182419\n","Epoch [13/50], Step [1024/1135], Loss: 1.4831 R2: 0.8347715802214206\n","Epoch [13/50], Step [1088/1135], Loss: 1.7376 R2: 0.8394122986697744\n","Epoch: 13/50.. Epoch Loss: 2.7101248062762573.. Validation Loss: 8.255991997796096 R2: 0.18850652606915697\n","Epoch [14/50], Step [64/1135], Loss: 2.0682 R2: 0.7861913027015478\n","Epoch [14/50], Step [128/1135], Loss: 1.6028 R2: 0.8346453153420254\n","Epoch [14/50], Step [192/1135], Loss: 2.3027 R2: 0.7817264643576728\n","Epoch [14/50], Step [256/1135], Loss: 1.7794 R2: 0.7448412385784235\n","Epoch [14/50], Step [320/1135], Loss: 2.6764 R2: 0.7676371722906369\n","Epoch [14/50], Step [384/1135], Loss: 2.5117 R2: 0.7599504112102\n","Epoch [14/50], Step [448/1135], Loss: 3.2661 R2: 0.6994093645027777\n","Epoch [14/50], Step [512/1135], Loss: 3.0681 R2: 0.7324698339471511\n","Epoch [14/50], Step [576/1135], Loss: 2.6076 R2: 0.8055669851869485\n","Epoch [14/50], Step [640/1135], Loss: 2.5262 R2: 0.7939979255501428\n","Epoch [14/50], Step [704/1135], Loss: 2.2638 R2: 0.7977645627432592\n","Epoch [14/50], Step [768/1135], Loss: 1.9608 R2: 0.8078883725070451\n","Epoch [14/50], Step [832/1135], Loss: 2.6899 R2: 0.7665368592589389\n","Epoch [14/50], Step [896/1135], Loss: 2.9482 R2: 0.7331786315675237\n","Epoch [14/50], Step [960/1135], Loss: 2.9751 R2: 0.6865178714288798\n","Epoch [14/50], Step [1024/1135], Loss: 3.7329 R2: 0.5841377112891226\n","Epoch [14/50], Step [1088/1135], Loss: 3.7273 R2: 0.6555127177296787\n","Epoch: 14/50.. Epoch Loss: 3.3459534778867037.. Validation Loss: 10.883575818859509 R2: -0.06976250883051827\n","Epoch [15/50], Step [64/1135], Loss: 3.3440 R2: 0.6543011140375523\n","Epoch [15/50], Step [128/1135], Loss: 2.4468 R2: 0.7475697044529955\n","Epoch [15/50], Step [192/1135], Loss: 2.4085 R2: 0.7717046271357717\n","Epoch [15/50], Step [256/1135], Loss: 2.1241 R2: 0.6954054107000038\n","Epoch [15/50], Step [320/1135], Loss: 3.6224 R2: 0.685508535117713\n","Epoch [15/50], Step [384/1135], Loss: 3.8512 R2: 0.631927878820127\n","Epoch [15/50], Step [448/1135], Loss: 4.0571 R2: 0.6266156666548905\n","Epoch [15/50], Step [512/1135], Loss: 3.7604 R2: 0.672099762794224\n","Epoch [15/50], Step [576/1135], Loss: 3.9791 R2: 0.7033030995634681\n","Epoch [15/50], Step [640/1135], Loss: 3.8985 R2: 0.6820835873236046\n","Epoch [15/50], Step [704/1135], Loss: 3.5489 R2: 0.6829635672384208\n","Epoch [15/50], Step [768/1135], Loss: 3.0364 R2: 0.7025009068169987\n","Epoch [15/50], Step [832/1135], Loss: 2.4007 R2: 0.791637395932362\n","Epoch [15/50], Step [896/1135], Loss: 1.5079 R2: 0.8635282525909025\n","Epoch [15/50], Step [960/1135], Loss: 1.8319 R2: 0.8069709342700744\n","Epoch [15/50], Step [1024/1135], Loss: 2.2269 R2: 0.751914491219984\n","Epoch [15/50], Step [1088/1135], Loss: 3.5372 R2: 0.6730833861976637\n","Epoch: 15/50.. Epoch Loss: 3.868846272396222.. Validation Loss: 13.760645365292469 R2: -0.35255386430887214\n","Epoch [16/50], Step [64/1135], Loss: 5.0541 R2: 0.477509784174625\n","Epoch [16/50], Step [128/1135], Loss: 5.1370 R2: 0.4700294234159923\n","Epoch [16/50], Step [192/1135], Loss: 3.4314 R2: 0.6747377116147781\n","Epoch [16/50], Step [256/1135], Loss: 2.5173 R2: 0.6390209465173013\n","Epoch [16/50], Step [320/1135], Loss: 2.1114 R2: 0.8166920818571718\n","Epoch [16/50], Step [384/1135], Loss: 2.3712 R2: 0.7733803016901362\n","Epoch [16/50], Step [448/1135], Loss: 2.5970 R2: 0.7609939770327012\n","Epoch [16/50], Step [512/1135], Loss: 2.9231 R2: 0.7451103488358259\n","Epoch [16/50], Step [576/1135], Loss: 4.1983 R2: 0.6869602702639465\n","Epoch [16/50], Step [640/1135], Loss: 4.0928 R2: 0.6662426750238154\n","Epoch [16/50], Step [704/1135], Loss: 4.5132 R2: 0.5968216795463055\n","Epoch [16/50], Step [768/1135], Loss: 4.2466 R2: 0.5839269924081341\n","Epoch [16/50], Step [832/1135], Loss: 4.8485 R2: 0.5791821536658301\n","Epoch [16/50], Step [896/1135], Loss: 4.0145 R2: 0.6366756638501059\n","Epoch [16/50], Step [960/1135], Loss: 2.4436 R2: 0.7425224966610359\n","Epoch [16/50], Step [1024/1135], Loss: 1.8049 R2: 0.7989243299334057\n","Epoch [16/50], Step [1088/1135], Loss: 1.3408 R2: 0.8760851399157512\n","Epoch: 16/50.. Epoch Loss: 3.9921045797851114.. Validation Loss: 10.727624585695667 R2: -0.054433838981986726\n","Epoch [17/50], Step [64/1135], Loss: 1.5683 R2: 0.8378651222699046\n","Epoch [17/50], Step [128/1135], Loss: 2.8393 R2: 0.7070776940343738\n","Epoch [17/50], Step [192/1135], Loss: 3.9556 R2: 0.6250553677532079\n","Epoch [17/50], Step [256/1135], Loss: 4.5691 R2: 0.3448052910592907\n","Epoch [17/50], Step [320/1135], Loss: 4.6889 R2: 0.5929118321583942\n","Epoch [17/50], Step [384/1135], Loss: 4.2114 R2: 0.5975022528803902\n","Epoch [17/50], Step [448/1135], Loss: 3.5082 R2: 0.6771314492893736\n","Epoch [17/50], Step [512/1135], Loss: 1.8101 R2: 0.8421636795102239\n","Epoch [17/50], Step [576/1135], Loss: 2.2013 R2: 0.8358628091001921\n","Epoch [17/50], Step [640/1135], Loss: 2.4177 R2: 0.8028442380134554\n","Epoch [17/50], Step [704/1135], Loss: 2.6798 R2: 0.7606022940362769\n","Epoch [17/50], Step [768/1135], Loss: 2.7542 R2: 0.7301440641527316\n","Epoch [17/50], Step [832/1135], Loss: 3.6805 R2: 0.6805574322569041\n","Epoch [17/50], Step [896/1135], Loss: 3.8557 R2: 0.6510535459921669\n","Epoch [17/50], Step [960/1135], Loss: 2.9004 R2: 0.6943929519214413\n","Epoch [17/50], Step [1024/1135], Loss: 3.0777 R2: 0.6571350030916179\n","Epoch [17/50], Step [1088/1135], Loss: 2.4085 R2: 0.7774054533858797\n","Epoch: 17/50.. Epoch Loss: 3.5547049194465408.. Validation Loss: 7.539405731569163 R2: 0.2589408331741486\n","Validation Loss Decreased(7.889072--->7.539406) \t Saving The Model\n","Epoch [18/50], Step [64/1135], Loss: 2.3491 R2: 0.7571492725516905\n","Epoch [18/50], Step [128/1135], Loss: 1.5869 R2: 0.8362845531458252\n","Epoch [18/50], Step [192/1135], Loss: 1.3653 R2: 0.8705805908681907\n","Epoch [18/50], Step [256/1135], Loss: 1.2001 R2: 0.8279073288529243\n","Epoch [18/50], Step [320/1135], Loss: 2.1620 R2: 0.8122939000858306\n","Epoch [18/50], Step [384/1135], Loss: 2.4689 R2: 0.7640402026260229\n","Epoch [18/50], Step [448/1135], Loss: 4.1187 R2: 0.6209423803859664\n","Epoch [18/50], Step [512/1135], Loss: 3.3959 R2: 0.7038853465725681\n","Epoch [18/50], Step [576/1135], Loss: 3.1144 R2: 0.7677770930310259\n","Epoch [18/50], Step [640/1135], Loss: 2.8297 R2: 0.7692436738874975\n","Epoch [18/50], Step [704/1135], Loss: 2.1474 R2: 0.8081656208936583\n","Epoch [18/50], Step [768/1135], Loss: 1.0631 R2: 0.8958413448241991\n","Epoch [18/50], Step [832/1135], Loss: 1.7925 R2: 0.8444196235899664\n","Epoch [18/50], Step [896/1135], Loss: 2.1871 R2: 0.8020597428076781\n","Epoch [18/50], Step [960/1135], Loss: 1.7865 R2: 0.8117607417172112\n","Epoch [18/50], Step [1024/1135], Loss: 2.2824 R2: 0.7457312646189802\n","Epoch [18/50], Step [1088/1135], Loss: 2.1983 R2: 0.796830961398495\n","Epoch: 18/50.. Epoch Loss: 2.679253108062194.. Validation Loss: 7.777981825529519 R2: 0.23549084148047716\n","Epoch [19/50], Step [64/1135], Loss: 2.8222 R2: 0.7082441447109983\n","Epoch [19/50], Step [128/1135], Loss: 2.0743 R2: 0.786000160815292\n","Epoch [19/50], Step [192/1135], Loss: 2.1951 R2: 0.7919298855620021\n","Epoch [19/50], Step [256/1135], Loss: 1.6951 R2: 0.7569258548399811\n","Epoch [19/50], Step [320/1135], Loss: 2.0090 R2: 0.8255837085139816\n","Epoch [19/50], Step [384/1135], Loss: 1.6636 R2: 0.8410075039518029\n","Epoch [19/50], Step [448/1135], Loss: 2.2114 R2: 0.7964810245114385\n","Epoch [19/50], Step [512/1135], Loss: 1.8505 R2: 0.8386419337626875\n","Epoch [19/50], Step [576/1135], Loss: 2.6309 R2: 0.8038283665850428\n","Epoch [19/50], Step [640/1135], Loss: 3.4862 R2: 0.7157056246165445\n","Epoch [19/50], Step [704/1135], Loss: 2.7676 R2: 0.7527591860412359\n","Epoch [19/50], Step [768/1135], Loss: 2.3555 R2: 0.7692124049705529\n","Epoch [19/50], Step [832/1135], Loss: 2.3070 R2: 0.7997712761743773\n","Epoch [19/50], Step [896/1135], Loss: 1.2563 R2: 0.8863028656119114\n","Epoch [19/50], Step [960/1135], Loss: 0.8677 R2: 0.9085702745414798\n","Epoch [19/50], Step [1024/1135], Loss: 0.8699 R2: 0.9030845773946312\n","Epoch [19/50], Step [1088/1135], Loss: 1.2060 R2: 0.8885357107483094\n","Epoch: 19/50.. Epoch Loss: 2.4983666729357688.. Validation Loss: 8.634798780360795 R2: 0.15127305326451124\n","Epoch [20/50], Step [64/1135], Loss: 1.6733 R2: 0.8270127722790618\n","Epoch [20/50], Step [128/1135], Loss: 1.2392 R2: 0.8721571107123709\n","Epoch [20/50], Step [192/1135], Loss: 1.6003 R2: 0.8483051876875802\n","Epoch [20/50], Step [256/1135], Loss: 1.4498 R2: 0.792101638006431\n","Epoch [20/50], Step [320/1135], Loss: 1.8721 R2: 0.8374675477763269\n","Epoch [20/50], Step [384/1135], Loss: 1.8995 R2: 0.8184608529949365\n","Epoch [20/50], Step [448/1135], Loss: 2.0287 R2: 0.8132979588320605\n","Epoch [20/50], Step [512/1135], Loss: 1.4780 R2: 0.8711248398907177\n","Epoch [20/50], Step [576/1135], Loss: 1.6738 R2: 0.8751979349363679\n","Epoch [20/50], Step [640/1135], Loss: 1.5887 R2: 0.8704453488440372\n","Epoch [20/50], Step [704/1135], Loss: 1.4670 R2: 0.8689471252005219\n","Epoch [20/50], Step [768/1135], Loss: 1.1202 R2: 0.8902461891807988\n","Epoch [20/50], Step [832/1135], Loss: 1.8774 R2: 0.8370540279901952\n","Epoch [20/50], Step [896/1135], Loss: 1.6160 R2: 0.8537479605186278\n","Epoch [20/50], Step [960/1135], Loss: 1.5455 R2: 0.8371493062514958\n","Epoch [20/50], Step [1024/1135], Loss: 1.4778 R2: 0.8353688358090894\n","Epoch [20/50], Step [1088/1135], Loss: 1.3868 R2: 0.8718281858481388\n","Epoch: 20/50.. Epoch Loss: 2.1859475949578084.. Validation Loss: 8.588991622474529 R2: 0.15577551057165429\n","Epoch [21/50], Step [64/1135], Loss: 1.0981 R2: 0.8864749669847978\n","Epoch [21/50], Step [128/1135], Loss: 1.0269 R2: 0.894062150648084\n","Epoch [21/50], Step [192/1135], Loss: 0.9735 R2: 0.9077208972671065\n","Epoch [21/50], Step [256/1135], Loss: 0.8340 R2: 0.8804134611694039\n","Epoch [21/50], Step [320/1135], Loss: 1.3845 R2: 0.8797975834523325\n","Epoch [21/50], Step [384/1135], Loss: 1.5615 R2: 0.8507597928434132\n","Epoch [21/50], Step [448/1135], Loss: 1.8265 R2: 0.8318985802296461\n","Epoch [21/50], Step [512/1135], Loss: 1.6709 R2: 0.8542983446174599\n","Epoch [21/50], Step [576/1135], Loss: 1.9181 R2: 0.8569824810917096\n","Epoch [21/50], Step [640/1135], Loss: 1.7173 R2: 0.8599600015072657\n","Epoch [21/50], Step [704/1135], Loss: 1.5105 R2: 0.8650655396987862\n","Epoch [21/50], Step [768/1135], Loss: 0.8712 R2: 0.9146463390391265\n","Epoch [21/50], Step [832/1135], Loss: 1.3698 R2: 0.8811074567732796\n","Epoch [21/50], Step [896/1135], Loss: 0.8381 R2: 0.924146905438897\n","Epoch [21/50], Step [960/1135], Loss: 0.7318 R2: 0.9228876500837879\n","Epoch [21/50], Step [1024/1135], Loss: 0.8294 R2: 0.9076061594527394\n","Epoch [21/50], Step [1088/1135], Loss: 1.2638 R2: 0.8831953709287366\n","Epoch: 21/50.. Epoch Loss: 1.9814501227314394.. Validation Loss: 10.196949193984082 R2: -0.002272983749963542\n","Epoch [22/50], Step [64/1135], Loss: 1.3270 R2: 0.8628109757110346\n","Epoch [22/50], Step [128/1135], Loss: 1.5292 R2: 0.8422330002111968\n","Epoch [22/50], Step [192/1135], Loss: 1.2014 R2: 0.8861211188691177\n","Epoch [22/50], Step [256/1135], Loss: 1.0155 R2: 0.8543760997839648\n","Epoch [22/50], Step [320/1135], Loss: 1.0719 R2: 0.906942314605822\n","Epoch [22/50], Step [384/1135], Loss: 0.9442 R2: 0.9097562090047576\n","Epoch [22/50], Step [448/1135], Loss: 1.2659 R2: 0.8834951982974001\n","Epoch [22/50], Step [512/1135], Loss: 1.0304 R2: 0.910150656631756\n","Epoch [22/50], Step [576/1135], Loss: 1.5243 R2: 0.8863448287847001\n","Epoch [22/50], Step [640/1135], Loss: 1.4892 R2: 0.878560126400013\n","Epoch [22/50], Step [704/1135], Loss: 1.4658 R2: 0.8690583358444602\n","Epoch [22/50], Step [768/1135], Loss: 1.0169 R2: 0.9003699946813318\n","Epoch [22/50], Step [832/1135], Loss: 1.6000 R2: 0.8611335542565728\n","Epoch [22/50], Step [896/1135], Loss: 1.2618 R2: 0.8858027088837136\n","Epoch [22/50], Step [960/1135], Loss: 0.8318 R2: 0.9123523164544305\n","Epoch [22/50], Step [1024/1135], Loss: 0.7905 R2: 0.9119377324463293\n","Epoch [22/50], Step [1088/1135], Loss: 0.8078 R2: 0.9253439741164948\n","Epoch: 22/50.. Epoch Loss: 1.8219077338587055.. Validation Loss: 8.909154673002417 R2: 0.12430621419538279\n","Epoch [23/50], Step [64/1135], Loss: 0.7712 R2: 0.9202745777857129\n","Epoch [23/50], Step [128/1135], Loss: 0.9819 R2: 0.8986987642356735\n","Epoch [23/50], Step [192/1135], Loss: 0.9601 R2: 0.9089956504690099\n","Epoch [23/50], Step [256/1135], Loss: 1.0131 R2: 0.8547251608631364\n","Epoch [23/50], Step [320/1135], Loss: 1.2331 R2: 0.8929464280085735\n","Epoch [23/50], Step [384/1135], Loss: 1.0760 R2: 0.8971613609146161\n","Epoch [23/50], Step [448/1135], Loss: 1.6297 R2: 0.8500142335227114\n","Epoch [23/50], Step [512/1135], Loss: 1.0721 R2: 0.9065117313410327\n","Epoch [23/50], Step [576/1135], Loss: 1.2591 R2: 0.9061190652450017\n","Epoch [23/50], Step [640/1135], Loss: 1.2272 R2: 0.8999281940587789\n","Epoch [23/50], Step [704/1135], Loss: 1.0821 R2: 0.9033356766589768\n","Epoch [23/50], Step [768/1135], Loss: 0.5867 R2: 0.9425180630423543\n","Epoch [23/50], Step [832/1135], Loss: 1.2488 R2: 0.89161587931668\n","Epoch [23/50], Step [896/1135], Loss: 1.0956 R2: 0.9008427377229793\n","Epoch [23/50], Step [960/1135], Loss: 0.8527 R2: 0.9101509640339948\n","Epoch [23/50], Step [1024/1135], Loss: 1.0486 R2: 0.8831832962222864\n","Epoch [23/50], Step [1088/1135], Loss: 1.0728 R2: 0.9008500305186701\n","Epoch: 23/50.. Epoch Loss: 1.5874972429128211.. Validation Loss: 7.5686057389478725 R2: 0.25607072377505535\n","Epoch [24/50], Step [64/1135], Loss: 1.0594 R2: 0.8904769725447331\n","Epoch [24/50], Step [128/1135], Loss: 0.8547 R2: 0.911825528331503\n","Epoch [24/50], Step [192/1135], Loss: 0.8061 R2: 0.9235943166372569\n","Epoch [24/50], Step [256/1135], Loss: 0.5776 R2: 0.9171744295460793\n","Epoch [24/50], Step [320/1135], Loss: 0.7655 R2: 0.9335364992338523\n","Epoch [24/50], Step [384/1135], Loss: 0.7222 R2: 0.9309771859767003\n","Epoch [24/50], Step [448/1135], Loss: 1.4330 R2: 0.8681147779594178\n","Epoch [24/50], Step [512/1135], Loss: 1.1398 R2: 0.9006122665108341\n","Epoch [24/50], Step [576/1135], Loss: 1.5265 R2: 0.8861802592804858\n","Epoch [24/50], Step [640/1135], Loss: 1.6814 R2: 0.8628825798052526\n","Epoch [24/50], Step [704/1135], Loss: 1.4004 R2: 0.874900619942194\n","Epoch [24/50], Step [768/1135], Loss: 0.8095 R2: 0.9206913326342934\n","Epoch [24/50], Step [832/1135], Loss: 1.1524 R2: 0.8999804933894542\n","Epoch [24/50], Step [896/1135], Loss: 0.7025 R2: 0.9364183137995009\n","Epoch [24/50], Step [960/1135], Loss: 0.4461 R2: 0.9529912544291987\n","Epoch [24/50], Step [1024/1135], Loss: 0.6925 R2: 0.9228558621022969\n","Epoch [24/50], Step [1088/1135], Loss: 0.9047 R2: 0.9163888249440562\n","Epoch: 24/50.. Epoch Loss: 1.467325624052392.. Validation Loss: 7.347567106755709 R2: 0.27779693247441317\n","Validation Loss Decreased(7.539406--->7.347567) \t Saving The Model\n","Epoch [25/50], Step [64/1135], Loss: 1.0086 R2: 0.8957323474313571\n","Epoch [25/50], Step [128/1135], Loss: 0.8458 R2: 0.9127397241158216\n","Epoch [25/50], Step [192/1135], Loss: 1.0481 R2: 0.9006489020418382\n","Epoch [25/50], Step [256/1135], Loss: 0.9150 R2: 0.8687959115441861\n","Epoch [25/50], Step [320/1135], Loss: 1.0565 R2: 0.9082779288999927\n","Epoch [25/50], Step [384/1135], Loss: 0.9288 R2: 0.9112312976704965\n","Epoch [25/50], Step [448/1135], Loss: 1.1579 R2: 0.8934331577809348\n","Epoch [25/50], Step [512/1135], Loss: 0.8067 R2: 0.9296528618805838\n","Epoch [25/50], Step [576/1135], Loss: 1.1168 R2: 0.916730454872313\n","Epoch [25/50], Step [640/1135], Loss: 1.3160 R2: 0.8926816809944379\n","Epoch [25/50], Step [704/1135], Loss: 1.2759 R2: 0.8860171126122856\n","Epoch [25/50], Step [768/1135], Loss: 0.9692 R2: 0.9050401531535646\n","Epoch [25/50], Step [832/1135], Loss: 1.4591 R2: 0.8733608230067641\n","Epoch [25/50], Step [896/1135], Loss: 0.9495 R2: 0.914071256259411\n","Epoch [25/50], Step [960/1135], Loss: 0.6442 R2: 0.9321243498854959\n","Epoch [25/50], Step [1024/1135], Loss: 0.5778 R2: 0.9356356918702028\n","Epoch [25/50], Step [1088/1135], Loss: 0.7054 R2: 0.934805882816345\n","Epoch: 25/50.. Epoch Loss: 1.5003907883240015.. Validation Loss: 7.441150488410529 R2: 0.26859848500150507\n","Epoch [26/50], Step [64/1135], Loss: 0.6760 R2: 0.9301192147429903\n","Epoch [26/50], Step [128/1135], Loss: 0.5938 R2: 0.9387380222742262\n","Epoch [26/50], Step [192/1135], Loss: 0.7740 R2: 0.9266328248850212\n","Epoch [26/50], Step [256/1135], Loss: 0.7065 R2: 0.898686611816074\n","Epoch [26/50], Step [320/1135], Loss: 1.1097 R2: 0.9036576717988258\n","Epoch [26/50], Step [384/1135], Loss: 1.1588 R2: 0.8892548866194729\n","Epoch [26/50], Step [448/1135], Loss: 1.3669 R2: 0.8741988377322745\n","Epoch [26/50], Step [512/1135], Loss: 1.1048 R2: 0.9036627083729981\n","Epoch [26/50], Step [576/1135], Loss: 1.2825 R2: 0.9043726707489743\n","Epoch [26/50], Step [640/1135], Loss: 1.1772 R2: 0.9040044406480997\n","Epoch [26/50], Step [704/1135], Loss: 1.0458 R2: 0.9065769219027058\n","Epoch [26/50], Step [768/1135], Loss: 0.5123 R2: 0.9498105211801202\n","Epoch [26/50], Step [832/1135], Loss: 1.0647 R2: 0.9075911234928219\n","Epoch [26/50], Step [896/1135], Loss: 0.8418 R2: 0.9238126856564519\n","Epoch [26/50], Step [960/1135], Loss: 0.8272 R2: 0.9128431670099939\n","Epoch [26/50], Step [1024/1135], Loss: 0.9262 R2: 0.896817218497558\n","Epoch [26/50], Step [1088/1135], Loss: 1.0848 R2: 0.8997371035399404\n","Epoch: 26/50.. Epoch Loss: 1.6455882699730184.. Validation Loss: 9.56106142524842 R2: 0.060229338902543894\n","Epoch [27/50], Step [64/1135], Loss: 0.9043 R2: 0.9065185526159084\n","Epoch [27/50], Step [128/1135], Loss: 0.8566 R2: 0.9116314289527716\n","Epoch [27/50], Step [192/1135], Loss: 0.7482 R2: 0.9290807703321436\n","Epoch [27/50], Step [256/1135], Loss: 0.5508 R2: 0.9210226143426371\n","Epoch [27/50], Step [320/1135], Loss: 0.8027 R2: 0.9303099208953584\n","Epoch [27/50], Step [384/1135], Loss: 0.9105 R2: 0.9129773086046084\n","Epoch [27/50], Step [448/1135], Loss: 1.2184 R2: 0.8878672002673759\n","Epoch [27/50], Step [512/1135], Loss: 1.0209 R2: 0.9109763858107639\n","Epoch [27/50], Step [576/1135], Loss: 1.3983 R2: 0.8957370420083939\n","Epoch [27/50], Step [640/1135], Loss: 1.3226 R2: 0.8921440442365625\n","Epoch [27/50], Step [704/1135], Loss: 1.2419 R2: 0.8890588362292194\n","Epoch [27/50], Step [768/1135], Loss: 0.7665 R2: 0.9248959929708501\n","Epoch [27/50], Step [832/1135], Loss: 1.1209 R2: 0.9027111467439566\n","Epoch [27/50], Step [896/1135], Loss: 0.6884 R2: 0.9376942942345987\n","Epoch [27/50], Step [960/1135], Loss: 0.3863 R2: 0.9592911482946493\n","Epoch [27/50], Step [1024/1135], Loss: 0.4479 R2: 0.9500971512956009\n","Epoch [27/50], Step [1088/1135], Loss: 0.7480 R2: 0.930865401251086\n","Epoch: 27/50.. Epoch Loss: 1.6037846715744284.. Validation Loss: 10.111020649050266 R2: 0.006173055280477735\n","Epoch [28/50], Step [64/1135], Loss: 0.8472 R2: 0.9124166561038967\n","Epoch [28/50], Step [128/1135], Loss: 1.1485 R2: 0.8815112445605826\n","Epoch [28/50], Step [192/1135], Loss: 1.0117 R2: 0.9041034168909305\n","Epoch [28/50], Step [256/1135], Loss: 1.0053 R2: 0.8558483498585434\n","Epoch [28/50], Step [320/1135], Loss: 0.9585 R2: 0.916780782585391\n","Epoch [28/50], Step [384/1135], Loss: 0.7598 R2: 0.9273828123288577\n","Epoch [28/50], Step [448/1135], Loss: 1.0475 R2: 0.9035921300818033\n","Epoch [28/50], Step [512/1135], Loss: 0.7437 R2: 0.9351503335898079\n","Epoch [28/50], Step [576/1135], Loss: 1.0313 R2: 0.9231012918502104\n","Epoch [28/50], Step [640/1135], Loss: 1.0471 R2: 0.9146112927674984\n","Epoch [28/50], Step [704/1135], Loss: 1.0554 R2: 0.9057197685478648\n","Epoch [28/50], Step [768/1135], Loss: 0.7822 R2: 0.923365832810427\n","Epoch [28/50], Step [832/1135], Loss: 1.4179 R2: 0.8769342738766307\n","Epoch [28/50], Step [896/1135], Loss: 1.1227 R2: 0.898396912186784\n","Epoch [28/50], Step [960/1135], Loss: 0.8675 R2: 0.9085952197988\n","Epoch [28/50], Step [1024/1135], Loss: 0.8781 R2: 0.902178892268218\n","Epoch [28/50], Step [1088/1135], Loss: 0.8005 R2: 0.9260152548873746\n","Epoch: 28/50.. Epoch Loss: 1.5762229547539992.. Validation Loss: 8.545206401690429 R2: 0.1600792270967335\n","Epoch [29/50], Step [64/1135], Loss: 0.6570 R2: 0.9320774054812869\n","Epoch [29/50], Step [128/1135], Loss: 0.6398 R2: 0.9339927304755326\n","Epoch [29/50], Step [192/1135], Loss: 0.6347 R2: 0.9398355373872793\n","Epoch [29/50], Step [256/1135], Loss: 0.7692 R2: 0.8897015010205737\n","Epoch [29/50], Step [320/1135], Loss: 1.0500 R2: 0.908838615015156\n","Epoch [29/50], Step [384/1135], Loss: 1.0447 R2: 0.9001525800186443\n","Epoch [29/50], Step [448/1135], Loss: 1.6552 R2: 0.8476655643325655\n","Epoch [29/50], Step [512/1135], Loss: 1.0851 R2: 0.9053829265134297\n","Epoch [29/50], Step [576/1135], Loss: 1.1842 R2: 0.9116983102509235\n","Epoch [29/50], Step [640/1135], Loss: 1.0439 R2: 0.9148741694794356\n","Epoch [29/50], Step [704/1135], Loss: 0.8844 R2: 0.9209908283643028\n","Epoch [29/50], Step [768/1135], Loss: 0.4318 R2: 0.9576907345176336\n","Epoch [29/50], Step [832/1135], Loss: 1.0296 R2: 0.9106396360861486\n","Epoch [29/50], Step [896/1135], Loss: 0.8778 R2: 0.9205584530568984\n","Epoch [29/50], Step [960/1135], Loss: 0.7368 R2: 0.9223669236892635\n","Epoch [29/50], Step [1024/1135], Loss: 1.0239 R2: 0.8859384743654519\n","Epoch [29/50], Step [1088/1135], Loss: 1.0865 R2: 0.8995874378797413\n","Epoch: 29/50.. Epoch Loss: 1.4218856706263954.. Validation Loss: 7.528391142019149 R2: 0.2600234739805388\n","Epoch [30/50], Step [64/1135], Loss: 1.0294 R2: 0.8935834445388773\n","Epoch [30/50], Step [128/1135], Loss: 0.8115 R2: 0.9162813603200919\n","Epoch [30/50], Step [192/1135], Loss: 0.7940 R2: 0.9247386260418389\n","Epoch [30/50], Step [256/1135], Loss: 0.6242 R2: 0.9104872944862296\n","Epoch [30/50], Step [320/1135], Loss: 0.6161 R2: 0.9465096731499386\n","Epoch [30/50], Step [384/1135], Loss: 0.5587 R2: 0.9465992777368654\n","Epoch [30/50], Step [448/1135], Loss: 1.1531 R2: 0.8938771508263123\n","Epoch [30/50], Step [512/1135], Loss: 0.9648 R2: 0.9158698956458007\n","Epoch [30/50], Step [576/1135], Loss: 1.4453 R2: 0.8922358934994381\n","Epoch [30/50], Step [640/1135], Loss: 1.7308 R2: 0.8588573131686974\n","Epoch [30/50], Step [704/1135], Loss: 1.5469 R2: 0.8618126609858936\n","Epoch [30/50], Step [768/1135], Loss: 0.9683 R2: 0.9051231180035142\n","Epoch [30/50], Step [832/1135], Loss: 1.1150 R2: 0.9032231587620989\n","Epoch [30/50], Step [896/1135], Loss: 0.5756 R2: 0.9479026617869671\n","Epoch [30/50], Step [960/1135], Loss: 0.3311 R2: 0.9651117083364817\n","Epoch [30/50], Step [1024/1135], Loss: 0.5906 R2: 0.9342067521345361\n","Epoch [30/50], Step [1088/1135], Loss: 0.8619 R2: 0.9203399922217783\n","Epoch: 30/50.. Epoch Loss: 1.3992332477688971.. Validation Loss: 7.522835059263854 R2: 0.26056959284782377\n","Epoch [31/50], Step [64/1135], Loss: 0.9920 R2: 0.8974467312802104\n","Epoch [31/50], Step [128/1135], Loss: 0.8209 R2: 0.9153067206168194\n","Epoch [31/50], Step [192/1135], Loss: 1.1142 R2: 0.8943853464996021\n","Epoch [31/50], Step [256/1135], Loss: 1.1303 R2: 0.837922741605718\n","Epoch [31/50], Step [320/1135], Loss: 1.2728 R2: 0.8894930071097205\n","Epoch [31/50], Step [384/1135], Loss: 1.1112 R2: 0.893804520181418\n","Epoch [31/50], Step [448/1135], Loss: 1.2077 R2: 0.8888542022521879\n","Epoch [31/50], Step [512/1135], Loss: 0.7264 R2: 0.9366612514652883\n","Epoch [31/50], Step [576/1135], Loss: 0.9445 R2: 0.9295780297099157\n","Epoch [31/50], Step [640/1135], Loss: 1.1179 R2: 0.9088373421241019\n","Epoch [31/50], Step [704/1135], Loss: 1.2545 R2: 0.8879304178455453\n","Epoch [31/50], Step [768/1135], Loss: 1.1180 R2: 0.8904580841936366\n","Epoch [31/50], Step [832/1135], Loss: 1.7072 R2: 0.851830486308129\n","Epoch [31/50], Step [896/1135], Loss: 1.2677 R2: 0.8852665830684654\n","Epoch [31/50], Step [960/1135], Loss: 0.9050 R2: 0.9046462370228598\n","Epoch [31/50], Step [1024/1135], Loss: 0.6439 R2: 0.9282719172969768\n","Epoch [31/50], Step [1088/1135], Loss: 0.6882 R2: 0.936395718920764\n","Epoch: 31/50.. Epoch Loss: 1.559235337656851.. Validation Loss: 7.741244915345881 R2: 0.23910176767511238\n","Epoch [32/50], Step [64/1135], Loss: 0.6075 R2: 0.9371953063824283\n","Epoch [32/50], Step [128/1135], Loss: 0.5394 R2: 0.9443496650145814\n","Epoch [32/50], Step [192/1135], Loss: 0.8175 R2: 0.92251078995537\n","Epoch [32/50], Step [256/1135], Loss: 0.8338 R2: 0.8804413317674881\n","Epoch [32/50], Step [320/1135], Loss: 1.3877 R2: 0.8795211536055507\n","Epoch [32/50], Step [384/1135], Loss: 1.5569 R2: 0.8512006029637852\n","Epoch [32/50], Step [448/1135], Loss: 1.5322 R2: 0.8589836849014993\n","Epoch [32/50], Step [512/1135], Loss: 1.2325 R2: 0.8925321232194267\n","Epoch [32/50], Step [576/1135], Loss: 1.3059 R2: 0.9026268918788023\n","Epoch [32/50], Step [640/1135], Loss: 1.1051 R2: 0.9098824274596032\n","Epoch [32/50], Step [704/1135], Loss: 0.9482 R2: 0.915295724978195\n","Epoch [32/50], Step [768/1135], Loss: 0.4658 R2: 0.9543618721160901\n","Epoch [32/50], Step [832/1135], Loss: 0.9709 R2: 0.9157285718245438\n","Epoch [32/50], Step [896/1135], Loss: 1.0649 R2: 0.9036204714135112\n","Epoch [32/50], Step [960/1135], Loss: 1.1086 R2: 0.8831863524156416\n","Epoch [32/50], Step [1024/1135], Loss: 1.4672 R2: 0.8365489637473824\n","Epoch [32/50], Step [1088/1135], Loss: 1.4709 R2: 0.8640590711760705\n","Epoch: 32/50.. Epoch Loss: 1.8802712120226708.. Validation Loss: 11.333278106698474 R2: -0.11396440684455311\n","Epoch [33/50], Step [64/1135], Loss: 1.0355 R2: 0.8929533104351793\n","Epoch [33/50], Step [128/1135], Loss: 0.9143 R2: 0.9056756688561434\n","Epoch [33/50], Step [192/1135], Loss: 0.7933 R2: 0.9248056401984179\n","Epoch [33/50], Step [256/1135], Loss: 0.5496 R2: 0.921190812653697\n","Epoch [33/50], Step [320/1135], Loss: 0.8370 R2: 0.927329136198322\n","Epoch [33/50], Step [384/1135], Loss: 1.0481 R2: 0.8998272653763945\n","Epoch [33/50], Step [448/1135], Loss: 1.3164 R2: 0.8788478264267476\n","Epoch [33/50], Step [512/1135], Loss: 1.1438 R2: 0.9002599208155622\n","Epoch [33/50], Step [576/1135], Loss: 1.5329 R2: 0.8857030571592922\n","Epoch [33/50], Step [640/1135], Loss: 1.4000 R2: 0.8858365288970369\n","Epoch [33/50], Step [704/1135], Loss: 1.4328 R2: 0.872003033100915\n","Epoch [33/50], Step [768/1135], Loss: 1.0252 R2: 0.8995561173141243\n","Epoch [33/50], Step [832/1135], Loss: 1.3039 R2: 0.8868284891850371\n","Epoch [33/50], Step [896/1135], Loss: 0.7159 R2: 0.9352054960822008\n","Epoch [33/50], Step [960/1135], Loss: 0.4297 R2: 0.9547204601242276\n","Epoch [33/50], Step [1024/1135], Loss: 0.4466 R2: 0.9502521583515009\n","Epoch [33/50], Step [1088/1135], Loss: 0.8807 R2: 0.9186058237045408\n","Epoch: 33/50.. Epoch Loss: 1.8300971671133806.. Validation Loss: 12.234070135959083 R2: -0.20250456892297852\n","Epoch [34/50], Step [64/1135], Loss: 1.1496 R2: 0.8811535635959938\n","Epoch [34/50], Step [128/1135], Loss: 1.5314 R2: 0.8420047619098432\n","Epoch [34/50], Step [192/1135], Loss: 1.4176 R2: 0.8656310130643653\n","Epoch [34/50], Step [256/1135], Loss: 1.3981 R2: 0.7995207103871905\n","Epoch [34/50], Step [320/1135], Loss: 1.2010 R2: 0.895731104896918\n","Epoch [34/50], Step [384/1135], Loss: 0.8590 R2: 0.9179002272091019\n","Epoch [34/50], Step [448/1135], Loss: 0.9854 R2: 0.9093152645457127\n","Epoch [34/50], Step [512/1135], Loss: 0.7260 R2: 0.9366980199733428\n","Epoch [34/50], Step [576/1135], Loss: 1.1693 R2: 0.9128138372737169\n","Epoch [34/50], Step [640/1135], Loss: 1.1875 R2: 0.9031591344289917\n","Epoch [34/50], Step [704/1135], Loss: 1.3259 R2: 0.8815506860719877\n","Epoch [34/50], Step [768/1135], Loss: 1.1671 R2: 0.8856452950645675\n","Epoch [34/50], Step [832/1135], Loss: 1.8598 R2: 0.8385803690018242\n","Epoch [34/50], Step [896/1135], Loss: 1.5584 R2: 0.8589599450027431\n","Epoch [34/50], Step [960/1135], Loss: 1.3151 R2: 0.8614294830713863\n","Epoch [34/50], Step [1024/1135], Loss: 1.3234 R2: 0.8525635150951281\n","Epoch [34/50], Step [1088/1135], Loss: 0.9702 R2: 0.9103368538948967\n","Epoch: 34/50.. Epoch Loss: 1.888887969176231.. Validation Loss: 9.077135158826048 R2: 0.10779517644109182\n","Epoch [35/50], Step [64/1135], Loss: 0.7495 R2: 0.9225158543868128\n","Epoch [35/50], Step [128/1135], Loss: 0.6912 R2: 0.9286933399610415\n","Epoch [35/50], Step [192/1135], Loss: 0.6900 R2: 0.9345962860992382\n","Epoch [35/50], Step [256/1135], Loss: 1.3007 R2: 0.8134819386331715\n","Epoch [35/50], Step [320/1135], Loss: 1.8990 R2: 0.8351258881465319\n","Epoch [35/50], Step [384/1135], Loss: 1.8244 R2: 0.8256393044307317\n","Epoch [35/50], Step [448/1135], Loss: 2.1856 R2: 0.7988576742265783\n","Epoch [35/50], Step [512/1135], Loss: 1.2291 R2: 0.8928281156597383\n","Epoch [35/50], Step [576/1135], Loss: 1.1304 R2: 0.9157144420945018\n","Epoch [35/50], Step [640/1135], Loss: 0.9305 R2: 0.9241226633688734\n","Epoch [35/50], Step [704/1135], Loss: 0.8690 R2: 0.9223705194859231\n","Epoch [35/50], Step [768/1135], Loss: 0.5534 R2: 0.9457778175343187\n","Epoch [35/50], Step [832/1135], Loss: 1.2614 R2: 0.8905167244456831\n","Epoch [35/50], Step [896/1135], Loss: 1.1885 R2: 0.8924368367493032\n","Epoch [35/50], Step [960/1135], Loss: 0.9949 R2: 0.895164088877477\n","Epoch [35/50], Step [1024/1135], Loss: 1.2653 R2: 0.8590447806589918\n","Epoch [35/50], Step [1088/1135], Loss: 1.1661 R2: 0.8922286642965368\n","Epoch: 35/50.. Epoch Loss: 1.657383644426469.. Validation Loss: 7.758437851478942 R2: 0.2374118459576603\n","Epoch [36/50], Step [64/1135], Loss: 1.0649 R2: 0.8899142865732188\n","Epoch [36/50], Step [128/1135], Loss: 0.8345 R2: 0.9139021365296565\n","Epoch [36/50], Step [192/1135], Loss: 0.9324 R2: 0.91161691700311\n","Epoch [36/50], Step [256/1135], Loss: 0.7006 R2: 0.8995309238005\n","Epoch [36/50], Step [320/1135], Loss: 0.5623 R2: 0.9511822837562701\n","Epoch [36/50], Step [384/1135], Loss: 0.5295 R2: 0.9493900813182207\n","Epoch [36/50], Step [448/1135], Loss: 1.2448 R2: 0.8854398821845167\n","Epoch [36/50], Step [512/1135], Loss: 1.1696 R2: 0.8980105075537598\n","Epoch [36/50], Step [576/1135], Loss: 1.6705 R2: 0.8754446986385919\n","Epoch [36/50], Step [640/1135], Loss: 2.0541 R2: 0.8324960303218891\n","Epoch [36/50], Step [704/1135], Loss: 1.8294 R2: 0.8365747110419061\n","Epoch [36/50], Step [768/1135], Loss: 1.1546 R2: 0.8868771125534289\n","Epoch [36/50], Step [832/1135], Loss: 1.0992 R2: 0.904596575420021\n","Epoch [36/50], Step [896/1135], Loss: 0.5442 R2: 0.9507445536044732\n","Epoch [36/50], Step [960/1135], Loss: 0.3581 R2: 0.9622706948334768\n","Epoch [36/50], Step [1024/1135], Loss: 0.7918 R2: 0.9117954398332514\n","Epoch [36/50], Step [1088/1135], Loss: 1.0572 R2: 0.9022952839335346\n","Epoch: 36/50.. Epoch Loss: 1.4878755700925828.. Validation Loss: 7.710109330604302 R2: 0.24216213182129487\n","Epoch [37/50], Step [64/1135], Loss: 1.1764 R2: 0.8783807739635778\n","Epoch [37/50], Step [128/1135], Loss: 0.9559 R2: 0.9013802320700826\n","Epoch [37/50], Step [192/1135], Loss: 1.2461 R2: 0.8818866731404985\n","Epoch [37/50], Step [256/1135], Loss: 1.3827 R2: 0.8017249792553139\n","Epoch [37/50], Step [320/1135], Loss: 1.5603 R2: 0.8645343980103584\n","Epoch [37/50], Step [384/1135], Loss: 1.4469 R2: 0.861714736794756\n","Epoch [37/50], Step [448/1135], Loss: 1.5198 R2: 0.8601334622909173\n","Epoch [37/50], Step [512/1135], Loss: 0.7685 R2: 0.9329849940029586\n","Epoch [37/50], Step [576/1135], Loss: 0.9340 R2: 0.9303562411115244\n","Epoch [37/50], Step [640/1135], Loss: 1.1924 R2: 0.9027617999927825\n","Epoch [37/50], Step [704/1135], Loss: 1.4535 R2: 0.8701531290572894\n","Epoch [37/50], Step [768/1135], Loss: 1.6088 R2: 0.8423719884090463\n","Epoch [37/50], Step [832/1135], Loss: 2.2159 R2: 0.807673835478796\n","Epoch [37/50], Step [896/1135], Loss: 1.5458 R2: 0.8601065777260243\n","Epoch [37/50], Step [960/1135], Loss: 1.1762 R2: 0.8760694992326037\n","Epoch [37/50], Step [1024/1135], Loss: 0.6642 R2: 0.9260088797238953\n","Epoch [37/50], Step [1088/1135], Loss: 0.7429 R2: 0.9313354637895641\n","Epoch: 37/50.. Epoch Loss: 1.755363850642969.. Validation Loss: 8.292290501068225 R2: 0.1849386896943882\n","Epoch [38/50], Step [64/1135], Loss: 0.6172 R2: 0.9361980219696456\n","Epoch [38/50], Step [128/1135], Loss: 0.6288 R2: 0.9351304484015286\n","Epoch [38/50], Step [192/1135], Loss: 0.9760 R2: 0.907490664679594\n","Epoch [38/50], Step [256/1135], Loss: 1.0191 R2: 0.8538632785384179\n","Epoch [38/50], Step [320/1135], Loss: 1.5437 R2: 0.8659798397440612\n","Epoch [38/50], Step [384/1135], Loss: 1.7037 R2: 0.837169410143083\n","Epoch [38/50], Step [448/1135], Loss: 1.5928 R2: 0.8534078139583379\n","Epoch [38/50], Step [512/1135], Loss: 1.2569 R2: 0.8903976783796201\n","Epoch [38/50], Step [576/1135], Loss: 1.4051 R2: 0.895231719135789\n","Epoch [38/50], Step [640/1135], Loss: 1.1794 R2: 0.9038254089298212\n","Epoch [38/50], Step [704/1135], Loss: 0.9774 R2: 0.91268239746886\n","Epoch [38/50], Step [768/1135], Loss: 0.5322 R2: 0.9478535673743036\n","Epoch [38/50], Step [832/1135], Loss: 0.9133 R2: 0.920730858039544\n","Epoch [38/50], Step [896/1135], Loss: 0.9766 R2: 0.911613529656307\n","Epoch [38/50], Step [960/1135], Loss: 1.1729 R2: 0.876418416040435\n","Epoch [38/50], Step [1024/1135], Loss: 1.7973 R2: 0.7997732578213628\n","Epoch [38/50], Step [1088/1135], Loss: 1.7409 R2: 0.8390990797155969\n","Epoch: 38/50.. Epoch Loss: 2.098741565149749.. Validation Loss: 13.0184530297686 R2: -0.27960270169980306\n","Epoch [39/50], Step [64/1135], Loss: 1.4146 R2: 0.8537617377350348\n","Epoch [39/50], Step [128/1135], Loss: 1.0079 R2: 0.8960162572630143\n","Epoch [39/50], Step [192/1135], Loss: 0.7462 R2: 0.9292662208249849\n","Epoch [39/50], Step [256/1135], Loss: 0.4688 R2: 0.932776761031146\n","Epoch [39/50], Step [320/1135], Loss: 0.7254 R2: 0.937020656548033\n","Epoch [39/50], Step [384/1135], Loss: 0.9873 R2: 0.9056403625726145\n","Epoch [39/50], Step [448/1135], Loss: 1.2773 R2: 0.882445834323729\n","Epoch [39/50], Step [512/1135], Loss: 1.0492 R2: 0.9085109131929403\n","Epoch [39/50], Step [576/1135], Loss: 1.4371 R2: 0.8928419060069617\n","Epoch [39/50], Step [640/1135], Loss: 1.3221 R2: 0.8921835368735316\n","Epoch [39/50], Step [704/1135], Loss: 1.3665 R2: 0.877922993422716\n","Epoch [39/50], Step [768/1135], Loss: 0.9296 R2: 0.9089179468892524\n","Epoch [39/50], Step [832/1135], Loss: 1.4710 R2: 0.872326553911306\n","Epoch [39/50], Step [896/1135], Loss: 1.0809 R2: 0.9021782832210309\n","Epoch [39/50], Step [960/1135], Loss: 0.6423 R2: 0.9323184927837941\n","Epoch [39/50], Step [1024/1135], Loss: 0.4991 R2: 0.9443941536890789\n","Epoch [39/50], Step [1088/1135], Loss: 0.6859 R2: 0.9366100581423812\n","Epoch: 39/50.. Epoch Loss: 1.9663020931894057.. Validation Loss: 13.725620738908788 R2: -0.34911124289410234\n","Epoch [40/50], Step [64/1135], Loss: 1.0109 R2: 0.8954905361118475\n","Epoch [40/50], Step [128/1135], Loss: 1.4380 R2: 0.8516451465405989\n","Epoch [40/50], Step [192/1135], Loss: 1.6696 R2: 0.8417441833485655\n","Epoch [40/50], Step [256/1135], Loss: 1.9755 R2: 0.7167112740680353\n","Epoch [40/50], Step [320/1135], Loss: 1.7630 R2: 0.846939503118467\n","Epoch [40/50], Step [384/1135], Loss: 1.1809 R2: 0.8871419687179114\n","Epoch [40/50], Step [448/1135], Loss: 1.2304 R2: 0.8867598688940125\n","Epoch [40/50], Step [512/1135], Loss: 0.7215 R2: 0.9370831238037375\n","Epoch [40/50], Step [576/1135], Loss: 0.8755 R2: 0.9347169499266469\n","Epoch [40/50], Step [640/1135], Loss: 0.9289 R2: 0.9242528342223966\n","Epoch [40/50], Step [704/1135], Loss: 1.1836 R2: 0.8942620697734237\n","Epoch [40/50], Step [768/1135], Loss: 1.0516 R2: 0.8969709054946893\n","Epoch [40/50], Step [832/1135], Loss: 1.8978 R2: 0.8352870608077589\n","Epoch [40/50], Step [896/1135], Loss: 1.6571 R2: 0.850032679037185\n","Epoch [40/50], Step [960/1135], Loss: 1.5466 R2: 0.8370323845345348\n","Epoch [40/50], Step [1024/1135], Loss: 1.6929 R2: 0.8114002394808677\n","Epoch [40/50], Step [1088/1135], Loss: 1.4033 R2: 0.8703064316511657\n","Epoch: 40/50.. Epoch Loss: 1.9656647122367001.. Validation Loss: 8.34231219926076 R2: 0.18002198924108426\n","Epoch [41/50], Step [64/1135], Loss: 1.3026 R2: 0.8653383001886765\n","Epoch [41/50], Step [128/1135], Loss: 0.9619 R2: 0.9007615345424773\n","Epoch [41/50], Step [192/1135], Loss: 0.6673 R2: 0.9367482602332191\n","Epoch [41/50], Step [256/1135], Loss: 0.6909 R2: 0.9009322161360568\n","Epoch [41/50], Step [320/1135], Loss: 1.3870 R2: 0.8795848368619924\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-192-1e1481faf5f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-169-46219ff92234>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, batch_size, verbose)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m               \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m               \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, device, BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"541EV6EvBTvU"},"source":["# Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzmAIQYkBSqO"},"outputs":[],"source":["def evaluate(model, device, test_loader, criterion):\n","  model.eval()\n","  test_loss = 0\n","  total = 0\n","  y_true = np.array([])\n","  y_pred = np.array([])\n","\n","  with torch.no_grad():\n","    for sat_inputs, street_inputs, targets in test_loader:\n","          sat_inputs = sat_inputs.to(device)\n","          targets = targets.to(device)\n","\n","          street_inputs = torch.stack(street_inputs, dim=1).squeeze(0)\n","          street_inputs = street_inputs.to(device)\n","\n","          output = model(sat_inputs, street_inputs)\n","          targets = targets.cpu().detach().numpy().squeeze()\n","          pred = output.cpu().detach().numpy().squeeze()\n","\n","          y_true = np.append(y_true, targets)\n","          y_pred = np.append(y_pred, pred)\n","\n","  # with torch.no_grad():\n","  #     for data, target in test_loader:\n","  #         data, target = data.to(device), target.to(device)\n","  #         total += target.size(0)\n","  #         output = model(data)\n","  #         target = target.cpu().detach().numpy().squeeze()\n","  #         pred = output.cpu().detach().numpy().squeeze()\n","  #         y_true = np.append(y_true, target)\n","  #         y_pred = np.append(y_pred, pred)\n","\n","  return y_true, y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":577,"status":"ok","timestamp":1670551439201,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"gJRh29uLE7d0","outputId":"0a1fbdea-e0f2-46b5-e9fc-8d9f77c01deb"},"outputs":[{"data":{"text/plain":["SatelliteStreetModel(\n","  (satellite_model): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (5): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (6): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (7): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n","  )\n","  (street_model): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (5): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (6): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (7): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n","  )\n","  (activation2): LeakyReLU(negative_slope=0.01)\n","  (fc1): Linear(in_features=1024, out_features=1000, bias=True)\n","  (fc2): Linear(in_features=1000, out_features=750, bias=True)\n","  (fc3): Linear(in_features=750, out_features=512, bias=True)\n","  (fc4): Linear(in_features=512, out_features=256, bias=True)\n","  (fc5): Linear(in_features=256, out_features=128, bias=True)\n","  (fc6): Linear(in_features=128, out_features=64, bias=True)\n","  (fclast): Linear(in_features=64, out_features=1, bias=True)\n",")"]},"execution_count":181,"metadata":{},"output_type":"execute_result"}],"source":["# load best model\n","# model = SatelliteStreetModel('resnet18', 'resnet50')\n","model.load_state_dict(torch.load('best_model.pth'))\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2KWjDsSmBfBg"},"outputs":[],"source":["# use best model test\n","y_test_preds, y_test = evaluate(model, device, test_loader, criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1670551421844,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"5hItESg66lCw","outputId":"d78912d7-dbed-44d5-dda7-2864483ba437"},"outputs":[{"data":{"text/plain":["379"]},"execution_count":179,"metadata":{},"output_type":"execute_result"}],"source":["len(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vR7t-FcVfzFv"},"outputs":[],"source":["def calculate_metrics(pred, actual, verbose=True):\n","    result_metrics = {'mae' : mean_absolute_error(pred, actual),\n","                      'mape' : mean_absolute_percentage_error(pred, actual),\n","                      'mse' : mean_squared_error(pred, actual), \n","                      'rmse' : mean_squared_error(pred, actual) ** 0.5,\n","                      'r2': r2_score(pred, actual)\n","                      }\n","    \n","    if verbose:\n","      print(\"Mean Absolute Error:       \", result_metrics[\"mae\"])\n","      print(\"Mean Absolute Percentage Error:       \", result_metrics[\"mape\"])\n","      print(\"Mean Squared Error:   \", result_metrics[\"mse\"])\n","      print(\"Root Mean Squared Error:   \", result_metrics[\"rmse\"])\n","      print(\"R^2:   \", result_metrics[\"r2\"])\n","    return result_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1326,"status":"ok","timestamp":1670551459814,"user":{"displayName":"Haya Hidayatullah","userId":"16049628537625116001"},"user_tz":480},"id":"PggG1oHRh7hb","outputId":"74a6a2c3-711c-46ae-bf39-5bce2f7b2aaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Absolute Error:        2.9798340523777664\n","Mean Absolute Percentage Error:        0.41536413493285534\n","Mean Squared Error:    13.25524150082962\n","Root Mean Squared Error:    3.6407748489613607\n","R^2:    -0.48220302667875514\n"]}],"source":["metrics = calculate_metrics(y_test_preds, y_test)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["F_mr_Fj6dXZK","zDrgLqlQbUUR"],"machine_shape":"hm","provenance":[{"file_id":"11Cu1C2JKlRPuzKYFuSXGusthoWGqvRDE","timestamp":1670231146170},{"file_id":"1DvbM3z7J0U0c-3EXRDpjpzjy-o2NWRJP","timestamp":1670119902708},{"file_id":"1pox5ISxnu2Ka7mxWOge2FpGZ8X5LA504","timestamp":1669965274706}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}